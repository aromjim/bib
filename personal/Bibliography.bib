@String{AAAI = {Association for the Advancement of Artificial Intelligence}}
@String{ACM = {Association for Computing Machinery}}
@String{Adaptive_Computation = {Adaptive Computation and Machine Learning}}
@String{Addison-Wesley = {Addison-Wesley}}
@String{AI_Access_Foundation = {AI Access Foundation}}
@String{Alianza = {Alianza editorial}}
@String{Apress = {Apress}}
@String{Ariel = {Editorial Ariel}}
@String{Birkhäuser = {Birkhäuser Basel}}
@String{CRC = {{CRC} Press}}
@String{CRC_Computer_Science = {Chapman \& Hall/{CRC} Computer Science and Data Analysis}}
@String{CRC_Modern_Statistical_Methods = {Chapman \& Hall/{CRC} Handbooks of Modern Statistical Methods}}
@String{CRC_R_Series = {Chapman \& Hall/{CRC} The R Series}}
@String{CRC_Statistical_Science = {Chapman \& Hall/{CRC} Texts in Statistical Science}}
@String{Cambridge = {Cambridge University Press}}
@String{Díaz_de_Santos = {Editorial Díaz de Santos}}
@String{Elsevier = {Elsevier}}
@String{Kronos = {Editorial Kronos}}
@String{LNCS = {Lecture Notes in Computer Science}}
@String{Microtome = {Microtome Publishing}}
@String{MIT_Press = {The {MIT} Press}}
@String{Modern_Birkhäuser_Classics = {Modern Birkhäuser Classics}}
@String{Morgan-Kaufmann = {Morgan-Kaufmann}}
@String{No_Starch = {No Starch Press}}
@String{OReilly = {O'Reilly}}
@String{Oxford = {Oxford University Press}}
@String{Oxford_Statistical_Science = {Oxford Statistical Science Series}}
@String{Packt = {Packt Publishing}}
@String{Prentice_Hall = {Prentice Hall}}
@String{Prentice_Hall_Artificial_Intelligence = {Prentice Hall Series in Artificial Intelligence}}
@String{RCLibros = {RC Libros}}
@String{Springer = {Springer}}
@String{Springer_AI_Foundations = {Artificial Intelligence: Foundations, Theory, and Algorithms}}
@String{Springer_Computational_Intelligence = {Studies in Computational Intelligence}}
@String{Springer_Computer_Science = {Graduate Texts in Computer Science}}
@String{Springer_Computer_Vision = {Advances in Computer Vision and Pattern Recognition}}
@String{Springer_Decision_Engineering = {Decision Engineering}}
@String{Springer_Information_Systems = {Integrated Series in Information Systems}}
@String{Springer_Natural_Computing = {Natural Computing Series}}
@String{Springer_Operations_Financial = {Springer Series in Operations Research and Financial Engineering}}
@String{Springer_Operations_Management = {International Series in Operations Research \& Management Science}}
@String{Springer_Simulation_Foundations = {Simulation Foundations, Methods and Applications}}
@String{Springer_Statistics = {Springer Series in Statistics}}
@String{Springer_Statistics_Computing = {Statistics and Computing}}
@String{Springer_Stochastic_Modelling = {Stochastic Modelling and Applied Probability}}
@String{Springer_Studies_Big_Data = {Studies in Big Data}}
@String{Springer_Text_Statistics = {Springer Texts in Statistics}}
@String{Springer_Undergraduate_Mathematics = {Springer Undergraduate Mathematics Series}}
@String{Springer_Undergraduate_Topics = {Undergraduate Topics in Computer Science}}
@String{Springer_Use_R = {Use R!}}
@String{Statistics_Practice = {Statistics in Practice}}
@String{Wiley = {Wiley}}
@String{Wiley_Probability_Statistics = {Wiley Series in Probability and Statistics}}
@String{Springer_Computational_Statistics = {Springer Handbooks of Computational Statistics}}
@String{Springer_Scientific_Computation = {Scientific Computation}}
@String{Pragmatic_Bookshelf = {The Pragmatic Bookshelf}}
@String{Hobart = {Hobart Press}}
@String{Princeton = {Princeton University Press}}
@String{Princeton_Applied_Mathematics = {Princeton Series in Applied Mathematics}}
@String{Springer_Combinatorial_Optimization = {Combinatorial Optimization}}
@String{Elsevier_Annals_Discrete_Mathematics = {Annals of Discrete Mathematics}}
@String{SIAM = {Society for Industrial and Applied Mathematics}}
@String{SIAM_Software_Environments_Tools = {Software, Environments and Tools}}
@String{IBM_Press = {IBM Press}}
@String{Intelligent_Systems_Library = {Intelligent Systems Reference Library}}
@String{Cengage = {Cengage}}
@String{CRC_Interdisciplinary_Statistics = {Chapman \& Hall/{CRC} Interdisciplinary Statistics}}
@String{Springer_Graduate_Texts_Mathematics = {Graduate Texts in Mathematics}}
@String{CRC_Data_Science = {Chapman \& Hall/{CRC} Data Science Series}}
@String{Springer_Theoretical_Computer_Science = {Theoretical Computer Science and General Issues}}
@String{Sage = {SAGE Publishing}}
@String{Morgan_Claypool = {Morgan \& Claypool Publishers}}
@String{MC_Human_Language_Technologies = {Synthesis Lectures on Human Language Technologies}}
@String{Springer_Adaptation_Learning_Optimization = {Adaptation, Learning, and Optimization}}
@String{IOS = {IOS_Press}}
@String{Frontiers_Artificial_Intelligence = {Frontiers in Artificial Intelligence and Applications}}
@String{Lehmanns_Media = {Lehmanns Media}}
@String{AMS = {American Mathematical Society}}
@String{Student_Mathematical_Society = {Student Mathematical Society}}
@String{ISTE = {International Society for Technology in Education}}

@InCollection{romero-jiménez_orellana-martín:design_patterns_efficient_solutions,
	file = {Capítulos_de_libros/Romero-Jiménez_Orellana-Martín-2018-Design_patterns_for_efficient_solutions_to_NP-complete_problems_in_membrane_computing.pdf},
	keywords = {Computación con membranas, Patrones de diseño},
	abstract = {Many variants of P systems have the ability to generate an exponential number
of membranes in linear time. This feature has been exploited to elaborate
(theoretical) efficient solutions to **NP**-complete, or even harder, problems.
A thorough review of the existent solutions shows the utilization of common
techniques and procedures. The abstraction of the latter into design patterns
can serve to ease and accelerate the construction of efficient solutions to new
hard problems.},
	doi = {10.1007/978-3-030-00265-7_19},
	crossref = {graciani_et_al:enjoying_natural_computing},
	title = {Design patterns for efficient solutions to NP-complete problems in membrane computing},
	author = {Romero-Jiménez, Álvaro and Orellana-Martín, David}
}

@InCollection{romero-jimenez_et_al:twelve_years_sevilla_carpets,
	abstract = {In the membrane computing community, efficiency and universality of multiple models have been
deeply studied, while descriptive complexity of the computations has not usually attracted so much
attention. Sevilla carpets are a tool to analyze in detail the computations of P systems, beyond of
just taking into consideration the time and space resources spent by them. This paper is a survey
of the variants of Sevilla carpets and associated parameters available in the literature up to now.
A simple case study is included, to illustrate the possible use of Sevilla carpets to help
comparing different cellular models solving the same task.},
	keywords = {Computación bioinspirada, Computación con membranas},
	pages = {141-152},
	author = {Romero-Jiménez, Álvaro and Riscos-Núñez, Agustín and Macías-Ramos, Luis Felipe and Graciani, Carmen and Pérez-Jiménez, Mario J. and Valencia-Cabrera, Luis},
	title = {Twelve Years of Sevilla Carpets: a Survey},
	crossref = {gheorghe_et_al:multidisciplinary_creativity}
}

@InProceedings{romero-jimenez_et_al:generating_diophantine_sets_virus_machines,
	date = {2015-12-24},
	abstract = {Virus Machines are a computational paradigm inspired by the manner in which viruses replicate and
transmit from one host cell to another. This paradigm provides non-deterministic sequential
devices. Non-restricted virus machines are unbounded virus machines, in the sense that no
restriction on the number of hosts, the number of instructions and the number of viruses contained
in any host along any computation is placed on them. The computational completeness of these
machines has been obtained by simulating register machines. In this paper, virus machines as set
generating devices are considered. Then, the universality of non-restricted virus machines is
proved by showing that they can compute all diophantine sets, which the MRDP theorem proves that
coincide with the recursively enumerable sets.},
	enlaces = {Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-3-662-49014-3_30
},
	file = {Contribuciones_a_congresos/Romero-Jiménez_et_al-2015-Generating_Diophantine_Sets_by_Virus_Machines.pdf},
	keywords = {Computación bioinspirada, Máquinas de virus},
	doi = {10.1007/978-3-662-49014-3_30},
	pages = {331-341},
	crossref = {gong_et_al:bio_inspired_computing_theories_applications},
	title = {Generating Diophantine Sets by Virus Machines},
	author = {Romero-Jiménez, Álvaro and Valencia-Cabrera, Luis and Pérez-Jiménez, Mario de Jesús}
}

@InProceedings{romero-jimenez_et_al:computing_partial_recursive_functions_virus_machines,
	date = {2015-12-30},
	author = {Romero-Jiménez, Álvaro and Valencia-Cabrera, Luis and Riscos-Núñez, Agustín and Pérez-Jiménez, Mario J.},
	title = {Computing Partial Recursive Functions by Virus Machines},
	pages = {353-368},
	doi = {10.1007/978-3-319-28475-0_24},
	keywords = {Computación bioinspirada, Máquinas de virus},
	file = {Contribuciones_a_congresos/Romero-Jiménez_et_al-2015-Computing_Partial_Recursive_Functions_by_Virus_Machines.pdf},
	enlaces = {Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-3-319-28475-0_24},
	abstract = {Virus Machines are a computational paradigm inspired by the manner in which viruses replicate and
transmit from one host cell to another. This paradigm provides non-deterministic sequential
devices. Non-restricted Virus Machines are unbounded Virus Machines, in the sense that no
restriction on the number of hosts, the number of instructions and the number of viruses contained
in any host along any computation is placed on them. The computational completeness of these
machines has been obtained by simulating register machines. In this paper, Virus Machines as
function computing devices are considered. Then, the universality of non-restricted virus machines
is proved by showing that they can compute all partial recursive functions.},
	crossref = {rozenberg_et_al:membrane_computing}
}

@InProceedings{mikolov_et_al:distributed_representations_words_phrases,
	file = {Contribuciones_a_congresos/Mikolov_et_al-2013-Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality.pdf},
	keywords = {Procesamiento del lenguaje natural, Redes neuronales},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for
learning high-quality distributed vector representations that capture a large
num- ber of precise syntactic and semantic word relationships. In this paper we
present several extensions that improve both the quality of the vectors and the
training speed. By subsampling of the frequent words we obtain significant
speedup and also learn more regular word representations. We also describe a
simple alterna- tive to the hierarchical softmax called negative sampling.

An inherent limitation of word representations is their indifference to word
order and their inability to represent idiomatic phrases. For example, the
meanings of “Canada” and “Air” cannot be easily combined to obtain “Air
Canada”. Motivated by this example, we present a simple method for finding
phrases in text, and show that learning good vector representations for
millions of phrases is possible.},
	crossref = {nips_2013},
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey}
}

@InProceedings{karp:reducibility_among_combinatorial_problems,
	file = {Contribuciones_a_congresos/Karp-1972-Reducibility_Among_Combinatorial_Problems.pdf},
	abstract = {A large class of computational problems involve the determination of properties
of graphs, digraphs, integers, arrays of integers, finite families of finite
sets, boolean formulas and elements of other countable domains. Through simple
encodings from such domains into the set of words over a finite alphabet these
problems can be converted into language recognition problems, and we can
inquire into their computational complexity. It is reasonable to consider such
a problem satisfactorily solved when an algorithm for its solution is found
which terminates within a number of steps bounded by a polynomial in the length
of the input. We show that a large number of classic unsolved problems of
covering, matching, packing, routing, assignment and sequencing are equivalent,
in the sense that either each of them possesses a polynomial-bounded algorithm
or none of them does.},
	pages = {85-103},
	crossref = {miller_et_al:complexity_computer_computations},
	title = {Reducibility Among Combinatorial Problems},
	author = {Karp, Richard Manning}
}

@InProceedings{cook:complexity_theorem_proving_procedures,
	file = {Contribuciones_a_congresos/Cook-1971-The_Complexity_of_Theorem-Proving_Procedures.pdf},
	keywords = {Complejidad computacional},
	abstract = {It is shown that any recognition problem solved by a polynomial time-bounded
nondeterministic Turing machine can be “reduced” to the problem of determining
whether a given propositional formula is a tautology. Here “reduced” means,
roughly speaking, that the first problem can be solved deterministically in
polynomial time provided an oracle is available for solving the second. From
this notion of reducible, polynomial degrees of difficulty are defined, and it
is shown that the problem of determining tautologyhood has the same polynomial
degree as the problem of determining whether the first of two given graphs is
isomorphic to a subgraph of the second. Other examples are discussed. A method
of measuring the complexity of proof procedures for the predicate calculus is
introduced and discussed.},
	doi = {10.1145/800157.805047},
	pages = {151-158},
	crossref = {stoc-71},
	title = {The Complexity of Theorem-Proving Procedures},
	author = {Cook, Stephen Arthur}
}

@Online{Kaggle_webpage,
	langid = {spanish},
	url = {https://www.kaggle.com/},
	title = {Página web de Kaggle}
}

@XData{acm_TOMACS,
	journaltitle = {ACM Transactions on Modeling and Computer Simulation},
	issn = {1049-3301},
	publisher = ACM
}

@XData{acm_computing_surveys,
	journaltitle = {ACM Computing Surveys},
	issn = {0360-0300},
	publisher = ACM
}

@Book{aggarwal:outlier_analysis,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2672716
Recurso electrónico (SpringerLink): http://www.us.debiblio.com/login?url=https://doi.org/10.1007/978-1-4614-6396-2
},
	file = {Libros/Aggarwal-2013-Outlier_Analysis.pdf},
	keywords = {Análisis de valores atípicos},
	abstract = {With the increasing advances in hardware technology for data collection, and
advances in software technology (databases) for data organization, computer
scientists have increasingly participated in the latest advancements of the
outlier analysis field. Computer scientists, specifically, approach this field
based on their practical experiences in managing large amounts of data, and
with far fewer assumptions– the data can be of any type, structured or
unstructured, and may be extremely large. Outlier Analysis is a comprehensive
exposition, as understood by data mining experts, statisticians and computer
scientists. The book has been organized carefully, and emphasis was placed on
simplifying the content, so that students and practitioners can also benefit.
Chapters will typically cover one of three areas: methods and techniques
commonly used in outlier analysis, such as linear methods, proximity-based
methods, subspace methods, and supervised methods; data domains, such as, text,
categorical, mixed-attribute, time-series, streaming, discrete sequence,
spatial and network data; and key applications of these methods as applied to
diverse domains such as credit card fraud detection, intrusion detection,
medical diagnosis, earth science, web log analytics, and social network
analysis are covered.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9781461463955},
	doi = {10.1007/978-1-4614-6396-2},
	pagetotal = {446},
	isbn = {9781461463955},
	publisher = Springer,
	language = {english},
	date = {2013},
	title = {Outlier Analysis},
	author = {Aggarwal, Charu C.}
}

@Book{aggarwal_sathe:outlier_ensembles,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013154696804987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	keywords = {Análisis de valores atípicos},
	abstract = {This book discusses a variety of methods for outlier ensembles and organizes
them by the specific principles with which accuracy improvements are achieved.
In addition, it covers the techniques with which such methods can be made more
effective. A formal classification of these methods is provided, and the
circumstances in which they work well are examined. The authors cover how
outlier ensembles relate (both theoretically and practically) to the ensemble
techniques used commonly for other data mining problems like classification.
The similarities and (subtle) differences in the ensemble techniques for the
classification and outlier detection problems are explored. These subtle
differences do impact the design of ensemble algorithms for the latter problem.
This book can be used for courses in data mining and related curricula. Many
illustrative examples and exercises are provided in order to facilitate
classroom teaching. A familiarity is assumed to the outlier detection problem
and also to generic problem of ensemble analysis in classification. This is
because many of the ensemble methods discussed in this book are adaptations
from their counterparts in the classification domain. Some techniques explained
in this book, such as wagging, randomized feature weighting, and geometric
subsampling, provide new insights that are not available elsewhere. Also
included is an analysis of the performance of various types of base detectors
and their relative effectiveness. The book is valuable for researchers and
practitioners for leveraging ensemble methods into optimal algorithmic design.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/978-3-319-54765-7},
	pagetotal = {276},
	isbn = {9783319547640},
	publisher = Springer,
	language = {english},
	subtitle = {An Introduction},
	date = {2017},
	title = {Outlier Ensembles},
	author = {Aggarwal, Charu C. and Sathe, Saket}
}

@Article{agha_palmskog:survey_statistical_model_checking,
	xdata = {acm_TOMACS},
	keywords = {Verificación de modelos},
	file = {Artículos_en_revistas/Agha_Palmskog-2018-A_Survey_of_Statistical_Model_Checking.pdf},
	abstract = {Interactive, distributed, and embedded systems often behave stochastically, for
example, when inputs, message delays, or failures conform to a probability
distribution. However, reasoning analytically about the behavior of complex
stochastic systems is generally infeasible. While simulations of systems are
commonly used in engineering practice, they have not traditionally been used to
reason about formal specifications. Statistical model checking (SMC) addresses
this weakness by using a simulation-based approach to reason about precise
properties specified in a stochastic temporal logic. A specification for a
communication system may state that within some time bound, the probability
that the number of messages in a queue will be greater than 5 must be less than
0.01. Using SMC, executions of a stochastic system are first sampled, after
which statistical techniques are applied to determine whether such a property
holds. While the output of sample-based methods are not always correct,
statistical inference can quantify the confidence in the result produced. In
effect, SMC provides a more widely applicable and scalable alternative to
analysis of properties of stochastic systems using numerical and symbolic
methods. SMC techniques have been successfully applied to analyze systems with
large state spaces in areas such as computer networking, security, and systems
biology. In this article, we survey SMC algorithms, techniques, and tools,
while emphasizing current limitations and tradeoffs between precision and
scalability.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1145/3158668},
	pagetotal = {39},
	number = {1},
	volume = {28},
	language = {english},
	date = {2018},
	title = {A Survey of Statistical Model Checking},
	author = {Agha, Gul and Palmskog, Karl}
}

@XData{ai_magazine,
	journaltitle = {AI Magazine},
	issn = {0738-4602},
	publisher = AAAI
}

@Article{aleti_moser:adaptive_parameter_evolutionary_algorithms,
	pagetotal = {35},
	xdata = {acm_computing_surveys},
	file = {Artículos_en_revistas/Aleti_Moser-2016-A_Systematic_Literature_Review_of_Adaptive_Parameter_Control_Methods_for_Evolutionary_Algorithms.pdf},
	keywords = {Algoritmos evolutivos, Inteligencia artificial},
	abstract = {Evolutionary algorithms (EAs) are robust stochastic optimisers that perform well over a wide range
of problems. Their robustness, however, may be affected by several adjustable parameters, such as
mutation rate, crossover rate, and population size. Algorithm parameters are usually
problem-specific, and often have to be tuned not only to the problem but even the problem instance
at hand to achieve ideal performance. In addition, research has shown that different parameter
values may be optimal at different stages of the optimisation process. To address these issues,
researchers have shifted their focus to adaptive parameter control, in which parameter values are
adjusted during the optimisation process based on the performance of the algorithm. These methods
redefine parameter values repeatedly based on implicit or explicit rules that decide how to make
the best use of feedback from the optimisation algorithm.

In this survey, we systematically investigate the state of the art in adaptive parameter control.
The approaches are classified using a new conceptual model that subdivides the process of adapting
parameter values into four steps that are present explicitly or implicitly in all existing
approaches that tune param- eters dynamically during the optimisation process. The analysis reveals
the major focus areas of adaptive parameter control research as well as gaps and potential
directions for further development in this area.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1145/2996355},
	number = {3},
	volume = {49},
	language = {english},
	date = {2016},
	title = {A Systematic Literature Review of Adaptive Parameter Control Methods for Evolutionary Algorithms},
	author = {Aleti, Aldeida and Moser, Irene}
}

@Book{alonso_borrego:deduccion_automática,
	keywords = {Lógica matemática},
	langid = {spanish},
	pagetotal = {98},
	isbn = {9788486273583},
	publisher = Kronos,
	language = {spanish},
	subtitle = {Vol. 1: Construcción lógica de sistemas lógicos},
	date = {2002},
	title = {Deducción automática},
	author = {Alonso Jiménez, José Antonio and Borrego Díaz, Joaquín}
}

@Book{applegate_et_al:traveling_salesman_problem,
	series = Princeton_Applied_Mathematics,
	publisher = Princeton,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1918794
Recurso electrónico (E-Libro): http://www.us.debiblio.com/login?url=http://site.ebrary.com/lib/unisev/Doc?id=10496632
},
	file = {Libros/Applegate_et_al-2007-The_Traveling_Salesman_Problem.pdf},
	keywords = {NP-completitud},
	abstract = {This book presents the latest findings on one of the most intensely
investigated subjects in computational mathematics--the traveling salesman
problem. It sounds simple enough: given a set of cities and the cost of travel
between each pair of them, the problem challenges you to find the cheapest
route by which to visit all the cities and return home to where you began.
Though seemingly modest, this exercise has inspired studies by mathematicians,
chemists, and physicists. Teachers use it in the classroom. It has practical
applications in genetics, telecommunications, and neuroscience.

The authors of this book are the same pioneers who for nearly two decades have
led the investigation into the traveling salesman problem. They have derived
solutions to almost eighty-six thousand cities, yet a general solution to the
problem has yet to be discovered. Here they describe the method and computer
code they used to solve a broad range of large-scale problems, and along the
way they demonstrate the interplay of applied mathematics with increasingly
powerful computing platforms. They also give the fascinating history of the
problem--how it developed, and why it continues to intrigue us.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://press.princeton.edu/titles/8451.html},
	pagetotal = {608},
	isbn = { 9780691129938},
	language = {english},
	subtitle = {A Computational Study},
	date = {2007},
	title = {The Traveling Salesman Problem},
	author = {Applegate, David L. and Bixby, Robert E. and Chvátal, Vasek and Cook, William John}
}

@Book{arenas:logica_formal_informaticos,
	keywords = {Lógica matemática},
	langid = {spanish},
	pagetotal = {331},
	isbn = {9788479782404},
	publisher = Díaz_de_Santos,
	language = {spanish},
	date = {1996},
	title = {Lógica formal para informáticos},
	author = {Arenas Alegría, Lourdes}
}

@Book{arora_barak:computational_complexity,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/3enc2g/alma991008441249704987; https://fama.us.es/permalink/34CBUA_US/3enc2g/alma991012836539704987
},
	file = {Libros/Arora_Barak-2009-Computational_Complexity.pdf},
	keywords = {Ciencias de la computación, Complejidad computacional, NP-completitud},
	abstract = {This beginning graduate textbook describes both recent achievements and
classical results of computational complexity theory. Requiring essentially no
background apart from mathematical maturity, the book can be used as a
reference for self-study for anyone interested in complexity, including
physicists, mathematicians, and other scientists, as well as a textbook for a
variety of courses and seminars. More than 300 exercises are included with a
selected hint set.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.cambridge.org/us/catalogue/catalogue.asp?isbn=9780521424264},
	pagetotal = {594},
	isbn = {9780521424264},
	publisher = Cambridge,
	language = {english},
	subtitle = {A Modern Approach},
	date = {2009},
	title = {Computational Complexity},
	author = {Arora, Sanjeev and Barak, Boaz}
}

@book{asmussen_glynn:stochastic_simulation,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1929163
Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-0-387-69033-9},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780387306797},
	doi = {10.1007/978-0-387-69033-9},
	language = {english},
	subtitle = {Algorithms and Analysis},
	title = {Stochastic Simulation},
	isbn = {9780387306797},
	series = Springer_Stochastic_Modelling,
	abstract = {Sampling-based computational methods have become a fundamental part of the numerical toolset of
practitioners and researchers across an enormous number of different applied domains and academic
disciplines. This book provides a broad treatment of such sampling-based methods, as well as
accompanying mathematical analysis of the convergence properties of the methods discussed. The
reach of the ideas is illustrated by discussing a wide range of applications and the models that
have found wide usage. The first half of the book focuses on general methods, whereas the second
half discusses model-specific algorithms.

Given the wide range of examples, exercises and applications students, practitioners and
researchers in probability, statistics, operations research, economics, finance, engineering as
well as biology and chemistry and physics will find the book of value.},
	pagetotal = {476},
	number = {57},
	publisher = Springer,
	author = {Asmussen, Søren and Glynn, Peter W.},
	date = {2007},
	keywords = {Métodos de Monte Carlo, Simulación estocástica},
	file = {Libros/Asmussen_Glynn-2007-Stochastic_Simulation.pdf}
}

@Book{ausiello:making_new_science,
	keywords = {Ciencias de la computación},
	abstract = {This book explains the development of theoretical computer science in its early
stages, specifically from 1965 to 1990. The author is among the pioneers of
theoretical computer science, and he guides the reader through the early stages
of development of this new discipline. He explains the origins of the field,
arising from disciplines such as logic, mathematics, and electronics, and he
describes the evolution of the key principles of computing in strands such as
computability, algorithms, and programming.

But mainly it's a story about people – pioneers with diverse backgrounds and
characters came together to overcome philosophical and institutional challenges
and build a community. They collaborated on research efforts, they established
schools and conferences, they developed the first related university courses,
they taught generations of future researchers and practitioners, and they set
up the key publications to communicate and archive their knowledge.

The book is a fascinating insight into the field as it existed and evolved, it
will be valuable reading for anyone interested in the history of computing.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783319626796},
	doi = {10.1007/978-3-319-62680-2},
	pagetotal = {290},
	isbn = {9783319626796},
	publisher = Springer,
	language = {english},
	subtitle = {A Personal Journey Through the Early Years of Theoretical Computer Science},
	date = {2018},
	title = {The Making of a New Science},
	author = {Ausiello, Giorgio}
}

@Book{ausiello_et_al:complexity_approximation,
	file = {Libros/Ausiello_et_al-1999-Complexity_and_Approximation.pdf},
	keywords = {Algoritmia, Complejidad computacional, Problemas de optimización},
	abstract = {IN COMPUTER applications we are used to live with approximation. Various
notions of approximation appear, in fact, in many circumstances. One notable
example is the type of approximation that arises in numerical analysis or in
computational geometry from the fact that we cannot perform computations with
arbitrary precision and we have to truncate the representation of real numbers.
In other cases, we use to approximate complex mathematical objects by simpler
ones: for example, we sometimes represent non-linear functions by means of
piecewise linear ones. The need to solve difficult optimization problems is
another reason that forces us to deal with approximation. In particular, when a
problem is computationally hard (i. e., the only way we know to solve it is by
making use of an algorithm that runs in exponential time), it may be
practically unfeasible to try to compute the exact solution, because it might
require months or years of machine time, even with the help of powerful
parallel computers. In such cases, we may decide to restrict ourselves to
compute a solution that, though not being an optimal one, nevertheless is close
to the optimum and may be determined in polynomial time. We call this type of
solution an approximate solution and the corresponding algorithm a
polynomial-time approximation algorithm. Most combinatorial optimization
problems of great practical relevance are, indeed, computationally intractable
in the above sense. In formal terms, they are classified as Np-hard
optimization problems.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783540654315},
	doi = {10.1007/978-3-642-58412-1},
	pagetotal = {524},
	isbn = {9783642635816},
	publisher = Springer,
	language = {english},
	subtitle = {Combinatorial Optimization Problems and Their Approximability Properties},
	date = {1999},
	title = {Complexity and Approximation},
	author = {Ausiello, Giorgio and Crescenzi, Pierluigi and Gambosi, Giorgio and Kann, Viggo and Marchetti-Spaccamela, Alberto and Protasi, Marco}
}

@Book{badenhorst:practical_python_design_patterns,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2835339
},
	keywords = {Lenguaje de programación Python},
	abstract = {Become a better, more productive programmer through a series of projects that
will help you deeply understand and master each of the design patterns covered.
In this book you will learn to write elegant "Pythonic" code to solve common
programming problems. You will also experience design thinking, by identifying
design patterns that would be helpful given a specific problem or situation.

Python is eating the world. In recent years it has become so much more than a
mere object-oriented, scripting language. Design patterns help you think of and
solve problems in chunks. They help you to stand on the shoulders of the giants
who have come before, instead of having to reinvent the wheel.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781484226797},
	doi = {10.1007/978-1-4842-2680-3},
	pagetotal = {350},
	isbn = {9781484226797},
	publisher = Apress,
	language = {english},
	subtitle = {Pythonic Solutions to Common Problems},
	date = {2017},
	title = {Practical Python Design Patterns},
	author = {Badenhorst, Wessel}
}

@Book{badesa_et_al:elementos_logica_formal,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2048812},
	keywords = {Lógica matemática},
	abstract = {La Filosofía se ha interesado prácticamente desde sus orígenes por los aspectos formales del
razonamiento. Aristóteles fue el primero en desarrollar una teoría de la argumentación deductiva,
por lo que se le considera con justicia el creador de la lógica como disciplina. La lógica
permaneció esencialmente en el mismo estado en que la dejó Aristóteles hasta mediados del siglo
xix, cuando inició un nuevo desarrollo, basado en gran medida en su capacidad para analizar con
ayuda de métodos matemáticos formas de razonamiento de las que la lógica aristotélica no podía dar
cuenta, en particular, aquellas en que intervienen expresiones cuantificacionales múltiples y
expresiones relacionales. Para el tratamiento sistemático de estas formas de razonamiento, se
desarrollaron a finales del siglo xix y principios del xx la teoría de las relaciones y la de la
cuantificación. Estas dos teorías, junto con el cálculo proposicional, cuyo estudio iniciaron los
lógicos megáricos y estoicos, constituyen el cuerpo básico de conocimientos de la lógica, una
disciplina que a lo largo del siglo xx se ha desarrollado considerablemente y que está todavía en
expansión. Elementos de lógica formal es un manual de introducción a la lógica, escrito
especialmente para estudiantes de filosofía, pero también para aquellas personas con formación
humanística interesadas en materias que requieran conocimientos lógicos. En él se exponen los
conceptos y resultados básicos de la lógica contemporánea sin presuponer ningún conocimiento
técnico especial por parte del lector. Los elementos de teoría de conjuntos necesarios para
presentar con rigor la lógica proposicional y, sobre todo, la cuantificacional se introducen de
forma pausada en los primeros capítulos del libro. El concepto de infinitud, que tradicionalmente
ha sido objeto de reflexión filosófica, es un concepto propio de la teoría de conjuntos que el
lector también encontrará caracterizado con rigor en estos capítulos.},
	langid = {spanish},
	pagetotal = {381},
	isbn = {9788434487772},
	publisher = Ariel,
	edition = {2},
	language = {spanish},
	date = {2007},
	title = {Elementos de lógica formal},
	author = {Badesa, Calixto and Jané, Ignacio and Jansana, Ramón}
}

@Book{ben-ari:mathematical_logic_computer_science,
	enlaces = {Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-1-4471-4129-7
Springer: http://www.springer.com/gp/book/9781447141280
},
	file = {Libros/Ben-Ari-2012-Mathematical_Logic_for_Computer_Science.pdf},
	keywords = {Lógica matemática},
	language = {english},
	langidopts = {variant=british},
	langid = {english},
	abstract = {_Mathematical Logic for Computer Science_ is a mathematics textbook with theorems and proofs, but
the choice of topics has been guided by the needs of students of computer science. The method of
semantic tableaux provides an elegant way to teach logic that is both theoretically sound and easy
to understand. The uniform use of tableaux-based techniques facilitates learning advanced logical
systems based on what the student has learned from elementary systems.

The logical systems presented are: propositional logic, first-order logic, resolution and its
application to logic programming, Hoare logic for the verification of sequential programs, and
linear temporal logic for the verification of concurrent programs.

The third edition has been entirely rewritten and includes new chapters on central topics of modern
computer science: SAT solvers and model checking.},
	doi = {10.1007/978-1-4471-4129-7},
	author = {Ben-Ari, Mordechai},
	date = {2012},
	edition = {3},
	isbn = {9781447141280},
	pagetotal = {346},
	publisher = Springer,
	title = {Mathematical Logic for Computer Science}
}

@Article{bengio_et_al:neural_probabilistic_language_model,
	xdata = {journal_machine_learning_research},
	file = {Artículos_en_revistas/Bengio_et_al-2003-A_Neural_Probabilistic_Language_Model.pdf},
	keywords = {Procesamiento del lenguaje natural},
	abstract = {A goal of statistical language modeling is to learn the joint probability
function of sequences of words in a language. This is intrinsically difficult
because of the **curse of dimensionality**: a word sequence on which the model
will be tested is likely to be different from all the word sequences seen
during training. Traditional but very successful approaches based on n-grams
obtain generalization by concatenating very short overlapping sequences seen in
the training set. We propose to fight the curse of dimensionality by **learning
a distributed representation for words** which allows each training sentence to
inform the model about an exponential number of semantically neighboring
sentences. The model learns simultaneously (1) a distributed representation for
each word along with (2) the probability function for word sequences, expressed
in terms of these representations. Generalization is obtained because a
sequence of words that has never been seen before gets high probability if it
is made of words that are similar (in the sense of having a nearby
representation) to words forming an already seen sentence. Training such large
models (with millions of parameters) within a reasonable time is itself a
significant challenge. We report on experiments using neural networks for the
probability function, showing on two text corpora that the proposed approach
significantly improves on state-of-the-art n-gram models, and that the proposed
approach allows to take advantage of longer contexts.},
	langidopts = {version=british},
	langid = {english},
	url = {http://www.jmlr.org/papers/v3/bengio03a.html},
	pages = {1137-1155},
	volume = {3},
	language = {english},
	date = {2003},
	title = {A Neural Probabilistic Language Model},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian}
}

@Article{bianchi_et_al:survey_metaheuristics_stochastic_combinatorial_optimization,
	file = {Artículos_en_revistas/Bianchi_et_al-2009-A_survey_on_metaheuristics_for_stochastic_combinatorial_optimization.pdf},
	keywords = {Metaheurísticas, Optimización estocástica},
	abstract = {Metaheuristics are general algorithmic frameworks, often nature-inspired,
designed to solve complex optimization problems, and they are a growing
research area since a few decades. In recent years, metaheuristics are emerging
as successful alternatives to more classical approaches also for solving
optimization problems that include in their mathematical formulation uncertain,
stochastic, and dynamic information. In this paper metaheuristics such as Ant
Colony Optimization, Evolutionary Computation, Simulated Annealing, Tabu Search
and others are introduced, and their applications to the class of Stochastic
Combinatorial Optimization Problems (SCOPs) is thoroughly reviewed. Issues
common to all metaheuristics, open problems, and possible directions of
research are proposed and discussed. In this survey, the reader familiar to
metaheuristics finds also pointers to classical algorithmic approaches to
optimization under uncertainty, and useful informations to start working on
this problem domain, while the reader new to metaheuristics should find a good
tutorial in those metaheuristics that are currently being applied to
optimization under uncertainty, and motivations for interest in this field.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/s11047-008-9098-4},
	pages = {239-287},
	number = {2},
	volume = {8},
	xdata = {natural_computing},
	language = {english},
	date = {2009},
	title = {A survey on metaheuristics for stochastic combinatorial optimization},
	author = {Bianchi, Leonora and Dorigo, Marco and Gambardella, Luca Maria and Gutjahr, Walter J.}
}

@Book{biere_et_al:handbook_satisfiability,
	editor = {Biere, Armin and Heule, Marijn and van Maaren, Hans and Walsh, Toby},
	series = Frontiers_Artificial_Intelligence,
	publisher = IOS,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/3enc2g/alma991008838569704987
Recurso electrónico: https://fama.us.es/view/action/uresolver.do?operation=resolveService&package_service_id=8572947430004987&institutionId=4987&customerId=4985
},
	file = {Libros/Biere_et_al-2009-Handbook_of_Satisfiability.pdf},
	keywords = {Ciencias de la computación, Problema SAT, Verificación de modelos},
	abstract = {Satisfiability (SAT) related topics have attracted researchers from various
disciplines: logic, applied areas such as planning, scheduling, operations
research and combinatorial optimization, but also theoretical issues on the
theme of complexity and much more, they all are connected through SAT. My
personal interest in SAT stems from actual solving: The increase in power of
modern SAT solvers over the past 15 years has been phenomenal. It has become
the key enabling technology in automated verification of both computer hardware
and software. Bounded Model Checking (BMC) of computer hardware is now probably
the most widely used model checking technique. The counterexamples that it
finds are just satisfying instances of a Boolean formula obtained by unwinding
to some fixed depth a sequential circuit and its specification in linear
temporal logic. Extending model checking to software verification is a much
more difficult problem on the frontier of current research. One promising
approach for languages like C with finite word-length integers is to use the
same idea as in BMC but with a decision procedure for the theory of bit-vectors
instead of SAT. All decision procedures for bit-vectors that I am familiar with
ultimately make use of a fast SAT solver to handle complex formulas. Decision
procedures for more complicated theories, like linear real and integer
arithmetic, are also used in program verification. Most of them use powerful
SAT solvers in an essential way. Clearly, efficient SAT solving is a key
technology for 21st century computer science. I expect this collection of
papers on all theoretical and practical aspects of SAT solving will be
extremely useful to both students and researchers and will lead to many further
advances in the field.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.iospress.nl/book/handbook-of-satisfiability/},
	pagetotal = {980},
	isbn = {9781586039295},
	number = {185},
	language = {english},
	date = {2009},
	title = {Handbook of Satisfiability}
}

@Book{birge_louveaux:introduction_stochastic_programming,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2604642
Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-1-4614-0237-4
},
	file = {Libros/Birge_Louveaux-2011-Introduction_to_Stochastic_Programming.pdf},
	keywords = {Investigación operativa, Optimización estocástica, Programación matemática},
	abstract = {The aim of stochastic programming is to find optimal decisions in problems
which involve uncertain data. This field is currently developing rapidly with
contributions from many disciplines including operations research, mathematics,
and probability. At the same time, it is now being applied in a wide variety of
subjects ranging from agriculture to financial planning and from industrial
engineering to computer networks. This textbook provides a first course in
stochastic programming suitable for students with a basic knowledge of linear
programming, elementary analysis, and probability. The authors aim to present a
broad overview of the main themes and methods of the subject. Its prime goal is
to help students develop an intuition on how to model uncertainty into
mathematical problems, what uncertainty changes bring to the decision process,
and what techniques help to manage uncertainty in solving the problems.

In this extensively updated new edition there is more material on methods and
examples including several new approaches for discrete variables, new results
on risk measures in modeling and Monte Carlo sampling methods, a new chapter on
relationships to other methods including approximate dynamic programming,
robust optimization and online methods.

The book is highly illustrated with chapter summaries and many examples and
exercises. Students, researchers and practitioners in operations research and
the optimization area will find it particularly of interest.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9781461402367},
	doi = {10.1007/978-1-4614-0237-4},
	pagetotal = {485},
	isbn = {9781461402367},
	publisher = Springer,
	series = Springer_Operations_Financial,
	edition = {2},
	language = {english},
	date = {2011},
	title = {Introduction to Stochastic Programming},
	author = {Birge, John R. and Louveaux, François}
}

@Book{bivand_et_al:applied_spatial_data_analysis_r,
	enlaces = {Recurso electrónico (SpringerLink): http://www.us.debiblio.com/login?url=http://dx.doi.org/10.1007/978-1-4614-7618-4
},
	file = {Libros/Bivand_et_al-2013-Applied_Spatial_Data_Analysis_with_R.pdf},
	keywords = {Análisis de datos, Lenguaje de programación R},
	abstract = {Applied Spatial Data Analysis with R, second edition, is divided into two basic
parts, the first presenting R packages, functions, classes and methods for
handling spatial data. This part is of interest to users who need to access and
visualise spatial data. Data import and export for many file formats for
spatial data are covered in detail, as is the interface between R and the open
source GRASS GIS and the handling of spatio-temporal data. The second part
showcases more specialised kinds of spatial data analysis, including spatial
point pattern analysis, interpolation and geostatistics, areal data analysis
and disease mapping. The coverage of methods of spatial data analysis ranges
from standard techniques to new developments, and the examples used are largely
taken from the spatial statistics literature. All the examples can be run using
R contributed packages available from the CRAN website, with code and
additional data sets from the book's own website. Compared to the first
edition, the second edition covers the more systematic approach towards
handling spatial data in R, as well as a number of important and widely used
CRAN packages that have appeared since the first edition.

This book will be of interest to researchers who intend to use R to handle,
visualise, and analyse spatial data. It will also be of interest to spatial
data analysts who do not use R, but who are interested in practical aspects of
implementing software for spatial data analysis. It is a suitable companion
book for introductory spatial statistics courses and for applied methods
courses in a wide range of subjects using spatial data, including human and
physical geography, geographical information science and geoinformatics, the
environmental sciences, ecology, public health and disease control, economics,
public administration and political science.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9781461476177
http://www.asdar-book.org},
	doi = {10.1007/978-1-4614-7618-4},
	pagetotal = {405},
	isbn = {9781461476177},
	publisher = Springer,
	series = Springer_Use_R,
	edition = {2},
	language = {english},
	date = {2013},
	title = {Applied Spatial Data Analysis with R},
	author = {Bivand, Roger S. and Pebesma, Edzer and Gómez-Rubio, Virgilio}
}

@Book{bolon-canedo_alonso-betanzos:recent_advances_ensembles_feature_selection,
	file = {Libros/Bolón-Canedo_Alonso-Betanzos-2018-Recent_Advances_in_Ensembles_for_Feature_Selection.pdf},
	series = Intelligent_Systems_Library,
	keywords = {Aprendizaje automático, No disponible en la BUS},
	abstract = {This book offers a comprehensive overview of ensemble learning in the field of
feature selection (FS), which consists of combining the output of multiple
methods to obtain better results than any single method. It reviews various
techniques for combining partial results, measuring diversity and evaluating
ensemble performance.

With the advent of Big Data, feature selection (FS) has become more necessary
than ever to achieve dimensionality reduction. With so many methods available,
it is difficult to choose the most appropriate one for a given setting, thus
making the ensemble paradigm an interesting alternative.

The authors first focus on the foundations of ensemble learning and classical
approaches, before diving into the specific aspects of ensembles for FS, such
as combining partial results, measuring diversity and evaluating ensemble
performance. Lastly, the book shows examples of successful applications of
ensembles for FS and introduces the new challenges that researchers now face.
As such, the book offers a valuable guide for all practitioners, researchers
and graduate students in the areas of machine learning and data mining.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783319900797},
	doi = {10.1007/978-3-319-90080-3},
	pagetotal = {205},
	isbn = {978-3-319-90079-7},
	publisher = Springer,
	number = {147},
	language = {english},
	date = {2018},
	title = {Recent Advances in Ensembles for Feature Selection},
	author = {Bolón-Canedo, Verónica and Alonso-Betanzos, Amparo}
}

@Book{bolon-canedo_et_al:feature_selection_high_dimensional_data,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2714872},
	series = Springer_AI_Foundations,
	keywords = {Aprendizaje automático, Inteligencia artificial},
	abstract = {This book offers a coherent and comprehensive approach to feature subset selection in the scope of
classification problems, explaining the foundations, real application problems and the challenges
of feature selection for high-dimensional data.

The authors first focus on the analysis and synthesis of feature selection algorithms, presenting a
comprehensive review of basic concepts and experimental results of the most well-known algorithms.

They then address different real scenarios with high-dimensional data, showing the use of feature
selection algorithms in different contexts with different requirements and information: microarray
data, intrusion detection, tear film lipid layer classification and cost-based features. The book
then delves into the scenario of big dimension, paying attention to important problems under
high-dimensional spaces, such as scalability, distributed processing and real-time processing,
scenarios that open up new and interesting challenges for researchers.

The book is useful for practitioners, researchers and graduate students in the areas of machine
learning and data mining.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/gp/book/9783319218571},
	doi = {10.1007/978-3-319-21858-8},
	pagetotal = {147},
	isbn = {9783319218571},
	publisher = Springer,
	language = {english},
	date = {2015},
	title = {Feature Selection for High-Dimensional Data},
	author = {Bolón-Canedo, Verónica and Sánchez-Maroño, Noelia and Alonso-Betanzos, Amparo}
}

@Article{branco_et_al:survey_predictive_modeling_imbalanced_domains,
	xdata = {acm_computing_surveys},
	pagetotal = {50},
	file = {Artículos_en_revistas/Branco_et_al-2016-A_Survey_of_Predictive_Modeling_on_Imbalanced_Domains.pdf},
	keywords = {Aprendizaje automático, Inteligencia artificial},
	abstract = {Many real-world data-mining applications involve obtaining predictive models using datasets with
strongly imbalanced distributions of the target variable. Frequently, the least-common values of
this target variable are associated with events that are highly relevant for end users (e.g., fraud
detection, unusual returns on stock markets, anticipation of catastrophes, etc.). Moreover, the
events may have different costs and benefits, which, when associated with the rarity of some of
them on the available training data, creates serious problems to predictive modeling techniques.
This article presents a survey of existing techniques for handling these important applications of
predictive analytics. Although most of the existing work addresses classification tasks (nominal
target variables), we also describe methods designed to handle similar problems within regression
tasks (numeric target variables). In this survey, we discuss the main challenges raised by
imbalanced domains, propose a definition of the problem, describe the main approaches to these
tasks, propose a taxonomy of the methods, summarize the conclusions of existing comparative studies
as well as some theoretical analyses of some methods, and refer to some related problems within
predictive modeling.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1145/2907070},
	number = {2},
	volume = {49},
	language = {english},
	date = {2016},
	title = {A Survey of Predictive Modeling on Imbalanced Domains},
	author = {Branco, Paula and Torgo, Luís and Ribeiro, Rita P.}
}

@Book{bratko:prolog_programming_artificial_intelligence,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2495400},
	keywords = {Inteligencia artificial, Lenguaje de programación Prolog},
	abstract = {The fourth edition of this best-selling guide to Prolog and Artificial Intelligence has been
updated to include key developments in the field while retaining its lucid approach to these
topics. New and extended topics include Constraint Logic Programming, abductive reasoning and
partial order planning.

Divided into two parts, the first part of the book introduces the programming language Prolog,
while the second part teaches Artificial Intelligence using Prolog as a tool for the implementation
of AI techniques.

This textbook is meant to teach Prolog as a practical programming tool and so it concentrates on
the art of using the basic mechanisms of Prolog to solve interesting problems. The fourth edition
has been fully revised and extended to provide an even greater range of applications, making it a
self-contained guide to Prolog, AI or AI Programming for students and professional programmers.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://catalogue.pearsoned.co.uk/educator/product/Prolog-Programming-for-Artificial-Intelligence/9780321417466.page},
	pagetotal = {696},
	isbn = {9780321417466},
	publisher = Addison-Wesley,
	edition = {4},
	language = {english},
	date = {2012},
	title = {Prolog Programming for Artificial Intelligence},
	author = {Bratko, Ivan}
}

@Book{breiman_et_al:classification_regression_trees,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1461888},
	keywords = {Aprendizaje automático, Inteligencia artificial},
	abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many
other statistical procedures, which moved from pencil and paper to calculators, this text's use of
trees was unthinkable before computers. Both the practical and theoretical sides have been
developed in the authors' study of tree methods. Classification and Regression Trees reflects these
two sides, covering the use of trees as a data analysis method, and in a more mathematical
framework, proving some of their fundamental properties.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Classification-and-Regression-Trees/Breiman-Friedman-Stone-Olshen/p/book/9780412048418},
	pagetotal = {368},
	isbn = {9780412048418},
	publisher = CRC,
	language = {english},
	date = {1984},
	title = {Classification and Regression Trees},
	author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.}
}

@Collection{brooks_et_al:handbook_markov_chain_monte_carlo,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2560735
Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2649734},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Handbook-of-Markov-Chain-Monte-Carlo/Brooks-Gelman-Jones-Meng/p/book/9781420079418},
	language = {english},
	editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin L. and Meng, Xiao-Ling},
	title = {Handbook of Markov Chain Monte Carlo},
	isbn = {9781420079418},
	series = CRC_Modern_Statistical_Methods,
	abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have
revolutionized statistical computing and have had an especially profound impact on the practice of
Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate
models in an astonishing array of disciplines as diverse as fisheries science and economics. The
wide-ranging practical importance of MCMC has sparked an expansive and deep investigation into
fundamental Markov chain theory.

The **Handbook of Markov Chain Monte Carlo** provides a reference for the broad audience of
developers and users of MCMC methodology interested in keeping up with cutting-edge theory and
applications. The first half of the book covers MCMC foundations, methodology, and algorithms. The
second half considers the use of MCMC in a variety of practical applications including in
educational research, astrophysics, brain imaging, ecology, and sociology.

The in-depth introductory section of the book allows graduate students and practicing scientists
new to MCMC to become thoroughly acquainted with the basic theory, algorithms, and applications.
The book supplies detailed examples and case studies of realistic scientific problems presenting
the diversity of methods used by the wide-ranging MCMC community. Those familiar with MCMC
methods will find this book a useful refresher of current theory and recent developments.},
	pagetotal = {619},
	publisher = CRC,
	date = {2011},
	keywords = {Estadística computacional, Métodos de Monte Carlo, Modelos de Markov}
}

@Article{bryce_kambhampati:tutorial_planning_graph_based_reachability_heuristics,
	enlaces = {Recurso electrónico (ProQuest): http://0-search.proquest.com.fama.us.es/docview/208135127/602D16260C344B34PQ/8},
	xdata = {ai_magazine},
	file = {Artículos_en_revistas/Bryce_Kambhampati-2007-A_Tutorial_on_Planning_Graph-Based_Reachability_Heuristics.pdf},
	keywords = {Inteligencia artificial, Planificación automática},
	abstract = {The primary revolution in automated planning in the last decade has been the very impressive
scale-up in planner performance. A large part of the credit for this can be attributed squarely to
the invention and deployment of powerful reachability heuristics. Most, if not all, modern
reachability heuristics are based on a remarkably extensible data structure called the planning
graph, which made its debut as a bit player in the success of GraphPlan, but quickly grew in
prominence to occupy the center stage. Planning graphs are a cheap means to obtain informative
look-ahead heuristics for search and have become ubiquitous in state-of-the-art heuristic search
planners. We present the foundations of planning graph heuristics in classical planning and explain
how their flexibility lets them adapt to more expressive scenarios that consider action costs, goal
utility, numeric resources, time, and uncertainty.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.aaai.org/ojs/index.php/aimagazine/article/view/2028},
	pages = {47-83},
	number = {1},
	volume = {28},
	language = {english},
	date = {2007},
	title = {A Tutorial on Planning Graph–Based Reachability Heuristics},
	author = {Bryce, Daniel and Kambhampati, Subbarao}
}

@Book{buuren:flexible_imputation_missing_data,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013145401604987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	series = CRC_Interdisciplinary_Statistics,
	keywords = {Análisis de valores perdidos},
	abstract = {Missing data pose challenges to real-life data analysis. Simple ad-hoc fixes,
like deletion or mean imputation, only work under highly restrictive
conditions, which are often not met in practice. Multiple imputation replaces
each missing value by multiple plausible values. The variability between these
replacements reflects our ignorance of the true (but missing) value. Each of
the completed data set is then analyzed by standard methods, and the results
are pooled to obtain unbiased estimates with correct confidence intervals.
Multiple imputation is a general approach that also inspires novel solutions to
old problems by reformulating the task at hand as a missing-data problem.

This is the second edition of a popular book on multiple imputation, focused on
explaining the application of methods through detailed worked examples using
the MICE package as developed by the author. This new edition incorporates the
recent developments in this fast-moving field.

This class-tested book avoids mathematical and technical details as much as
possible: formulas are accompanied by verbal statements that explain the
formula in accessible terms. The book sharpens the reader’s intuition on how to
think about missing data, and provides all the tools needed to execute a
well-grounded quantitative analysis in the presence of missing data.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Flexible-Imputation-of-Missing-Data-Second-Edition/Buuren/p/book/9781138588318},
	pagetotal = {416},
	isbn = {9781138588318},
	publisher = CRC,
	language = {english},
	date = {2018},
	title = {Flexible Imputation of Missing Data},
	author = {van Buuren, Stef}
}

@Article{cao:data_science_comprehensive_overview,
	file = {Artículos_en_revistas/Cao-2017-Data_Science:_A_Comprehensive_Overview.pdf},
	keywords = {Ciencia del dato},
	abstract = {The 21st century has ushered in the age of big data and data economy, in which
*data DNA*, which carries im- portant knowledge, insights, and potential, has
become an intrinsic constituent of all data-based organisms. An appropriate
understanding of data DNA and its organisms relies on the new field of *data
science* and its keystone, *analytics*. Although it is widely debated whether
big data is only hype and buzz, and data science is still in a very early
phase, significant challenges and opportunities are emerging or have been
inspired by the research, innovation, business, profession, and education of
data science. This article provides a com- prehensive survey and tutorial of
the fundamental aspects of data science: the evolution from data analysis to
data science, the data science concepts, a big picture of the era of data
science, the major challenges and directions in data innovation, the nature of
data analytics, new industrialization and service opportunities in the data
economy, the profession and competency of data education, and the future of
data science. This article is the first in the field to draw a comprehensive
big picture, in addition to offering rich observations, lessons, and thinking
about data science and analytics.},
	langidopts = {variant=british},
	langid = {english},
	pages = {43:1-43:42},
	doi = {10.1145/3076253},
	number = {3},
	volume = {50},
	language = {english},
	date = {2017},
	xdata = {acm_computing_surveys},
	title = {Data Science: A Comprehensive Overview},
	author = {Cao, Longbing}
}

@Book{cappe_et_al:inference_hidden_markov_models,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/0-387-28982-8},
	file = {Libros/Cappé_et_al-2005-Inference_in_Hidden_Markov_Models.pdf},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/978-0-387-40264-2},
	doi = {10.1007/0-387-28982-8},
	language = {english},
	title = {Inference in Hidden Markov Models},
	isbn = {9780387402642},
	series = {Springer Series in Statistics},
	abstract = {Hidden Markov models have become a widely used class of statistical models with applications in
diverse areas such as communications engineering, bioinformatics, finance and many more. This book
is a comprehensive treatment of inference for hidden Markov models, including both algorithms and
statistical theory. Topics range from filtering and smoothing of the hidden Markov chain to
parameter estimation, Bayesian methods and estimation of the number of states.

In a unified way the book covers both models with finite state spaces, which allow for exact
algorithms for filtering, estimation etc. and models with continuous state spaces (also called
state-space models) requiring approximate simulation-based algorithms that are also described in
detail. Simulation in hidden Markov models is addressed in five different chapters that cover both
Markov chain Monte Carlo and sequential Monte Carlo approaches. Many examples illustrate the
algorithms and theory. The book also carefully treats Gaussian linear state-space models and their
extensions and it contains a chapter on general Markov chain theory and probabilistic aspects of
hidden Markov models.

This volume will suit anybody with an interest in inference for stochastic processes, and it will
be useful for researchers and practitioners in areas such as statistics, signal processing,
communications engineering, control theory, econometrics, finance and more. The algorithmic parts
of the book do not require an advanced mathematical background, while the more theoretical parts
require knowledge of probability theory at the measure-theoretical level.},
	pagetotal = {653},
	publisher = Springer,
	author = {Cappé, Olivier and Moulines, Eric and Rydén, Tobias},
	date = {2005},
	keywords = {Modelos de Markov}
}

@book{castillo_et_al:sistemas_expertos_modelos_redes_probabilisticas,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2692134
Recurso electrónico (ebrary): http://0-site.ebrary.com.fama.us.es/lib/unisev/Doc?id=10467096},
	langid = {spanish},
	publisher = {Academia Española de Ingeniería},
	language = {spanish},
	date = {1996},
	title = {Sistemas Expertos y Modelos de Redes Probabilísticas},
	isbn = {9788460093954},
	pagetotal = {627},
	author = {Castillo, Enrique and Gutiérrez, José Manuel and Hadi, Ali S.},
	keywords = {Inteligencia artificial, Redes bayesianas},
	file = {Libros/Castillo_et_al-1996-Sistemas_Expertos_y_Modelos_de_Redes_Probabilísticas.pdf}
}

@Book{chambers:extending_R,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2793838
},
	keywords = {Lenguaje de programación R},
	abstract = {Written by John M. Chambers, the leading developer of the original S software,
__Extending R__ covers key concepts and techniques in R to support analysis and
research projects. It presents the core ideas of R, provides programming
guidance for projects of all scales, and introduces new, valuable techniques
that extend R.

The book first describes the fundamental characteristics and background of R,
giving readers a foundation for the remainder of the text. It next discusses
topics relevant to programming with R, including the apparatus that supports
extensions. The book then extends R’s data structures through object-oriented
programming, which is the key technique for coping with complexity. The book
also incorporates a new structure for interfaces applicable to a variety of
languages.

A reflection of what R is today, this guide explains how to design and organize
extensions to R by correctly using objects, functions, and interfaces. It
enables current and future users to add their own contributions and packages to
R.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Extending-R/Chambers/p/book/9781498775717},
	pagetotal = {364},
	isbn = {9781498775717},
	publisher = CRC,
	series = CRC_R_Series,
	language = {english},
	date = {2016},
	author = {Chambers, John M.},
	title = {Extending R}
}

@Book{chopard_tomassini:introduction_metaheuristics_optimization,
	keywords = {Metaheurísticas},
	abstract = {The authors stress the relative simplicity, efficiency, flexibility of use, and
suitability of various approaches used to solve difficult optimization
problems. The authors are experienced, interdisciplinary lecturers and
researchers and in their explanations they demonstrate many shared foundational
concepts among the key methodologies.

This textbook is a suitable introduction for undergraduate and graduate
students, researchers, and professionals in computer science, engineering, and
logistics.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/gp/book/9783319930725},
	doi = {10.1007/978-3-319-93073-2},
	pagetotal = {226},
	isbn = {9783319930725},
	publisher = Springer,
	series = Springer_Natural_Computing,
	language = {english},
	date = {2018},
	title = {An Introduction to Metaheuristics for Optimization},
	author = {Chopard, Bastien and Tomassini, Marco}
}

@Collection{clarke_et_al:handbook_model_checking,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013090791804987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	keywords = {Lógica matemática, Verificación de modelos},
	abstract = {Model checking is a computer-assisted method for the analysis of dynamical
systems that can be modeled by state-transition systems. Drawing from research
traditions in mathematical logic, programming languages, hardware design, and
theoretical computer science, model checking is now widely used for the
verification of hardware and software in industry.

The editors and authors of this handbook are among the world's leading
researchers in this domain, and the 32 contributed chapters present a thorough
view of the origin, theory, and application of model checking. In particular,
the editors classify the advances in this domain and the chapters of the
handbook in terms of two recurrent themes that have driven much of the research
agenda: the algorithmic challenge, that is, designing model-checking algorithms
that scale to real-life problems; and the modeling challenge, that is,
extending the formalism beyond Kripke structures and temporal logic.

The book will be valuable for researchers and graduate students engaged with
the development of formal methods and verification tools.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783319105741},
	doi = {10.1007/978-3-319-10575-8},
	pagetotal = {1210},
	isbn = {9783319105741},
	publisher = Springer,
	language = {english},
	date = {2018},
	title = {Handbook of Model Checking},
	editor = {Clarke, Edmund Melson and Henzinger, Thomas A. and Veith, Helmut and Bloem, Roderick}
}

@Book{cleveland:elements_graphing_data,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1496987
},
	keywords = {Visualización de datos},
	langidopts = {variant=british},
	langid = {english},
	pagetotal = {297},
	isbn = {9780963488411},
	publisher = Hobart,
	edition = "Revised Edition",
	language = {english},
	date = {1994},
	title = {The Elements of Graphing Data},
	author = {Cleveland, William Swain}
}

@Book{cleveland:visualizing_data,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1504807
},
	keywords = {Visualización de datos},
	langidopts = {variant=british},
	langid = {english},
	pagetotal = {360},
	isbn = {9780963488404},
	publisher = Hobart,
	language = {english},
	date = {1993},
	title = {Visualizing Data},
	author = {Cleveland, William Swain}
}

@XData{computers_communications_control,
	journaltitle = {International Journal of Computers Communications \& Control},
	journalsubtitle = {With Emphasis on the Integration of Three Technologies},
	issn = {1841-9836}
}

@Book{cook_swayne:interactive_dynamic_graphics_data_analysis,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1987556
Recurso electrónico (SpringerLink): http://www.us.debiblio.com/login?url=http://dx.doi.org/10.1007/978-0-387-71762-3
},
	file = {Libros/Cook_Swayne-2007-Interactive_and_Dynamic_Graphics_for_Data_Analysis.pdf},
	keywords = {Gráficos interactivos, Visualización de datos},
	abstract = {This richly illustrated book describes the use of interactive and dynamic
graphics as part of multidimensional data analysis. Chapters include
clustering, supervised classification, and working with missing values. A
variety of plots and interaction methods are used in each analysis, often
starting with brushing linked low-dimensional views and working up to manual
manipulation of tours of several variables. The role of graphical methods is
shown at each step of the analysis, not only in the early exploratory phase,
but in the later stages, too, when comparing and evaluating models.

All examples are based on freely available software: GGobi for interactive
graphics and R for static graphics, modeling, and programming. The printed book
is augmented by a wealth of material on the web, encouraging readers follow the
examples themselves. The web site has all the data and code necessary to
reproduce the analyses in the book, along with movies demonstrating the
examples.

The book may be used as a text in a class on statistical graphics or
exploratory data analysis, for example, or as a guide for the independent
learner. Each chapter ends with a set of exercises.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780387717616},
	doi = {10.1007/978-0-387-71762-3},
	pagetotal = {188},
	isbn = {9780387717616},
	publisher = Springer,
	series = Springer_Use_R,
	language = {english},
	subtitle = {With R and GGobi},
	date = {2007},
	title = {Interactive and Dynamic Graphics for Data Analysis},
	author = {Cook, Dianne and Swayne, Deborah F.}
}

@Book{cortez:modern_optimization_r,
	date = {2014},
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2667273
Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-3-319-08263-9},
	file = {Libros/Cortez-2014-Modern_Optimization_with_R.pdf},
	keywords = {Inteligencia artificial, Lenguaje de programación R, Metaheurísticas},
	abstract = {The goal of this book is to gather in a single document the most relevant concepts related to
modern optimization methods, showing how such concepts and methods can be addressed using the open
source, multi-platform R tool. Modern optimization methods, also known as metaheuristics, are
particularly useful for solving complex problems for which no specialized optimization algorithm
has been developed. These methods often yield high quality solutions with a more reasonable use of
computational resources (e.g. memory and processing effort). Examples of popular modern methods
discussed in this book are: simulated annealing; tabu search; genetic algorithms; differential
evolution; and particle swarm optimization. This book is suitable for undergraduate and graduate
students in Computer Science, Information Technology, and related areas, as well as data analysts
interested in exploring modern optimization methods using R.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783319082622},
	doi = {10.1007/978-3-319-08263-9},
	pagetotal = {188},
	isbn = {9783319082622},
	publisher = Springer,
	series = Springer_Use_R,
	language = {english},
	title = {Modern Optimization with R},
	author = {Cortez, Paulo}
}

@Book{cotton:testing_R_code,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2757658},
	keywords = {Lenguaje de programación R},
	abstract = {The problem with programming is that you are always one typo away from writing
something silly. Likewise with data analysis, a small mistake in your model can
lead to a big mistake in your results. Combining the two disciplines means that
it is all too easy for a missed minus sign to generate a false prediction that
you don’t spot until it’s too late. Testing is the only way to be sure that
your code, and your results, are correct.

__Testing R Code__ teaches you how to perform development-time testing using
the testthat package, allowing you to ensure that your code works as intended.
The book also teaches run-time testing using the assertive package; enabling
your users to correctly run your code.

After beginning with an introduction to testing in R, the book explores more
advanced cases such as integrating tests into R packages; testing code that
accesses databases; testing C++ code with Rcpp; and testing graphics. Each
topic is explained with real-world examples, and has accompanying exercises for
readers to practise their skills — only a small amount of experience with R is
needed to get started!},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Testing-R-Code/Cotton/p/book/9781498763653},
	pagetotal = {178},
	isbn = {9781498763653},
	publisher = CRC,
	series = CRC_R_Series,
	language = {english},
	date = {2017},
	title = {Testing R Code},
	author = {Cotton, Richard}
}

@Book{cuena:logica_informatica,
	keywords = {Lógica matemática},
	langid = {spanish},
	pagetotal = {552},
	isbn = {9788420686018},
	publisher = Alianza,
	language = {spanish},
	date = {1985},
	title = {Lógica informática},
	author = {Cuena Bartolomé, José}
}

@Book{cuevas_et_al:evolutionary_computation_techniques,
	file = {Libros/Cuevas_et_al-2017-Evolutionary_Computation_Techniques:_A_Comparative_Perspective.pdf},
	keywords = {Computación evolutiva, No disponible en la BUS},
	abstract = {This book compares the performance of various evolutionary computation (EC)
techniques when they are faced with complex optimization problems extracted
from different engineering domains. Particularly focusing on recently developed
algorithms, it is designed so that each chapter can be read independently.
Several comparisons among EC techniques have been reported in the literature,
however, they all suffer from one limitation: their conclusions are based on
the performance of popular evolutionary approaches over a set of synthetic
functions with exact solutions and well-known behaviors, without considering
the application context or including recent developments. In each chapter, a
complex engineering optimization problem is posed, and then a particular EC
technique is presented as the best choice, according to its search
characteristics. Lastly, a set of experiments is conducted in order to compare
its performance to other popular EC methods.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783319511092},
	doi = {10.1007/978-3-319-51109-2},
	pagetotal = {222},
	isbn = {9783319511085},
	publisher = Springer,
	number = {686},
	series = Springer_Computational_Intelligence,
	language = {english},
	date = {2017},
	title = {Evolutionary Computation Techniques: A Comparative Perspective},
	author = {Cuevas, Erik and Osuna, Valentín and Oliva, Diego Alberto}
}

@InProceedings{daly_shen:accelerate_learning_bayesian_network,
	file = {Contribuciones_a_congresos/Daly_Shen-2007-Methods_to_Accelerate_the_Learning_of_Bayesian_Network_Structures.pdf},
	keywords = {Redes bayesianas},
	abstract = {Bayesian networks have become a standard technique in the representation of uncertain knowledge.
This paper proposes methods that can accelerate the learning of a Bayesian network structure from a
data set. These methods are applicable when learning an equivalence class of Bayesian network
structures whilst using a score and search strategy. They work by constraining the number of
validity tests that need to be done and by caching the results of validity tests. The results of
experiments show that the methods improve the performance of algorithms that search through the
space of equivalence classes multiple times and that operate on wide data sets. The experiments
were performed by sampling data from six standard Bayesian networks and running an ant colony
optimization algorithm designed to learn a Bayesian network equivalence class.},
	langidopts = {variant=british},
	langid = {english},
	language = {english},
	booktitle = {Proceedings of the 2007 UK Workshop on Computational Intelligence},
	date = {2007},
	title = {Methods to Accelerate the Learning of Bayesian Network Structures},
	author = {Daly, Rónán and Shen, Qiang}
}

@book{darwiche:modeling_reasoning_bayesian_networks,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2068386},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.cambridge.org/core/books/modeling-and-reasoning-with-bayesian-networks/8A3769B81540EA93B525C4C2700C9DE6},
	doi = {10.1017/CBO9780511811357},
	language = {english},
	title = {Modeling and Reasoning with Bayesian Networks},
	isbn = {9780521884389},
	abstract = {This book is a thorough introduction to the formal foundations and practical applications of
Bayesian networks. It provides an extensive discussion of techniques for building Bayesian networks
that model real-world situations, including techniques for synthesizing models from design,
learning models from data, and debugging models using sensitivity analysis. It also treats exact
and approximate inference algorithms at both theoretical and practical levels. The treatment of
exact algorithms covers the main inference paradigms based on elimination and conditioning and
includes advanced methods for compiling Bayesian networks, time-space tradeoffs, and exploiting
local structure of massively connected networks. The treatment of approximate algorithms covers the
main inference paradigms based on sampling and optimization and includes influential algorithms
such as importance sampling, MCMC, and belief propagation. The author assumes very little
background on the covered subjects, supplying in-depth discussions for theoretically inclined
readers and enough practical details to provide an algorithmic cookbook for the system developer.},
	pagetotal = {548},
	publisher = Cambridge,
	author = {Darwiche, Adnan},
	date = {2009},
	keywords = {Inteligencia artificial, Redes bayesianas}
}

@Book{dathan_ramnath:object_oriented_analysis_design_implementation,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2705307},
	keywords = {Programación orientada a objetos},
	abstract = {The second edition of this textbook includes revisions based on the feedback on the first edition.
In a new chapter the authors provide a concise introduction to the remainder of UML diagrams,
adopting the same holistic approach as the first edition.

Using a case-study-based approach for providing a comprehensive introduction to the principles of
object-oriented design, it includes:

* A sound footing on object-oriented concepts such as classes, objects, interfaces, inheritance,
  polymorphism, dynamic linking, etc.
* A good introduction to the stage of requirements analysis
* Use of UML to document user requirements and design
* An extensive treatment of the design process
* Coverage of implementation issues
* Appropriate use of design and architectural patterns
* Introduction to the art and craft of refactoring
* Pointers to resources that further the reader's knowledge

The focus of the book is on implementation aspects, without which the learning is incomplete. This
is achieved through the use of case studies for introducing the various concepts of analysis and
design, ensuring that the theory is never separate from the implementation aspects.

All the main case studies used in this book have been implemented by the authors using Java. An
appendix on Java provides a useful short tutorial on the language.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/978-3-319-24278-1},
	doi = {10.1007/978-3-319-24280-4},
	pagetotal = {471},
	isbn = {9783319242781},
	publisher = Springer,
	series = Springer_Undergraduate_Topics,
	edition = {2},
	language = {english},
	date = {2015},
	subtitle = {An Integrated Approach},
	title = {Object-Oriented Analysis, Design and Implementation},
	author = {Dathan, Brahma and Ramnath, Sarnath}
}

@Book{de:api_management,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2793819
},
	keywords = {Programación},
	abstract = {Maximize the impact of your assets and business services by providing APIs for
developers and other users. The journey described in this book starts with
identifying business assets. As part of the API team, you then need to identify
and define the requirements of traffic management, security, mediation, and
orchestration. You also must define metrics for the analytics to measure the
success of the overall API program. API documentation and the ease of developer
onboarding also determine the success of the APIs. Finally, monetization of
these APIs leads to revenue generation for the enterprise.

Author De — an expert in building and managing API solutions — provides
enterprise architects, designers, and technologists with insight into the world
of APIs and the various technical aspects of building and managing an effective
API management solution. _API Management: Developing and Managing APIs for your
Organization_:

* Introduces the basics of APIs and highlights their value
* Provides an overview of technologies for building an API management solution
  and defines the requirements, including how to build a RESTful API
* Offers design principles for building developer-friendly APIs
* Explains how to secure your APIs
* Shows how to use API analytics to measure the success of your APIs
* Demonstrates how to monetize APIs 

Finally, _API Management_ touches on various technical nuances of creating,
distributing, and managing an API. This book will not only help you learn how
to design, build, deploy, and manage an API for an enterprise scale, but also
generate revenue for your organization.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781484213063},
	doi = {10.1007/978-1-4842-1305-6},
	pagetotal = {195},
	isbn = {9781484213063},
	publisher = Apress,
	language = {english},
	subtitle = {An Architect's Guide to Developing and Managing APIs for Your Organization},
	date = {2017},
	title = {API Management},
	author = {De, Brajesh}
}

@Book{diestel:graph_theory,
	file = {Libros/Diestel-2017-Graph_Theory.pdf},
	series = Springer_Graduate_Texts_Mathematics,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013068915504987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	keywords = {Teoría de grafos},
	abstract = {This standard textbook of modern graph theory, now in its fifth edition,
combines the authority of a classic with the engaging freshness of style that
is the hallmark of active mathematics. It covers the core material of the
subject with concise yet reliably complete proofs, while offering glimpses of
more advanced methods in each field by one or two deeper results, again with
proofs given in full detail.

The book can be used as a reliable text for an introductory course, as a
graduate text, and for self-study.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783662536216},
	pagetotal = {428},
	isbn = {9783662536216},
	publisher = Springer,
	number = {173},
	edition = {5},
	language = {english},
	date = {2017},
	title = {Graph Theory},
	author = {Diestel, Reinhard}
}

@Book{diez:iniciación_logica,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1575263},
	keywords = {Lógica matemática},
	langid = {spanish},
	pagetotal = {301},
	isbn = {9788434487640},
	publisher = Ariel,
	language = {spanish},
	date = {2002},
	title = {Iniciación a la lógica},
	author = {Díez Calzada, José A.}
}

@Book{doglio:mastering_python_high_performance,
	enlaces = {Recurso electrónico: http://0-proquest.safaribooksonline.com.fama.us.es//?uiCode=sevil&xmlId=9781783989300},
	keywords = {Lenguaje de programación Python},
	abstract = {Simply knowing how to code is not enough; on mission-critical pieces of code, every bit of memory
and every CPU cycle counts, and knowing how to squish every bit of processing power out of your
code is a crucial and sought-after skill. Nowadays, Python is used for many scientific projects,
and sometimes the calculations done in those projects require some serious fine-tuning. Profilers
are tools designed to help you measure the performance of your code and help you during the
optimization process, so knowing how to use them and read their output is very handy.

This book starts from the basics and progressively moves on to more advanced topics. You’ll learn
everything from profiling all the way up to writing a real-life application and applying a full set
of tools designed to improve it in different ways. In the middle, you’ll stop to learn about the
major profilers used in Python and about some graphic tools to help you make sense of their output.
You’ll then move from generic optimization techniques onto Python-specific ones, going over the
main constructs of the language that will help you improve your speed without much of a change.
Finally, the book covers some number-crunching-specific libraries and how to use them properly to
get the best speed out of them.

After reading this book, you will know how to take any Python code, profile it, find out where the
bottlenecks are, and apply different techniques to remove them.
},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.packtpub.com/application-development/mastering-python-high-performance},
	pagetotal = {260},
	isbn = {9781783989300},
	publisher = Packt,
	language = {english},
	date = {2015},
	subtitle = {Measure, optimize, and improve the performance of your Python code with this easy-to-follow guide},
	title = {Mastering Python High Performance},
	author = {Doglio, Fernando}
}

@Book{dooley:software_development_design_coding,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2835340
},
	keywords = {Programación},
	abstract = {Learn the principles of good software design, and how to turn those principles
into great code. This book introduces you to software engineering — from the
application of engineering principles to the development of software. You'll
see how to run a software development project, examine the different phases of
a project, and learn how to design and implement programs that solve specific
problems. It's also about code construction — how to write great programs and
make them work.

Whether you're new to programming or have written hundreds of applications, in
this book you'll re-examine what you already do, and you'll investigate ways to
improve. Using the Java language, you'll look deeply into coding standards,
debugging, unit testing, modularity, and other characteristics of good
programs. With Software Development, Design and Coding, author and professor
John Dooley distills his years of teaching and development experience to
demonstrate practical techniques for great coding.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781484231524},
	doi = {10.1007/978-1-4842-3153-1},
	pagetotal = {320},
	isbn = {9781484231524},
	publisher = Apress,
	edition = {2},
	language = {english},
	subtitle = {With Patterns, Debugging, Unit Testing, and Refactoring},
	date = {2017},
	title = {Software Development, Design and Coding},
	author = {Dooley, John F.}
}

@Book{dreo_et_al:metaheuristics_hard_optimization,
	subtitle = {Methods and Case Studies},
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1690625
Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/3-540-30966-7},
	file = {Libros/Dréo_et_al-2006-Metaheuristics_for_Hard_Optimization.pdf},
	keywords = {Inteligencia artificial, Metaheurísticas},
	abstract = {Metaheuristics for Hard Optimization comprises of three parts. The first part is devoted to the
detailed presentation of the four most widely known metaheuristics:

* the simulated annealing method,
* tabu search,
* the evolutionary algorithms,
* ant colony algorithms.

Each one of these metaheuristics is actually a family of methods, of which the essential elements
are discussed. In the second part, the book presents some other less widespread metaheuristics,
then, extensions of metaheuristics and some ways of research are described . The problem of the
choice of a metaheuristic is posed and solution methods are discussed. The last part concentrates
on three case studies from telecommunications, air traffic control, and vehicle routing.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783540230229},
	doi = {10.1007/3-540-30966-7},
	pagetotal = {372},
	isbn = {9783540230229},
	publisher = Springer,
	language = {english},
	date = {2006},
	title = {Metaheuristics for Hard Optimization},
	author = {Dréo, Johann and Pétrowski, Alain and Siarry, Patrick and Taillard, Eric}
}

@Book{du_swamy:neural_networks_statistical_learning,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-1-4471-5571-3},
	langidopts = {variant=english},
	langid = {english},
	url = {http://www.springer.com/978-1-4471-5570-6},
	doi = {10.1007/978-1-4471-5571-3},
	language = {english},
	title = {Neural Networks and Statistical Learning},
	isbn = {9781447155706},
	abstract = {Providing a broad but in-depth introduction to neural network and machine learning in a statistical
framework, this book provides a single, comprehensive resource for study and further research. All
the major popular neural network models and statistical learning approaches are covered with
examples and exercises in every chapter to develop a practical working understanding of the
content.

Each of the twenty-five chapters includes state-of-the-art descriptions and important research
results on the respective topics. The broad coverage includes the multilayer perceptron, the
Hopfield network, associative memory models, clustering models and algorithms, the radial basis
function network, recurrent neural networks, principal component analysis, nonnegative matrix
factorization, independent component analysis, discriminant analysis, support vector machines,
kernel methods, reinforcement learning, probabilistic and Bayesian networks, data fusion and
ensemble learning, fuzzy sets and logic, neurofuzzy models, hardware implementations, and some
machine learning topics. Applications to biometric/bioinformatics and data mining are also
included.

Focusing on the prominent accomplishments and their practical aspects, academic and technical
staff, graduate students and researchers will find that this provides a solid foundation and
encompassing reference for the fields of neural networks, pattern recognition, signal processing,
machine learning, computational intelligence, and data mining.},
	pagetotal = {824},
	publisher = Springer,
	author = {Du, Ke-Lin and Swamy, M. N. S.},
	date = {2014},
	keywords = {Aprendizaje automático, Redes neuronales},
	file = {Libros/Du_Swamy-2014-Neural_Networks_and_Statistical_Learning.pdf}
}

@Book{dürr_vie:competitive_programming_python,
	keywords = {Algoritmia, Lenguaje de programación Python, Libro solicitado, No disponible en la BUS},
	abstract = {Want to kill it at your job interview in the tech industry? Want to win that
coding competition? Learn all the algorithmic techniques and programming skills
you need from two experienced coaches, problem setters, and jurors for coding
competitions. The authors highlight the versatility of each algorithm by
considering a variety of problems and show how to implement algorithms in simple
and efficient code. Readers can expect to master 128 algorithms in Python and
discover the right way to tackle a problem and quickly implement a solution of
low complexity. Classic problems like Dijkstra's shortest path algorithm and
Knuth-Morris-Pratt's string matching algorithm are featured alongside lesser
known data structures like Fenwick trees and Knuth's dancing links. The book
provides a framework to tackle algorithmic problem solving, including:
Definition, Complexity, Applications, Algorithm, Key Information,
Implementation, Variants, In Practice, and Problems. Python code included in the
book and on the companion website.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.cambridge.org/es/academic/subjects/computer-science/algorithmics-complexity-computer-algebra-and-computational-g/competitive-programming-python-128-algorithms-develop-your-coding-skills},
	isbn = {9781108716826},
	publisher = Cambridge,
	language = {english},
	subtitle = {128 Algorithms to Develop your Coding Skills},
	date = {2020},
	title = {Competitive Programming in Python},
	author = {Dürr, Christoph and Vie, Jill-Jênn}
}

@XData{ecosistemas,
	journaltitle = {Ecosistemas},
	journalsubtitle = {Revista científica de ecología y medio ambiente},
	issn = {1697-2473}
}

@Book{edelkamp_schroedl:heuristic_search,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2705304},
	isbn = {9780123725127},
	publisher = Elsevier,
	keywords = {Búsqueda heurística, Inteligencia artificial},
	abstract = {Search has been vital to artificial intelligence from the very beginning as a core technique in
problem solving. The authors present a thorough overview of heuristic search with a balance of
discussion between theoretical analysis and efficient implementation and application to real-world
problems. Current developments in search such as pattern databases and search with efficient use of
external memory and parallel processing units on main boards and graphics cards are detailed.

Heuristic search as a problem solving tool is demonstrated in applications for puzzle solving, game
playing, constraint satisfaction and machine learning. While no previous familiarity with heuristic
search is necessary the reader should have a basic knowledge of algorithms, data structures, and
calculus. Real-world case studies and chapter ending exercises help to create a full and realized
picture of how search fits into the world of artificial intelligence and the one around us.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://store.elsevier.com/Heuristic-Search/Stefan-Edelkamp/isbn-9780123725127/},
	pagetotal = {712},
	language = {english},
	date = {2011},
	subtitle = {Theory and Applications},
	title = {Heuristic Search},
	author = {Edelkamp, Stefan and Schroedl, Stefan}
}

@Book{eiben_smith:introduction_evolutionary_computing,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2646429},
	keywords = {Computación evolutiva},
	abstract = {The overall structure of this new edition is three-tier: Part I presents the basics, Part II is
concerned with methodological issues, and Part III discusses advanced topics. In the second edition
the authors have reorganized the material to focus on problems, how to represent them, and then how
to choose and design algorithms for different representations. They also added a chapter on
problems, reflecting the overall book focus on problem-solvers, a chapter on parameter tuning,
which they combined with the parameter control and "how-to" chapters into a methodological part,
and finally a chapter on evolutionary robotics with an outlook on possible exciting developments in
this field.

The book is suitable for undergraduate and graduate courses in artificial intelligence and
computational intelligence, and for self-study by practitioners and researchers engaged with all
aspects of bioinspired design and optimization.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/gp/book/9783662448731},
	doi = {10.1007/978-3-662-44874-8},
	pagetotal = {287},
	isbn = {9783662448731},
	publisher = Springer,
	series = Springer_Natural_Computing,
	edition = {2},
	language = {english},
	date = {2015},
	title = {Introduction to Evolutionary Computing},
	author = {Eiben, A.E. and Smith, James E.}
}

@Book{einarsson:accuracy_reliability_scientific_computing,
	publisher = SIAM,
	series = SIAM_Software_Environments_Tools,
	file = {Libros/Einarsson-2005-Accuracy_and_Reliability_in_Scientific_Computing.pdf},
	keywords = {Algoritmia, Ciencias de la computación},
	abstract = {Numerical software is used to test scientific theories, design airplanes and
bridges, operate manufacturing lines, control power plants and refineries,
analyze financial derivatives, identify genomes, and provide the understanding
necessary to derive and analyze cancer treatments. Because of the high stakes
involved, it is essential that results computed using software be accurate,
reliable, and robust. Unfortunately, developing accurate and reliable
scientific software is notoriously difficult. This book investigates some of
the difficulties related to scientific computing and provides insight into how
to overcome them and obtain dependable results. The tools to assess existing
scientific applications are described, and a variety of techniques that can
improve the accuracy and reliability of newly developed applications is
discussed.

_Accuracy and Reliability in Scientific Computing_ can be considered a handbook
for improving the quality of scientific computing. It will help computer
scientists address the problems that affect software in general as well as the
particular challenges of numerical computation: approximations occurring at all
levels, continuous functions replaced by discretized versions, infinite
processes replaced by finite ones, and real numbers replaced by finite
precision numbers. Divided into three parts, it starts by illustrating some of
the difficulties in producing robust and reliable scientific software.
Well-known cases of failure are reviewed and the what and why of numerical
computations are considered. The second section describes diagnostic tools that
can be used to assess the accuracy and reliability of existing scientific
applications. In the last section, the authors describe a variety of techniques
that can be employed to improve the accuracy and reliability of newly developed
scientific applications. The authors of the individual chapters are
international experts, many of them members of the IFIP Working Group on
Numerical Software.

_Accuracy and Reliability in Scientific Computing_ contains condensed
information on the main features of six major programming languages -- Ada, C,
C++, Fortran, Java, and Python -- and the INTLAB toolbox of the MATLAB software
and the PRECISE toolbox of Fortran are discussed in detail. This book has an
accompanying website, http://www.nsc.liu.se/wg25/book/, with codes, links,
color versions of some illustrations, and additional material.},
	url = {https://www.nsc.liu.se/wg25/book/},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1137/1.9780898718157},
	pagetotal = {327},
	isbn = {978-0-89871-584-2},
	language = {english},
	editor = {Einarsson, Bo},
	date = {2005},
	title = {Accuracy and Reliability in Scientific Computing}
}

@Book{eisenstein:introduction_natural_language_processing,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/3enc2g/alma991013169487704987
},
	file = {Libros/Eisenstein-2019-Introduction_to_Natural_Language_Processing.pdf},
	keywords = {Procesamiento del lenguaje natural},
	abstract = {This textbook provides a technical perspective on natural language
processing—methods for building computer software that understands, generates,
and manipulates human language. It emphasizes contemporary data-driven
approaches, focusing on techniques from supervised and unsupervised machine
learning. The first section establishes a foundation in machine learning by
building a set of tools that will be used throughout the book and applying them
to word-based textual analysis. The second section introduces structured
representations of language, including sequences, trees, and graphs. The third
section explores different approaches to the representation and analysis of
linguistic meaning, ranging from formal logic to neural word embeddings. The
final section offers chapter-length treatments of three transformative
applications of natural language processing: information extraction, machine
translation, and text generation. End-of-chapter exercises include both
paper-and-pencil analysis and software implementation.

The text synthesizes and distills a broad and diverse research literature,
linking contemporary machine learning techniques with the field's linguistic
and computational foundations. It is suitable for use in advanced undergraduate
and graduate-level courses and as a reference for software engineers and data
scientists. Readers should have a background in computer programming and
college-level mathematics. After mastering the material presented, students
will have the technical skill to build and analyze novel natural language
processing systems and to understand the latest research in the field.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://mitpress.mit.edu/books/introduction-natural-language-processing},
	pagetotal = {536},
	isbn = {9780262042840},
	publisher = MIT_Press,
	language = {english},
	date = {2019},
	title = {Introduction to Natural Language Processing},
	author = {Eisenstein, Jacob}
}

@Book{erciyes:guide_graph_algorithms,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013144894504987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	isbn = {9783319732343},
	keywords = {Algoritmia, Teoría de grafos},
	abstract = {This clearly structured textbook/reference presents a detailed and
comprehensive review of the fundamental principles of sequential graph
algorithms, approaches for NP-hard graph problems, and approximation algorithms
and heuristics for such problems. The work also provides a comparative analysis
of sequential, parallel and distributed graph algorithms – including algorithms
for big data – and an investigation into the conversion principles between the
three algorithmic methods.

Topics and features: presents a comprehensive analysis of sequential graph
algorithms; offers a unifying view by examining the same graph problem from
each of the three paradigms of sequential, parallel and distributed algorithms;
describes methods for the conversion between sequential, parallel and
distributed graph algorithms; surveys methods for the analysis of large graphs
and complex network applications; includes full implementation details for the
problems presented throughout the text; provides additional supporting material
at an accompanying website.

This practical guide to the design and analysis of graph algorithms is ideal
for advanced and graduate students of computer science, electrical and
electronic engineering, and bioinformatics. The material covered will also be
of value to any researcher familiar with the basics of discrete mathematics,
graph theory and algorithms.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783319732343},
	doi = {10.1007/978-3-319-73235-0},
	pagetotal = {471},
	publisher = Springer,
	series = Springer_Computer_Science,
	language = {english},
	subtitle = {Sequential, Parallel and Distributed},
	date = {2018},
	title = {Guide to Graph Algorithms},
	author = {Erciyeş, Kayhan}
}

@book{evans_swartz:approximating_integrals_monte_carlo_deterministic_methods,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2523312
Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2649726},
	langidopts = {variant=british},
	langid = {english},
	url = {https://global.oup.com/academic/product/approximating-integrals-via-monte-carlo-and-deterministic-methods-9780198502784},
	language = {english},
	title = {Approximating Integrals via Monte Carlo and Deterministic Methods},
	isbn = {9780198502784},
	series = Oxford_Statistical_Science,
	abstract = {This book is designed to introduce graduate students and researchers to the primary methods useful
for approximating integrals. The emphasis is on those methods that have been found to be of
practical use, and although the focus is on approximating higher- dimensional integrals the
lower-dimensional case is also covered. Included in the book are asymptotic techniques, multiple
quadrature and quasi-random techniques as well as a complete development of Monte Carlo algorithms.
For the Monte Carlo section importance sampling methods, variance reduction techniques and the
primary Markov Chain Monte Carlo algorithms are covered. This book brings these various techniques
together for the first time, and hence provides an accessible textbook and reference for
researchers in a wide variety of disciplines.},
	pagetotal = {298},
	number = {20},
	publisher = Oxford,
	author = {Evans, Michael and Swartz, Timothy},
	date = {2000},
	keywords = {Estadística computacional, Métodos de Monte Carlo, Métodos numéricos}
}

@Book{fernandez-montoro:python_3_descubierto,
	publisher = RCLibros,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2455210},
	keywords = {Lenguaje de programación Python},
	langid = {spanish},
	url = {http://rclibros.es/producto/python-3/},
	pagetotal = {262},
	isbn = {9788493945046},
	language = {spanish},
	date = {2012},
	title = {Python 3 al descubierto},
	author = {Fernández Montoro, Arturo}
}

@Book{fernandez_et_al:learning_imbalanced_data_sets,
	keywords = {Aprendizaje automático, Ciencia del dato, No disponible en la BUS},
	abstract = {This book provides a general and comprehensible overview of imbalanced
learning. It contains a formal description of a problem, and focuses on its
main features, and the most relevant proposed solutions. Additionally, it
considers the different scenarios in Data Science for which the imbalanced
classification can create a real challenge.

This book stresses the gap with standard classification tasks by reviewing the
case studies and ad-hoc performance metrics that are applied in this area. It
also covers the different approaches that have been traditionally applied to
address the binary skewed class distribution. Specifically, it reviews
cost-sensitive learning, data-level preprocessing methods and algorithm-level
solutions, taking also into account those ensemble-learning solutions that
embed any of the former alternatives. Furthermore, it focuses on the extension
of the problem for multi-class problems, where the former classical methods are
no longer to be applied in a straightforward way.

This book also focuses on the data intrinsic characteristics that are the main
causes which, added to the uneven class distribution, truly hinders the
performance of classification algorithms in this scenario. Then, some notes on
data reduction are provided in order to understand the advantages related to
the use of this type of approaches.

Finally this book introduces some novel areas of study that are gathering a
deeper attention on the imbalanced data issue. Specifically, it considers the
classification of data streams, non-classical classification problems, and the
scalability related to Big Data. Examples of software libraries and modules to
address imbalanced classification are provided.

This book is highly suitable for technical professionals, senior undergraduate
and graduate students in the areas of data science, computer science and
engineering. It will also be useful for scientists and researchers to gain
insight on the current developments in this area of study, as well as future
research directions.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783319980737},
	doi = {10.1007/978-3-319-98074-4},
	pagetotal = {377},
	isbn = {9783319980737},
	publisher = Springer,
	language = {english},
	date = {2018},
	title = {Learning from Imbalanced Data Sets},
	author = {Fernández, Alberto and García, Salvador and Galar, Mikel and Prati, Ronaldo C. and Krawczyk, Bartosz and Herrera, Francisco

}
}

@Article{filippone_et_al:survey_kernel_spectral_methods_clustering,
	xdata = {pattern_recognition},
	file = {Artículos_en_revistas/Filippone_et_al-2008-A_survey_of_kernel_and_spectral_methods_for_clustering.pdf},
	keywords = {Análisis de grupos, Aprendizaje automático},
	abstract = {Clustering algorithms are a useful tool to explore data structures and have
been employed in many disciplines. The focus of this paper is the partitioning
clustering problem with a special interest in two recent approaches: kernel and
spectral methods. The aim of this paper is to present a survey of kernel and
spectral clustering methods, two approaches able to produce nonlinear
separating hypersurfaces between clusters. The presented kernel clustering
methods are the kernel version of many classical clustering algorithms, e.g.,
*K*-means, SOM and neural gas. Spectral clustering arise from concepts in
spectral graph theory and the clustering problem is configured as a graph cut
problem where an appropriate objective function has to be optimized. An
explicit proof of the fact that these two paradigms have the same objective is
reported since it has been proven that these two seemingly different approaches
have the same mathematical foundation. Besides, fuzzy kernel clustering methods
are presented as extensions of kernel *K*-means clustering algorithm.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1016/j.patcog.2007.05.018},
	pages = {176-190},
	number = {1},
	volume = {41},
	language = {english},
	date = {2008},
	title = {A survey of kernel and spectral methods for clustering},
	author = {Filippone, Maurizio and Camastra, Francesco and Masulli, Francesco and Rovetta, Stefano}
}

@Book{fitting:first_order_logic_automated_theorem_proving,
	isbn = {9781461275152},
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1286179},
	keywords = {Lógica matemática},
	abstract = {There are many kinds of books on formal logic. Some have philosophers as their intended audience,
some mathematicians, some computer scien­ tists. Although there is a common core to all such books,
they will be very different in emphasis, methods, and even appearance. This book is intended for
computer scientists. But even this is not precise. Within computer science formal logic turns up in
a number of areas, from pro­ gram verification to logic programming to artificial intelligence.
This book is intended for computer scientists interested in automated theo­ rem proving in
classical logic. To be more precise yet, it is essentially a theoretical treatment, not a how-to
book, although how-to issues are not neglected. This does not mean, of course, that the book will
be of no interest to philosophers or mathematicians. It does contain a thorough presentation of
formal logic and many proof techniques, and as such it contains all the material one would expect
to find in a course in formal logic covering completeness but, not incompleteness issues. The first
item to be addressed is, What are we talking about and why are we interested in it? We are
primarily talking about truth as used in mathematical discourse, and our interest in it is, or
should be, self­ evident. Truth is a semantic concept, so we begin with models and their
properties. These are used to define our subject.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781461275152},
	doi = {10.1007/978-1-4612-2360-3},
	pagetotal = {326},
	publisher = Springer,
	series = Springer_Computer_Science,
	edition = {2},
	language = {english},
	date = {1996},
	title = {First–Order Logic and Automated Theorem Proving},
	author = {Fitting, Melvin}
}

@Book{flach:machine_learning,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2563364},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.cambridge.org/9781107422223},
	language = {english},
	subtitle = {The Art and Science of Algorithms that Make Sense of Data},
	title = {Machine Learning},
	isbn = {9781107422223},
	abstract = {As one of the most comprehensive machine learning texts around, this book does justice to the
field's incredible richness, but without losing sight of the unifying principles. Peter Flach's
clear, example-based approach begins by discussing how a spam filter works, which gives an
immediate introduction to machine learning in action, with a minimum of technical fuss. Flach
provides case studies of increasing complexity and variety with well-chosen examples and
illustrations throughout. He covers a wide range of logical, geometric and statistical models and
state-of-the-art topics such as matrix factorisation and {ROC} analysis. Particular attention is
paid to the central role played by features. The use of established terminology is balanced with
the introduction of new and useful concepts, and summaries of relevant background material are
provided with pointers for revision if necessary. These features ensure Machine Learning will set a
new standard as an introductory textbook.},
	pagetotal = {409},
	publisher = Cambridge,
	author = {Flach, Peter A.},
	date = {2012},
	keywords = {Aprendizaje automático}
}

@Article{friedman_et_al:bayesian_network_classifiers,
	file = {Artículos_en_revistas/Friedman_et_al-1997-Bayesian_Network_Classifiers.pdf},
	keywords = {Aprendizaje automático, Inteligencia artificial, Redes bayesianas},
	abstract = {Recent work in supervised learning has shown that a surprisingly simple
Bayesian classifier with strong assumptions of independence among features,
called naive Bayes, is competitive with state-of-the-art classifiers such as
C4.5. This fact raises the question of whether a classifier with less
restrictive assumptions can perform even better. In this paper we evaluate
approaches for inducing classifiers from data, based on the theory of learning
Bayesian networks. These networks are factored representations of probability
distributions that generalize the naive Bayesian classifier and explicitly
represent statements about independence. Among these approaches we single out a
method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes,
yet at the same time maintains the computational simplicity (no search
involved) and robustness that characterize naive Bayes. We experimentally
tested these approaches, using problems from the University of California at
Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods
for feature selection.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1023/A:1007465528199},
	pages = {131-163},
	number = {2-3},
	volume = {29},
	language = {english},
	date = {1997},
	xdata = {machine_learning},
	title = {Bayesian Network Classifiers},
	author = {Friedman, Nir and Geiger, Dan and Goldszmidt, Moises}
}

@Book{friendly_meyer:discrete_data_analysis_r,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2715358},
	keywords = {Lenguaje de programación R, Visualización de datos},
	abstract = {__Discrete Data Analysis with R: Visualization and Modeling Techniques for Categorical and Count
Data__ presents an applied treatment of modern methods for the analysis of categorical data, both
discrete response data and frequency data. It explains how to use graphical methods for exploring
data, spotting unusual features, visualizing fitted models, and presenting results.

The book is designed for advanced undergraduate and graduate students in the social and health
sciences, epidemiology, economics, business, statistics, and biostatistics as well as researchers,
methodologists, and consultants who can use the methods with their own data and analyses. Along
with describing the necessary statistical theory, the authors illustrate the practical application
of the techniques to a large number of substantive problems, including how to organize data,
conduct an analysis, produce informative graphs, and evaluate what the graphs reveal about the
data.

The first part of the book contains introductory material on graphical methods for discrete data,
basic R skills, and methods for fitting and visualizing one-way discrete distributions. The second
part focuses on simple, traditional nonparametric tests and exploratory methods for visualizing
patterns of association in two-way and larger frequency tables. The final part of the text
discusses model-based methods for the analysis of discrete data.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/9781498725835},
	pagetotal = {544},
	isbn = {9781498725835},
	publisher = CRC,
	series = CRC_Statistical_Science,
	language = {english},
	date = {2015},
	subtitle = {Visualization and Modeling Techniques for Categorical and Count Data},
	title = {Discrete Data Analysis with R},
	author = {Friendly, Michael and Meyer, David}
}

@Book{gamerman_lopes:markov_chain_monte_carlo,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2152028
},
	keywords = {Modelos de Markov, Métodos de Monte Carlo},
	abstract = {While there have been few theoretical contributions on the Markov Chain Monte
Carlo (MCMC) methods in the past decade, current understanding and application
of MCMC to the solution of inference problems has increased by leaps and
bounds. Incorporating changes in theory and highlighting new applications,
**Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference,
Second Edition** presents a concise, accessible, and comprehensive introduction
to the methods of this valuable simulation technique. The second edition
includes access to an internet site that provides the code, written in R and
WinBUGS, used in many of the previously existing and new examples and
exercises. More importantly, the self-explanatory nature of the codes will
enable modification of the inputs to the codes and variation on many directions
will be available for further exploration.

Major changes from the previous edition:

* More examples with discussion of computational details in chapters on Gibbs
  sampling and Metropolis-Hastings algorithms
* Recent developments in MCMC, including reversible jump, slice sampling,
  bridge sampling, path sampling, multiple-try, and delayed rejection
* Discussion of computation using both R and WinBUGS
* Additional exercises and selected solutions within the text, with all data
  sets and software available for download from the Web
* Sections on spatial models and model adequacy

The self-contained text units make MCMC accessible to scientists in other
disciplines as well as statisticians. The book will appeal to everyone working
with MCMC techniques, especially research and graduate statisticians and
biostatisticians, and scientists handling data and formulating models. The book
has been substantially reinforced as a first reading of material on MCMC and,
consequently, as a textbook for modern Bayesian computation and Bayesian
inference courses.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Markov-Chain-Monte-Carlo-Stochastic-Simulation-for-Bayesian-Inference/Gamerman-Lopes/p/book/9781584885870},
	pagetotal = {342},
	isbn = {9781584885870},
	publisher = CRC,
	series = CRC_Statistical_Science,
	edition = {2},
	language = {english},
	subtitle = {Stochastic Simulation for Bayesian Inference},
	date = {2006},
	title = {Markov Chain Monte Carlo},
	author = {Gamerman, Dani and Lopes, Hedibert F.}
}

@Proceedings{gamez_puerta:sistemas_expertos_probabilísticos,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1408263},
	file = {Actas_de_congresos/Gámez-Martín_Puerta-Callejón-1998-Sistemas_expertos_probabilísticos.pdf},
	keywords = {Inteligencia artificial, Redes bayesianas},
	abstract = {Un Sistema Experto es una herramienta informática que es capaz de simular el comportamiento de un
experto humano en una materia especializada. Un problema clave en el desarrollo de sistemas
expertos es encontrar la forma de representar y usar el conocimiento que los expertos humanos en
esa materia poseen y utilizan. Este problema se hace más difícil por el hecho de que. En muchos
campos, el conocimiento de los expertos es a menudo impreciso o incierto y. sin embargo, los
expertos son capaces de llegar a conclusiones útiles. Por tanto, todo sistema experto que pretenda
razonar 'como si' lo hiciese un ser humano debe ser capaz de trabajar con este tipo de información.
Uno de los formalismos mas potentes y mejor desarrollados para el tratamiento del conocimiento
incierto es la Teoría de la Probabilidad, que nos permite medir la creencia que tenemos en la
ocurrencia de un determinado suceso. Este libro recoge los trabajos presentados en el VIII Curso de
Verano de Informática: Sistemas Expertos Probabilísticos. Por parte de un grupo de relevantes
investigadores nacionales en el tema.},
	langid = {spanish},
	url = {https://ruidera.uclm.es/xmlui/handle/10578/6096},
	pagetotal = {318},
	isbn = {9788489958351},
	publisher = {Servicio de Publicaciones de la Universidad de Castilla-La Mancha},
	organization = {Departamento de Informática de la Escuela Universitaria Politécnica de Albacete},
	language = {spanish},
	date = {1998},
	title = {Sistemas expertos probabilísticos},
	editor = {Gámez Martín, José Antonio and Puerta Callejón, José Miguel}
}

@book{gandrud:reproducible_research_r_r_studio,
	langidopts = {variant=british},
	langid = {english},
	url = {http://christophergandrud.github.io/RepResR-RStudio/},
	language = {english},
	edition = {2},
	title = {Reproducible Research with R and R Studio},
	isbn = {9781498715379},
	series = CRC_R_Series,
	abstract = {*All the Tools for Gathering and Analyzing Data and Presenting Results*

**Reproducible Research with R and RStudio, Second Edition** brings together the skills and tools
needed for doing and presenting computational research. Using straightforward examples, the book
takes you through an entire reproducible research workflow. This practical workflow enables you to
gather and analyze data as well as dynamically present results in print and on the web.

*New to the Second Edition*

* The rmarkdown package that allows you to create reproducible research documents in PDF, HTML, and
  Microsoft Word formats using the simple and intuitive Markdown syntax
* Improvements to RStudio’s interface and capabilities, such as its new tools for handling R
  Markdown documents
* Expanded knitr R code chunk capabilities
* The kable function in the knitr package and the texreg package for dynamically creating tables to
  present your data and statistical results
* An improved discussion of file organization, enabling you to take full advantage of relative file
  paths so that your documents are more easily reproducible across computers and systems
* The dplyr, magrittr, and tidyr packages for fast data manipulation
* Numerous modifications to R syntax in user-created packages
* Changes to GitHub’s and Dropbox’s interfaces

*Create Dynamic and Highly Reproducible Research*

This updated book provides all the tools to combine your research with the presentation of your
findings. It saves you time searching for information so that you can spend more time actually
addressing your research questions. Supplementary files used for the examples and a reproducible
research project are available on the author’s website.},
	pagetotal = {323},
	publisher = {CRC},
	author = {Gandrud, Christopher},
	date = {2015},
	keywords = {Investigación reproducible, Lenguaje de programación R, No disponible en la BUS},
	file = {Libros/Gandrud-2015-Reproducible_Research_with_R_and_R_Studio.pdf}
}

@Book{gendreau_potvin:handbook_metaheuristics,
	series = Springer_Operations_Management,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-1-4419-1665-5},
	file = {Libros/Gendreau_Potvin-2010-Handbook_of_Metaheuristics.pdf},
	keywords = {Inteligencia artificial, Metaheurísticas},
	abstract = {“… an excellent book if you want to learn about a number of individual metaheuristics." (U.
Aickelin, Journal of the Operational Research Society, Issue 56, 2005, on the First Edition) The
first edition of the Handbook of Metaheuristics was published in 2003 under the editorship of Fred
Glover and Gary A. Kochenberger. Given the numerous developments observed in the field of
metaheuristics in recent years, it appeared that the time was ripe for a second edition of the
Handbook. When Glover and Kochenberger were unable to prepare this second edition, they suggested
that Michel Gendreau and Jean-Yves Potvin should take over the editorship, and so this important
new edition is now available. Through its 21 chapters, this second edition is designed to provide a
broad coverage of the concepts, implementations and applications in this important field of
optimization. Original contributors either revised or updated their work, or provided entirely new
chapters. The Handbook now includes updated chapters on the best known metaheuristics, including
simulated annealing, tabu search, variable neighborhood search, scatter search and path relinking,
genetic algorithms, memetic algorithms, genetic programming, ant colony optimization, multi-start
methods, greedy randomized adaptive search procedure, guided local search, hyper-heuristics and
parallel metaheuristics. It also contains three new chapters on large neighborhood search,
artificial immune systems and hybrid metaheuristics. The last four chapters are devoted to more
general issues related to the field of metaheuristics, namely reactive search, stochastic search,
fitness landscape analysis and performance comparison.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781441916631},
	doi = {10.1007/978-1-4419-1665-5},
	pagetotal = {648},
	isbn = {9781441916631},
	publisher = Springer,
	number = {146},
	edition = {2},
	language = {english},
	editor = {Gendreau, Michel and Potvin, Jean-Yves},
	date = {2010},
	title = {Handbook of Metaheuristics}
}

@book{gentle:computational_statistics,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-0-387-98144-4},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780387981437},
	doi = {10.1007/978-0-387-98144-4},
	language = {english},
	title = {Computational Statistics},
	isbn = {9780387981437},
	series = Springer_Statistics_Computing,
	abstract = {Computational inference has taken its place alongside asymptotic inference and exact techniques in
the standard collection of statistical methods. Computational inference is based on an approach to
statistical methods that uses modern computational power to simulate distributional properties of
estimators and test statistics. This book describes computationally-intensive statistical methods
in a unified presentation, emphasizing techniques, such as the {PDF} decomposition, that arise in a
wide range of methods.

The book assumes an intermediate background in mathematics, computing, and applied and theoretical
statistics. The first part of the book, consisting of a single long chapter, reviews this
background material while introducing computationally-intensive exploratory data analysis and
computational inference.

The six chapters in the second part of the book are on statistical computing. This part describes
arithmetic in digital computers and how the nature of digital computations affects algorithms used
in statistical methods. Building on the first chapters on numerical computations and algorithm
design, the following chapters cover the main areas of statistical numerical analysis, that is,
approximation of functions, numerical quadrature, numerical linear algebra, solution of nonlinear
equations, optimization, and random number generation.

The third and fourth parts of the book cover methods of computational statistics, including Monte
Carlo methods, randomization and cross validation, the bootstrap, probability density estimation,
and statistical learning.

The book includes a large number of exercises with some solutions provided in an appendix.},
	pagetotal = {728},
	publisher = Springer,
	author = {Gentle, James E.},
	date = {2009},
	keywords = {Álgebra lineal, Estadística computacional, Estadística no paramétrica, Estimación de densidades, Métodos de Monte Carlo, Métodos de regresión, Métodos de remuestreo, Minería de datos, Optimización matemática},
	file = {Libros/Gentle-2009-Computational_Statistics.pdf}
}

@Book{gentle_et_al:handbook_computational_statistics,
	series = Springer_Computational_Statistics,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-3-642-21551-3
},
	file = {Libros/Gentle_et_al-2012-Handbook_of_Computational_Statistics.pdf},
	keywords = {Estadística computacional},
	abstract = {The Handbook of Computational Statistics - Concepts and Methods (second
edition) is a revision of the first edition published in 2004, and contains
additional comments and updated information on the existing chapters, as well
as three new chapters addressing recent work in the field of computational
statistics. This new edition is divided into 4 parts in the same way as the
first edition. It begins with "How Computational Statistics became the backbone
of modern data science" (Ch.1): an overview of the field of Computational
Statistics, how it emerged as a separate discipline, and how its own
development mirrored that of hardware and software, including a discussion of
current active research. The second part (Chs. 2 - 15) presents several topics
in the supporting field of statistical computing. Emphasis is placed on the
need for fast and accurate numerical algorithms, and some of the basic
methodologies for transformation, database handling, high-dimensional data and
graphics treatment are discussed. The third part (Chs. 16 - 33) focuses on
statistical methodology. Special attention is given to smoothing, iterative
procedures, simulation and visualization of multivariate data. Lastly, a set of
selected applications (Chs. 34 - 38) like Bioinformatics, Medical Imaging,
Finance, Econometrics and Network Intrusion Detection highlight the usefulness
of computational statistics in real-world applications.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783642215506},
	doi = {10.1007/978-3-642-21551-3},
	pagetotal = {1192},
	isbn = {9783642215506},
	publisher = Springer,
	edition = {2},
	language = {english},
	subtitle = {Concepts and Methods},
	date = {2012},
	title = {Handbook of Computational Statistics},
	editor = {Gentle, James E. and Härdle, Wolfgang Karl and Mori, Yuichi}
}

@Book{ghallab_et_al:automated_planning,
	subtitle = {Theory and Practice},
	file = {Libros/Ghallab_et_al-2004-Automated_Planning.pdf; Libros/Ghallab_et_al-2004-Automated_Planning-Errata.pdf},
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1633381
Recurso electrónico (ScienceDirect): http://0-www.sciencedirect.com.fama.us.es/science/book/9781558608566
Recurso electrónico (E-Libro): http://0-site.ebrary.com.fama.us.es/lib/unisev/Doc?id=10226616
},
	abstract = {Automated planning technology now plays a significant role in a variety of demanding applications,
ranging from controlling space vehicles and robots to playing the game of bridge. These real-world
applications create new opportunities for synergy between theory and practice: observing what works
well in practice leads to better theories of planning, and better theories lead to better
performance of practical applications. Automated Planning mirrors this dialogue by offering a
comprehensive, up-to-date resource on both the theory and practice of automated planning. The book
goes well beyond classical planning, to include temporal planning, resource scheduling, planning
under uncertainty, and modern techniques for plan generation, such as task decomposition,
propositional satisfiability, constraint satisfaction, and model checking. The authors combine over
30 years experience in planning research and development to offer an invaluable text to
researchers, professionals, and graduate students.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://projects.laas.fr/planning/},
	author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
	date = {2004},
	isbn = {9781558608566},
	keywords = {Inteligencia artificial, Planificación automática},
	language = {english},
	pagetotal = {635},
	publisher = Morgan-Kaufmann,
	title = {Automated Planning}
}

@Book{ghallab_et_al:automated_planning_acting,
	file = {Libros/Ghallab_et_al-2016-Automated_Planning_and_Acting.pdf},
	keywords = {Inteligencia artificial, Planificación automática},
	abstract = {Autonomous AI systems need complex computational techniques for planning and performing actions.
Planning and acting require significant deliberation because an intelligent system must coordinate
and integrate these activities in order to act effectively in the real world. This book presents a
comprehensive paradigm of planning and acting using the most recent and advanced automated-planning
techniques. It explains the computational deliberation capabilities that allow an actor, whether
physical or virtual, to reason about its actions, choose them, organize them purposefully, and act
deliberately to achieve an objective. Useful for students, practitioners, and researchers, this
book covers state-of-the-art planning techniques, acting techniques, and their integration which
will allow readers to design intelligent systems that are able to act effectively in the real
world.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.cambridge.org/us/academic/subjects/computer-science/artificial-intelligence-and-natural-language-processing/automated-planning-and-acting},
	isbn = {9781107037274},
	publisher = Cambridge,
	language = {english},
	date = {2016},
	title = {Automated Planning and Acting},
	author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo}
}

@Collection{gheorghe_et_al:multidisciplinary_creativity,
	keywords = {Ciencias de la computación, Computación bioinspirada, Computación con membranas},
	langidopts = {variant=british},
	langid = {english},
	pagetotal = {334},
	isbn = {9786068401638},
	publisher = {Spandugino},
	language = {english},
	subtitle = {Homage to Gheorghe Păun on His 65th Birthday},
	editor = {Gheorghe, Marian and Petre, Ion and Pérez-Jiménez, Mario J. and Rozenberg, Grzegorz and Salomaa, Arto},
	date = {2015},
	title = {Multidisciplinary Creativity}
}

@Book{goldberg:neural_network_methods_natural_language_processing,
	series = MC_Human_Language_Technologies,
	publisher = Morgan_Claypool,
	file = {Libros/Goldberg-2017-Neural_Network_Methods_for_Natural_Language_Processing.pdf},
	keywords = {Procesamiento del lenguaje natural, Redes neuronales},
	abstract = {Neural networks are a family of powerful machine learning models. This book
focuses on the application of neural network models to natural language data.
The first half of the book (Parts I and II) covers the basics of supervised
machine learning and feed-forward neural networks, the basics of working with
machine learning over language data, and the use of vector-based rather than
symbolic representations for words. It also covers the computation-graph
abstraction, which allows to easily define and train arbitrary neural networks,
and is the basis behind the design of contemporary neural network software
libraries.

The second part of the book (Parts III and IV) introduces more specialized
neural network architectures, including 1D convolutional neural networks,
recurrent neural networks, conditioned-generation models, and attention-based
models. These architectures and techniques are the driving force behind
state-of-the-art algorithms for machine translation, syntactic parsing, and
many other applications. Finally, we also discuss tree-shaped networks,
structured prediction, and the prospects of multi-task learning.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.2200/S00762ED1V01Y201703HLT037},
	pagetotal = {309},
	isbn = {9781627052986},
	language = {english},
	date = {2017},
	title = {Neural Network Methods for Natural Language Processing},
	author = {Goldberg, Yoav}
}

@Proceedings{gong_et_al:bio_inspired_computing_theories_applications,
	enlaces = {Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-3-662-49014-3
},
	venue = {Hefei, China},
	eventtitleaddon = {BIC-TA 2015},
	eventdate = {2015-09-25/2015-09-28},
	eventtitle = {10th International Conference on Bio-Inspired Computing: Theories and Applications},
	file = {Actas_de_congresos/Gong_et_al-2015-Bio-Inspired_Computing-Theories_and_Applications.pdf},
	keywords = {Computación bioinspirada},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/gp/book/9783662490136},
	doi = {10.1007/978-3-662-49014-3},
	pagetotal = {727},
	isbn = {9783662490136},
	publisher = Springer,
	series = {Communications in Computer and Information Science},
	volume = {562},
	language = {english},
	date = {2015},
	title = {Bio-Inspired Computing – Theories and Applications},
	editor = {Gong, Maoguo and Pan, Linqiang and Song, Tao and Tang, Ke and Zhang, Xingyi}
}

@Book{goodfellow_et_al:deep_learning,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/3enc2g/alma991012440839704987
},
	file = {Libros/Goodfellow_et_al-2016-Deep_Learning.pdf},
	keywords = {Aprendizaje automático, Aprendizaje profundo, Redes neuronales},
	abstract = {Deep learning is a form of machine learning that enables computers to learn
from experience and understand the world in terms of a hierarchy of concepts.
Because the computer gathers knowledge from experience, there is no need for a
human computer operator to formally specify all the knowledge that the computer
needs. The hierarchy of concepts allows the computer to learn complicated
concepts by building them out of simpler ones; a graph of these hierarchies
would be many layers deep. This book introduces a broad range of topics in deep
learning.

The text offers mathematical and conceptual background, covering relevant
concepts in linear algebra, probability theory and information theory,
numerical computation, and machine learning. It describes deep learning
techniques used by practitioners in industry, including deep feedforward
networks, regularization, optimization algorithms, convolutional networks,
sequence modeling, and practical methodology; and it surveys such applications
as natural language processing, speech recognition, computer vision, online
recommendation systems, bioinformatics, and videogames. Finally, the book
offers research perspectives, covering such theoretical topics as linear factor
models, autoencoders, representation learning, structured probabilistic models,
Monte Carlo methods, the partition function, approximate inference, and deep
generative models.

_Deep Learning_ can be used by undergraduate or graduate students planning
careers in either industry or research, and by software engineers who want to
begin using deep learning in their products or platforms. A website offers
supplementary material for both readers and instructors.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://mitpress.mit.edu/books/deep-learning; http://www.deeplearningbook.org/},
	pagetotal = {800},
	isbn = {9780262035613},
	publisher = MIT_Press,
	series = Adaptive_Computation,
	language = {english},
	date = {2016},
	title = {Deep Learning},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron}
}

@Book{goodrich_tamassia:algorithm_design_applications,
	file = {Libros/Goodrich_Tamassia-2015-Algorithm_Design_and_Applications.pdf},
	keywords = {Algoritmia, No disponible en la BUS},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.wiley.com/en-es/Algorithm+Design+and+Applications-p-x000595611},
	pagetotal = {804},
	isbn = {9781118335918},
	publisher = Wiley,
	language = {english},
	date = {2015},
	title = {Algorithm Design and Applications},
	author = {Goodrich, Michael T. and Tamassia, Roberto}
}

@Collection{graciani_et_al:enjoying_natural_computing,
	subseries = Springer_Theoretical_Computer_Science,
	number = {11270},
	series = LNCS,
	file = {Libros/Graciani_et_al-2018-Enjoying_Natural_Computing.pdf},
	keywords = {Ciencias de la computación, Computación bioinspirada, Festschrift},
	abstract = {This Festschrift is in honor of Mario de Jesús Pérez-Jiménez, Professor in the
Department of Computer Science of University of Seville, Spain, on the occasion
of his 70th birthday. The title of this volume reflects both his main research
area, viz., Natural Computing, and the guiding principle of his functioning:
“once you choose to do something, enjoy doing it".

The respect that Professor Mario de Jesús Pérez-Jiménez enjoys in the
scientific community was well demonstrated by the enthusiastic response
received to the request to contribute to this book. The contributions by more
than 70 authors from 15 countries cover a wide spectrum of research areas and
reflect well the broad range of research interests of Professor Mario de Jesús
Pérez-Jiménez.

The research areas presented in this Festschrift include membrane computing,
spiking neural networks, phylogenetic networks, ant colonies optimization,
workbench for biocomputing, reaction systems, entropy of computation, rewriting
systems, and insertion-deletion systems.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783030002640},
	doi = {10.1007/978-3-030-00265-7},
	pagetotal = {331},
	isbn = {9783030002640},
	publisher = Springer,
	language = {english},
	subtitle = {Essays Dedicated to Mario de Jesús Pérez-Jiménez on the Occasion of His 70th Birthday},
	date = {2018},
	title = {Enjoying Natural Computing},
	editor = {Graciani, Carmen and Riscos-Núñez, Agustín and Păun, Gheorghe and Rozenberg, Grzegorz and Salomaa, Arto}
}

@Book{grolemund:hands-on_programming_r,
	file = {Libros/Grolemund-2014-Hands-On_Programming_with_R.pdf},
	keywords = {Lenguaje de programación R, No disponible en la BUS},
	abstract = {Learn how to program by diving into the R language, and then use your newfound skills to solve
practical data science problems. With this book, you’ll learn how to load data, assemble and
disassemble data objects, navigate R’s environment system, write your own functions, and use all of
R’s programming tools.

RStudio Master Instructor Garrett Grolemund not only teaches you how to program, but also shows you
how to get more from R than just visualizing and modeling data. You’ll gain valuable programming
skills and support your work as a data scientist at the same time.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://shop.oreilly.com/product/0636920028574.do},
	pagetotal = {250},
	isbn = {9781449359010},
	publisher = OReilly,
	language = {english},
	date = {2014},
	subtitle = {Write Your Own Functions and Simulations},
	title = {Hands-On Programming with R},
	author = {Grolemund, Garret}
}

@Collection{gutin_punnen:traveling_salesman_problem_variations,
	series = Springer_Combinatorial_Optimization,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1924759
Recurso electrónico (SpringerLink): http://www.us.debiblio.com/login?url=https://doi.org/10.1007/b101971
Recurso electrónico (E-Libro): http://www.us.debiblio.com/login?url=http://site.ebrary.com/lib/unisev/Doc?id=10067445
},
	file = {Libros/Gutin_Punnen-2007-The_Traveling_Salesman_Problem_and_its_Variations.pdf},
	keywords = {NP-completitud},
	abstract = {This volume, which contains chapters written by reputable researchers, provides
the state of the art in theory and algorithms for the traveling salesman
problem (TSP). The book covers all important areas of study on TSP, including
polyhedral theory for symmetric and asymmetric TSP, branch and bound, and
branch and cut algorithms, probabilistic aspects of TSP, thorough computational
analysis of heuristic and metaheuristic algorithms, theoretical analysis of
approximation algorithms, including the emerging area of domination analysis of
algorithms, discussion of TSP software and variations of TSP such as bottleneck
TSP, generalized TSP, prize collecting TSP, maximizing TSP, orienteering
problem, etc.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9781402006647},
	doi = {10.1007/b101971},
	pagetotal = {830},
	isbn = {9781402006647},
	publisher = Springer,
	number = {12},
	language = {english},
	date = {2007},
	title = {The Traveling Salesman Problem and Its Variations},
	editor = {Gutin, Gregory and Punnen, Abraham P.}
}

@Book{hackeling:mastering_machine_learning,
	enlaces = {Recurso electrónico (Safari Books Online): http://0-proquest.safaribooksonline.com.fama.us.es/?uiCode=sevil&xmlId=9781783988365},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.packtpub.com/big-data-and-business-intelligence/mastering-machine-learning-scikit-learn},
	language = {english},
	subtitle = {Apply effective learning algorithms to real-world problems using scikit-learn},
	title = {Mastering Machine Learning With scikit-learn},
	isbn = {9781783988365},
	abstract = {This book examines machine learning models including logistic regression, decision trees, and
support vector machines, and applies them to common problems such as categorizing documents and
classifying images. It begins with the fundamentals of machine learning, introducing you to the
supervised-unsupervised spectrum, the uses of training and test data, and evaluating models. You
will learn how to use generalized linear models in regression problems, as well as solve problems
with text and categorical features.

You will be acquainted with the use of logistic regression, regularization, and the various loss
functions that are used by generalized linear models. The book will also walk you through an
example project that prompts you to label the most uncertain training examples. You will also use
an unsupervised Hidden Markov Model to predict stock prices.

By the end of the book, you will be an expert in scikit-learn and will be well versed in machine
learning.},
	pagetotal = {238},
	publisher = Packt,
	author = {Hackeling, Gavin},
	date = {2014},
	keywords = {Aprendizaje automático, Lenguaje de programación Python}
}

@Book{hall_stacey:python_3_absolute_beginners,
	file = {Libros/Hall_Stacey-2009-Python_3_for_Absolute_Beginners.pdf},
	publisher = Apress,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-1-4302-1633-9},
	keywords = {Lenguaje de programación Python},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.apress.com/9781430216322},
	doi = {10.1007/978-1-4302-1633-9},
	pagetotal = {295},
	isbn = {9781430216322},
	language = {english},
	date = {2009},
	title = {Python 3 for Absolute Beginners},
	author = {Hall, Tim and Stacey, J-P}
}

@Book{hampton-smith:pro_css3_layout_techniques,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2715365},
	keywords = {Desarrollo web},
	abstract = {This books demonstrates the freshest cutting-edge layout tools found within CSS3, teaching you the
skills you’ll need to create advanced design patterns for websites and apps.

_Pro CSS3 Layout Techniques_ teaches you how to make the most of CSS3’s existing specification,
including those parts of the specification already widely implemented, as well as the upcoming
modules that are still being developed by the W3C. After reading this book you’ll be able to
confidently develop sophisticated, flexible layouts that aren't possible with CSS2.1.

CSS1 allowed designers to separate content from presentation for the first time and CSS2 cemented
support for advanced typographical control, but neither specification provided more than
rudimentary layout control. CSS3’s latest additions allow designers to craft fully responsive,
sophisticated layouts without the need for complex scripts or smoke-and-mirror workarounds.

CSS3 is still in active development, with browser vendors racing against each other to implement
the latest recommendations from the W3C. _Pro CSS3 Layout Techniques_ will help you cut through the
waffle and get straight to the heart of what works now, while showing you how to be ready for the
future of CSS!},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.apress.com/9781430265023},
	doi = {10.1007/978-1-4302-6503-0},
	pagetotal = {183},
	isbn = {9781430265023},
	publisher = Apress,
	language = {english},
	date = {2016},
	title = {Pro CSS3 Layout Techniques},
	author = {Hampton-Smith, Sam}
}

@Book{haroon:python_machine_learning_case_studies,
	keywords = {Aprendizaje automático, Lenguaje de programación Python},
	abstract = {Embrace machine learning approaches and Python to enable automatic rendering of
rich insights and solve business problems. The book uses a hands-on case
study-based approach to crack real-world applications to which machine learning
concepts can be applied. These smarter machines will enable your business
processes to achieve efficiencies on minimal time and resources.

_Python Machine Learning Case Studies_ takes you through the steps to improve
business processes and determine the pivotal points that frame strategies.
You’ll see machine learning techniques that you can use to support your
products and services. Moreover you’ll learn the pros and cons of each of the
machine learning concepts to help you decide which one best suits your needs.

By taking a step-by-step approach to coding in Python you’ll be able to
understand the rationale behind model selection and decisions within the
machine learning process. The book is equipped with practical examples along
with code snippets to ensure that you understand the data science approach to
solving real-world problems.
},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/gp/book/9781484228227},
	doi = {10.1007/978-1-4842-2823-4},
	pagetotal = {204},
	isbn = {9781484228227},
	publisher = Apress,
	language = {english},
	subtitle = {Five Case Studies for the Data Scientist},
	date = {2017},
	title = {Python Machine Learning Case Studies},
	author = {Haroon, Danish}
}

@Article{haslum_et_al:extending_classical_planning_state_constraints,
	file = {Artículos_en_revistas/Haslum_et_al-2018-Extending_Classical_Planning_with_State_Constraints.pdf},
	keywords = {Inteligencia artificial, Planificación automática},
	abstract = {We present a principled way of extending a classical AI planning formalism with
systems of state constraints, which relate - sometimes determine - the values
of variables in each state traversed by the plan. This extension occupies an
attractive middle ground between expressivity and complexity. It enables
modelling a new range of problems, as well as formulating more efficient models
of classical planning problems. An example of the former is planning-based
control of networked physical systems - power networks, for example - in which
a local, discrete control action can have global effects on continuous
quantities, such as altering flows across the entire network. At the same time,
our extension remains decidable as long as the satisfiability of sets of state
constraints is decidable, including in the presence of numeric state variables,
and we demonstrate that effective techniques for cost-optimal planning known in
the classical setting - in particular, relaxation-based admissible heuristics -
can be adapted to the extended formalism. In this paper, we apply our approach
to constraints in the form of linear or non-linear equations over numeric state
variables, but the approach is independent of the type of state constraints, as
long as there exists a procedure that decides their consistency. The planner
and the constraint solver interact through a well-defined, narrow interface, in
which the solver requires no specialisation to the planning context.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1613/jair.1.11213},
	pages = {373-431},
	volume = {62},
	language = {english},
	date = {2018},
	xdata = {journal_artificial_intelligence_research},
	title = {Extending Classical Planning with State Constraints: Heuristics and Search for Optimal Planning},
	author = {Haslum, Patrik and Ivankovic, Franc and Ramirez, Miquel and Gordon, Dan and Thiebaux, Sylvie and Shivashankar, Vikas and Nau, Dana S.}
}

@Book{hastie_et_al:elements_statistical_learning,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2016560
Recurso electrónico: http://0-link.springer.com.fama.us.es/book/10.1007/978-0-387-84858-7},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/gp/book/9780387848570},
	doi = {10.1007/978-0-387-84858-7},
	language = {english},
	subtitle = {Data Mining, Inference, and Prediction},
	edition = {2},
	title = {The Elements of Statistical Learning},
	isbn = {9780387848570},
	series = Springer_Statistics,
	abstract = {During the past decade there has been an explosion in computation and information technology. With
it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and
marketing. The challenge of understanding these data has led to the development of new tools in the
field of statistics, and spawned new areas such as data mining, machine learning, and
bioinformatics. Many of these tools have common underpinnings but are often expressed with
different terminology. This book describes the important ideas in these areas in a common
conceptual framework. While the approach is statistical, the emphasis is on concepts rather than
mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable
resource for statisticians and anyone interested in data mining in science or industry. The book's
coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics
include neural networks, support vector machines, classification trees and boosting---the first
comprehensive treatment of this topic in any book.

This major new edition features many topics not covered in the original, including graphical
models, random forests, ensemble methods, least angle regression and path algorithms for the lasso,
non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for
"wide" data ($p$ bigger than $n$), including multiple testing and false discovery rates.},
	pagetotal = {745},
	publisher = Springer,
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	date = {2009},
	keywords = {Aprendizaje automático, Inteligencia artificial},
	file = {Libros/Hastie_et_al-2009-The_Elements_of_Statistical_Learning.pdf}
}

@Article{hatem_et_al:solving_large_problems_heuristic_search,
	file = {Artículos_en_revistas/Hatem_et_al-2018-Solving_Large_Problems_with_Heuristic_Search.pdf},
	keywords = {Búsqueda heurística, Inteligencia artificial},
	abstract = {Classic best-first heuristic search algorithms, like A*, record every unique
state they encounter in RAM, making them infeasible for solving large problems.
In this paper, we demonstrate how best-first search can be scaled to solve much
larger problems by exploiting disk storage and parallel processing and, in some
cases, slightly relaxing the strict best-first node expansion order. Some
previous disk-based search algorithms abandon best-first search order in an
attempt to increase efficiency. We present two case studies showing that A*,
when augmented with Delayed Duplicate Detection, can actually be more efficient
than these non-best-first search orders. First, we present a straightforward
external variant of A*, called PEDAL, that slightly relaxes best-first order in
order to be I/O efficient in both theory and practice, even on problems
featuring real-valued node costs. Because it is easy to parallelize, PEDAL can
be faster than in-memory IDA* even on domains with few duplicate states, such
as the sliding-tile puzzle. Second, we present a variant of PEDAL, called
PE2A*, that uses partial expansion to handle problems that have large branching
factors. When tested on the problem of Multiple Sequence Alignment, PE2A* is
the first algorithm capable of solving the entire Reference Set 1 of the
standard BAliBASE benchmark using a biologically accurate cost function. This
work shows that classic best-first algorithms like A* can be applied to large
real-world problems. We also provide a detailed implementation guide with
source code both for generic parallel disk-based best-first search and for
Multiple Sequence Alignment with a biologically accurate cost function. Given
its effectiveness as a general-purpose problem-solving method, we hope that
this makes parallel and disk-based search accessible to a wider audience.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1613/jair.1.11209},
	pages = {233-268},
	volume = {62},
	language = {english},
	date = {2018},
	title = {Solving Large Problems with Heuristic Search: General-Purpose Parallel External-Memory Search},
	author = {Hatem, Matthew and Burns, Ethan and Ruml, Wheeler}
}

@Book{hils_loeser:first_journey_logic,
	number = {89},
	series = Student_Mathematical_Society,
	publisher = AMS,
	keywords = {Libro solicitado, Lógica matemática, No disponible en la BUS},
	abstract = {The aim of this book is to present mathematical logic to students who are
interested in what this field is but have no intention of specializing in it.
The point of view is to treat logic on an equal footing to any other topic in
the mathematical curriculum. The book starts with a presentation of naive set
theory, the theory of sets that mathematicians use on a daily basis. Each
subsequent chapter presents one of the main areas of mathematical logic: first
order logic and formal proofs, model theory, recursion theory, Gödel's
incompleteness theorem, and, finally, the axiomatic set theory. Each chapter
includes several interesting highlights—outside of logic when possible—either in
the main text, or as exercises or appendices. Exercises are an essential
component of the book, and a good number of them are designed to provide an
opening to additional topics of interest.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://bookstore.ams.org/stml-89/},
	pagetotal = {185},
	isbn = {9781470452728},
	language = {english},
	date = {2019},
	title = {A First Journey through Logic},
	author = {Hils, Martin and Loeser, François}
}

@Book{hofer:uncertainty_analysis_model_results,
	enlaces = {Recurso electrónico (SpringerLink): http://www.us.debiblio.com/login?url=http://dx.doi.org/10.1007/978-3-319-76297-5
},
	file = {Libros/Hofer-2018-The_Uncertainty_Analysis_of_Model_Results.pdf},
	keywords = {Simulación por ordenador},
	abstract = {This book is a practical guide to the uncertainty analysis of computer model
applications. Used in many areas, such as engineering, ecology and economics,
computer models are subject to various uncertainties at the level of model
formulations, parameter values and input data. Naturally, it would be
advantageous to know the combined effect of these uncertainties on the model
results as well as whether the state of knowledge should be improved in order
to reduce the uncertainty of the results most effectively. The book supports
decision-makers, model developers and users in their argumentation for an
uncertainty analysis and assists them in the interpretation of the analysis
results.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783319762968},
	doi = {10.1007/978-3-319-76297-5},
	pagetotal = {346},
	isbn = {978-3-319-76296-8},
	publisher = Springer,
	language = {english},
	subtitle = {A Practical Guide},
	date = {2018},
	title = {The Uncertainty Analysis of Model Results},
	author = {Hofer, Eduard}
}

@Book{holden_piene:abel_prize_2008-2012,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-3-642-39449-2
},
	file = {Libros/Holden_Piene-2014-The_Abel_Prize_2008-2012.pdf},
	keywords = {Historia de las Matemáticas},
	abstract = {Covering the years 2008-2012, this book profiles the life and work of recent
winners of the Abel Prize:

* John G. Thompson and Jacques Tits, 2008
* Mikhail Gromov, 2009
* John T. Tate Jr., 2010
* John W. Milnor, 2011
* Endre Szemerédi, 2012.

The profiles feature autobiographical information as well as a description of
each mathematician's work. In addition, each profile contains a complete
bibliography, a curriculum vitae, as well as photos — old and new. As an added
feature, interviews with the Laureates are presented on an accompanying web
site (http://extras.springer.com/).

The book also presents a history of the Abel Prize written by the historian Kim
Helsvig, and includes a facsimile of a letter from Niels Henrik Abel, which is
transcribed, translated into English, and placed into historical perspective by
Christian Skau.

This book follows on The Abel Prize: 2003-2007, The First Five Years (Springer,
2010), which profiles the work of the first Abel Prize winners.
},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783642394485},
	doi = {10.1007/978-3-642-39449-2},
	pagetotal = {571},
	isbn = {9783642394485},
	publisher = Springer,
	language = {english},
	date = {2014},
	title = {The Abel Prize 2008-2012},
	editor = {Holden, Helge and Piene, Ragni}
}

@Book{holmes_jain:innovations_bayesian_networks,
	series = Springer_Computational_Intelligence,
	enlaces = {Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-3-540-85066-3},
	file = {Libros/Holmes_Jain-2008-Innovations_in_Bayesian_Networks.pdf},
	keywords = {Inteligencia artificial, Redes bayesianas},
	abstract = {Bayesian networks currently provide one of the most rapidly growing areas of research in computer
science and statistics. In compiling this volume we have brought together contributions from some
of the most prestigious researchers in this field. Each of the twelve chapters is self-contained.

Both theoreticians and application scientists/engineers in the broad area of artificial
intelligence will find this volume valuable. It also provides a useful sourcebook for Graduate
students since it shows the direction of current research.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783540850656},
	doi = {10.1007/978-3-540-85066-3},
	pagetotal = {322},
	isbn = {9783540850656},
	publisher = Springer,
	number = {156},
	language = {english},
	subtitle = {Theory and Applications},
	editor = {Holmes, Dawn E. and Jain, Lakhmi C.},
	date = {2008},
	title = {Innovations in Bayesian Networks}
}

@Article{hu_et_al:closer_look_gpgpu,
	doi = {10.1145/2873053},
	number = {4},
	volume = {48},
	xdata = {acm_computing_surveys},
	pagetotal = {20},
	file = {Artículos_en_revistas/Hu_et_al-2016-A_Closer_Look_at_GPGPU.pdf},
	keywords = {Computación paralela, GPGPU},
	abstract = {The lack of detailed white box illustration leaves a gap in the field of GPGPU (General-Purpose
Computing on the Graphic Processing Unit), thus hindering users and researchers from exploring
hardware potential while improving application performance. This article bridges the gap by
demystifying the micro-architecture and operating mechanism of GPGPU. We propose a descriptive
model that addresses key issues of most concerns, including task organization, hardware structure,
scheduling mechanism, execution mechanism, and memory access. We also validate the effectiveness of
our model by interpreting the software/hardware cooperation of CUDA.},
	langidopts = {variant=british},
	langid = {english},
	language = {english},
	date = {2016},
	title = {A Closer Look at GPGPU},
	author = {Hu, Liang and Che, Xilong and Zheng, Si-Qing}
}

@Book{huth_ryan:logic_computer_science,
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.cambridge.org/us/academic/subjects/computer-science/programming-languages-and-applied-logic/logic-computer-science-modelling-and-reasoning-about-systems-2nd-edition},
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1604277
},
	abstract = {The second edition of this successful textbook continues to provide a clear introduction to formal
reasoning relevant to the needs of modern computer science and sufficiently exacting for practical
applications. Improvements have been made throughout with many new and expanded text sections. The
coverage of model-checking has been substantially updated and additional exercises are included.
Internet support includes worked solutions for teacher exercises and model solutions to some
student exercises.},
	author = {Huth, Michael and Ryan, Mark},
	date = {2004-08},
	edition = {2},
	hyphenation = {english},
	isbn = {9780521543101},
	keywords = {Lógica matemática},
	language = {english},
	pagetotal = {440},
	publisher = Cambridge,
	title = {Logic in Computer Science: Modelling and Reasoning about Systems}
}

@Book{inselberg:parallel_coordinates,
	enlaces = {Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-0-387-68628-8},
	file = {Libros/Inselberg-2009-Parallel_Coordinates.pdf},
	keywords = {Geometría, Visualización de datos},
	abstract = {This book is about visualization, systematically incorporating the fantastic human pattern
recognition into the problem-solving process, and focusing on parallel coordinates. The barrier,
imposed by our three-dimensional habitation and perceptual experience, has been breached by this
innovative and versatile methodology. The accurate visualization of multidimensional problems and
multivariate data unlocks insights into the role of dimensionality.

Beginning with an introductory chapter on geometry, the mathematical foundations are intuitively
developed, interlaced with applications to data mining, information visualization, computer vision,
geometric modeling, collision avoidance for air traffic and process-control. Many results appear
for the first time. Multidimensional lines, planes, proximities, surfaces and their properties are
unambiguously recognized (i.e. convexity viewed in any dimension) enabling powerful construction
algorithms (for intersections, interior-points, linear-programming).

Key features of Parallel Coordinates:
* An easy-to-read self-contained chapter on data mining and information visualization
* Numerous exercises with solutions, from basic to advanced topics, course projects and research
  directions
* "Fast Track" markers throughout provide a quick grasp of essential material.
* Extensive bibliography, index, and a chapter containing a collection of recent results (i.e.
  visualizing large networks, complex-valued functions and more)

Parallel Coordinates requires only an elementary knowledge of linear algebra. It is well-suited for
self-study and as a textbook (or companion) for courses on information visualization, data mining,
mathematics, statistics, computer science, engineering, finance, management, manufacturing, in
scientific disciplines and even the arts.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/gp/book/9780387215075},
	doi = {10.1007/978-0-387-68628-8},
	pagetotal = {554},
	isbn = {9780387215075},
	publisher = Springer,
	subtitle = {Visual Multidimensional Geometry and Its Applications},
	language = {english},
	date = {2009},
	title = {Parallel Coordinates},
	author = {Inselberg, Alfred}
}

@Book{irizarry:introduction_data_science,
	keywords = {Ciencia del dato, Lenguaje de programación R},
	abstract = {**Introduction to Data Science: Data Analysis and Prediction Algorithms with
R** introduces concepts and skills that can help you tackle real-world data
analysis challenges. It covers concepts from probability, statistical
inference, linear regression, and machine learning. It also helps you develop
skills such as R programming, data wrangling, data visualization, predictive
algorithm building, file organization with UNIX/Linux shell, version control
with Git and GitHub, and reproducible document preparation.

This book is a textbook for a first course in data science. No previous
knowledge of R is necessary, although some experience with programming may be
helpful. The book is divided into six parts: R, data visualization, statistics
with R, data wrangling, machine learning, and productivity tools. Each part has
several chapters meant to be presented as one lecture.

The author uses motivating case studies that realistically mimic a data
scientist’s experience. He starts by asking specific questions and answers
these through data analysis so concepts are learned as a means to answering the
questions. Examples of the case studies included are: US murder rates by state,
self-reported student heights, trends in world health and economics, the impact
of vaccines on infectious disease rates, the financial crisis of 2007-2008,
election forecasting, building a baseball team, image processing of
hand-written digits, and movie recommendation systems.

The statistical concepts used to answer the case study questions are only
briefly introduced, so complementing with a probability and statistics textbook
is highly recommended for in-depth understanding of these concepts. If you read
and understand the chapters and complete the exercises, you will be prepared to
learn the more advanced concepts and skills needed to become an expert.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Introduction-to-Data-Science-Data-Analysis-and-Prediction-Algorithms-with/Irizarry/p/book/9780367357986},
	pagetotal = {784},
	isbn = {9780367357986},
	publisher = CRC,
	series = CRC_Data_Science,
	language = {english},
	subtitle = {Data Analysis and Prediction Algorithms with R},
	date = {2019},
	title = {Introduction to Data Science},
	author = {Irizarry, Rafael A.}
}

@Book{izenman:modern_multivariate_statistical_techniques,
	subtitle = {Regression, Classification, and Manifold Learning},
	enlaces = {Recurso electrónico (SpringerLink): http://www.us.debiblio.com/login?url=http://dx.doi.org/10.1007/978-0-387-78189-1
},
	file = {Libros/Izenman-2008-Modern_Multivariate_Statistical_Techniques.pdf},
	keywords = {Aprendizaje automático, Estadística},
	abstract = {Remarkable advances in computation and data storage and the ready availability
of huge data sets have been the keys to the growth of the new disciplines of
data mining and machine learning, while the enormous success of the Human
Genome Project has opened up the field of bioinformatics.

These exciting developments, which led to the introduction of many innovative
statistical tools for high-dimensional data analysis, are described here in
detail. The author takes a broad perspective; for the first time in a book on
multivariate analysis, nonlinear methods are discussed in detail as well as
linear methods. Techniques covered range from traditional multivariate methods,
such as multiple regression, principal components, canonical variates, linear
discriminant analysis, factor analysis, clustering, multidimensional scaling,
and correspondence analysis, to the newer methods of density estimation,
projection pursuit, neural networks, multivariate reduced-rank regression,
nonlinear manifold learning, bagging, boosting, random forests, independent
component analysis, support vector machines, and classification and regression
trees. Another unique feature of this book is the discussion of database
management systems.

This book is appropriate for advanced undergraduate students, graduate
students, and researchers in statistics, computer science, artificial
intelligence, psychology, cognitive sciences, business, medicine,
bioinformatics, and engineering. Familiarity with multivariable calculus,
linear algebra, and probability and statistics is required. The book presents a
carefully-integrated mixture of theory and applications, and of classical and
modern multivariate statistical techniques, including Bayesian methods. There
are over 60 interesting data sets used as examples in the book, over 200
exercises, and many color illustrations and photographs.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780387781884},
	doi = {10.1007/978-0-387-78189-1},
	pagetotal = {733},
	isbn = {9780387781884},
	publisher = Springer,
	series = Springer_Text_Statistics,
	language = {english},
	date = {2008},
	title = {Modern Multivariate Statistical Techniques},
	author = {Izenman, Alan Julian}
}

@Article{jain:data_clustering_50_years_beyond_K-means,
	abstract = {Organizing data into sensible groupings is one of the most fundamental modes of
understanding and learning. As an example, a common scheme of scientific
classification puts organisms into a system of ranked taxa: domain, kingdom,
phylum, class, etc. Cluster analysis is the formal study of methods and
algorithms for grouping, or clustering, objects according to measured or
perceived intrinsic characteristics or similarity. Cluster analysis does not
use category labels that tag objects with prior identifiers, i.e., class
labels. The absence of category information distinguishes data clustering
(unsupervised learning) from classification or discriminant analysis
(supervised learning). The aim of clustering is to find structure in data and
is therefore exploratory in nature. Clustering has a long and rich history in a
variety of scientific fields. One of the most popular and simple clustering
algorithms, K-means, was first published in 1955. In spite of the fact that
K-means was proposed over 50 years ago and thousands of clustering algorithms
have been published since then, K-means is still widely used. This speaks to
the difficulty in designing a general purpose clustering algorithm and the
ill-posed problem of clustering. We provide a brief overview of clustering,
summarize well known clustering methods, discuss the major challenges and key
issues in designing clustering algorithms, and point out some of the emerging
and useful research directions, including semi-supervised clustering, ensemble
clustering, simultaneous feature selection during data clustering, and large
scale data clustering.},
	file = {Artículos_en_revistas/Jain-2010-Data_clustering:_50_years_beyond_K-means.pdf},
	keywords = {Análisis de grupos, Aprendizaje automático},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1016/j.patrec.2009.09.011},
	pages = {651-666},
	number = {8},
	volume = {31},
	language = {english},
	xdata = {pattern_recognition_letters},
	date = {2010},
	title = {Data clustering: 50 years beyond K-means},
	author = {Jain, Anil K.}
}

@Book{japkowicz_shah:evaluating_learning_algorithms,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2331551},
	keywords = {Aprendizaje automático, Inteligencia artificial},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.mohakshah.com/books/ELA},
	pagetotal = {424},
	isbn = {9780521196000},
	publisher = Cambridge,
	language = {english},
	date = {2011},
	title = {Evaluating Learning Algorithms},
	author = {Japkowicz, Nathalie and Shah, Mohak}
}

@Book{jensen_nielsen:bayesian_networks_decision_graphs,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2015603
Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-0-387-68282-2},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/978-0-387-68281-5},
	doi = {10.1007/978-0-387-68282-2},
	language = {english},
	edition = {2},
	title = {Bayesian Networks and Decision Graphs},
	isbn = {9780387682815},
	series = {Information Science and Statistics},
	abstract = {Probabilistic graphical models and decision graphs are powerful modeling tools for reasoning and
decision making under uncertainty. As modeling languages they allow a natural specification of
problem domains with inherent uncertainty, and from a computational perspective they support
efficient algorithms for automatic construction and query answering. This includes belief updating,
finding the most probable explanation for the observed evidence, detecting conflicts in the
evidence entered into the network, determining optimal strategies, analyzing for relevance, and
performing sensitivity analysis.

The book introduces probabilistic graphical models and decision graphs, including Bayesian networks
and influence diagrams. The reader is introduced to the two types of frameworks through examples
and exercises, which also instruct the reader on how to build these models.

The book is a new edition of _Bayesian Networks and Decision Graphs_ by _Finn V. Jensen_. The new
edition is structured into two parts. The first part focuses on probabilistic graphical models.
Compared with the previous book, the new edition also includes a thorough description of recent
extensions to the Bayesian network modeling language, advances in exact and approximate belief
updating algorithms, and methods for learning both the structure and the parameters of a Bayesian
network. The second part deals with decision graphs, and in addition to the frameworks described in
the previous edition, it also introduces Markov decision processes and partially ordered decision
problems. The authors also provide a well-founded practical introduction to Bayesian networks,
object-oriented Bayesian networks, decision trees, influence diagrams (and variants hereof), and
Markov decision processes.},
	pagetotal = {448},
	publisher = Springer,
	author = {Jensen, Finn Verner and Nielsen, Thomas Dyhre},
	date = {2007},
	keywords = {Modelos gráficos probabilísticos, Redes bayesianas},
	file = {Libros/Jensen_Nielsen-2007-Bayesian_Networks_and_Decision_Graphs.pdf}
}

@XData{journal_artificial_intelligence_research,
	journaltitle = {Journal of Artificial Intelligence Research},
	issn = {1076 - 9757},
	publisher = AI_Access_Foundation
}

@XData{journal_cloud_computing,
	journaltitle = {Journal of Cloud Computing},
	journalsubtitle = {Advances, Systems and Applications},
	issn = {2192-113X},
	publisher = Springer
}

@XData{journal_computer_system_sciences,
	journaltitle = {Journal of Computer and System Sciences},
	issn = {0022-0000},
	publisher = Elsevier
}

@XData{journal_machine_learning_research,
	journaltitle = {Journal of Machine Learning Research},
	issn = {1532-4435},
	publisher = Microtome
}

@XData{journal_software,
	journaltitle = {Journal of Software},
	issn = {1796-217X}
}

@Article{juan_et_al:review_simheuristics,
	file = {Artículos_en_revistas/Juan_et_al-2015-A_review_of_simheuristics:_Extending_metaheuristics_to_deal_with_stochastic_combinatorial_optimization_problems.pdf},
	keywords = {Metaheurísticas, Problemas de optimización},
	abstract = {Many combinatorial optimization problems (COPs) encountered in real-world
logistics, transportation, production, healthcare, financial,
telecommunication, and computing applications are NP-hard in nature. These
real-life COPs are frequently characterized by their large-scale sizes and the
need for obtaining high-quality solutions in short computing times, thus
requiring the use of metaheuristic algorithms. Metaheuristics benefit from
different random-search and parallelization paradigms, but they frequently
assume that the problem inputs, the underlying objective function, and the set
of optimization constraints are deterministic. However, uncertainty is all
around us, which often makes deterministic models oversimplified versions of
real-life systems. After completing an extensive review of related work, this
paper describes a general methodology that allows for extending metaheuristics
through simulation to solve stochastic COPs. ‘Simheuristics’ allow modelers for
dealing with real-life uncertainty in a natural way by integrating simulation
(in any of its variants) into a metaheuristic-driven framework. These
optimization-driven algorithms rely on the fact that efficient metaheuristics
already exist for the deterministic version of the corresponding COP.
Simheuristics also facilitate the introduction of risk and/or reliability
analysis criteria during the assessment of alternative high-quality solutions
to stochastic COPs. Several examples of applications in different fields
illustrate the potential of the proposed methodology.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1016/j.orp.2015.03.001},
	pages = {62-72},
	volume = {2},
	language = {english},
	date = {2015},
	xdata = {operations_research_perspectives},
	title = {A review of simheuristics: Extending metaheuristics to deal with stochastic combinatorial optimization problems},
	author = {Juan, Ángel A. and Faulin, Javier and Grasman, Scott E. and Rabe, Markus and Figueira, Gonçalo}
}

@Book{jurafsky_martin:speech_language_processing,
	file = {Libros/Jurafsky_Martin-Speech_and_Language_Processing.pdf},
	keywords = {Procesamiento del lenguaje natural},
	langidopts = {variant=british},
	langid = {english},
	url = {https://web.stanford.edu/~jurafsky/slp3/},
	edition = {3rd ed. draft},
	language = {english},
	subtitle = {An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
	title = {Speech and Language Processing},
	author = {Jurafsky, Dan and Martin, James H.}
}

@Book{kapil:clean_python,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/nhtkp3/alma991013170108904987
},
	keywords = {Lenguaje de programación Python},
	abstract = {Discover the right way to code in Python. This book provides the tips and
techniques you need to produce cleaner, error-free, and eloquent Python
projects.

Your journey to better code starts with understanding the importance of
formatting and documenting your code for maximum readability, utilizing
built-in data structures and Python dictionary for improved maintainability,
and working with modules and meta-classes to effectively organize your code.
You will then dive deep into the new features of the Python language and learn
how to effectively utilize them. Next, you will decode key concepts such as
asynchronous programming, Python data types, type hinting, and path handling.
Learn tips to debug and conduct unit and integration tests in your Python code
to ensure your code is ready for production. The final leg of your learning
journey equips you with essential tools for version management, managing live
code, and intelligent code completion.

After reading and using this book, you will be proficient in writing clean
Python code and successfully apply these principles to your own Python
projects.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9781484248775},
	doi = {10.1007/978-1-4842-4878-2},
	pagetotal = {267},
	isbn = {9781484248775},
	publisher = Apress,
	language = {english},
	subtitle = {Elegant Coding in Python},
	date = {2019},
	title = {Clean Python},
	author = {Kapil, Sunil}
}

@book{karkera:building_probabilistic_graphical_models_python,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2612388
Recurso electrónico (E-Libro): http://0-site.ebrary.com.fama.us.es/lib/unisev/Doc?id=10887716},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.packtpub.com/big-data-and-business-intelligence/building-probabilistic-graphical-models-python},
	language = {english},
	title = {Building Probabilistic Graphical Models with Python},
	isbn = {9781783289004},
	abstract = {With the increasing prominence in machine learning and data science applications, probabilistic
graphical models are a new tool that machine learning users can use to discover and analyze
structures in complex problems. The variety of tools and algorithms under the PGM framework extend
to many domains such as natural language processing, speech processing, image processing, and
disease diagnosis.

You've probably heard of graphical models before, and you're keen to try out new landscapes in the
machine learning area. This book gives you enough background information to get started on
graphical models, while keeping the math to a minimum.
},
	pagetotal = {172},
	publisher = Packt,
	author = {Karkera, Kiran R.},
	date = {2014},
	keywords = {Inteligencia artificial, Lenguaje de programación Python, Redes bayesianas}
}

@Book{keen:graphics_statistics_data_analysis_r,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/nhtkp3/alma991013169905104987
},
	keywords = {Lenguaje de programación R, Visualización de datos},
	abstract = {**Graphics for Statistics and Data Analysis with R**, Second Edition, presents
the basic principles of graphical design and applies these principles to
engaging examples using the graphics and lattice packages in R. It offers a
wide array of modern graphical displays for data visualization and
representation. Added in the second edition are coverage of the ggplot2
graphics package, material on human visualization and color rendering in R, on
screen, and in print.

**Features**

* Emphasizes the fundamentals of statistical graphics and best practice
  guidelines for producing and choosing among graphical displays in R
* Presents technical details on topics such as: the estimation of quantiles,
  nonparametric and parametric density estimation; diagnostic plots for the
  simple linear regression model; polynomial regression, splines, and locally
  weighted polynomial regression for producing a smooth curve; Trellis graphics
  for multivariate data
* Provides downloadable R code and data for figures at
  www.graphicsforstatistics.com},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Graphics-for-Statistics-and-Data-Analysis-with-R-Second-Edition/Keen/p/book/9781498779838},
	pagetotal = {590},
	isbn = {9781498779838},
	publisher = CRC,
	series = CRC_Statistical_Science,
	edition = {2},
	language = {english},
	date = {2018},
	title = {Graphics for Statistics and Data Analysis with R},
	author = {Keen, Kevin J.}
}

@Book{ketkar:deep_learning_python,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013154696604987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	keywords = {Aprendizaje automático, Aprendizaje profundo, Lenguaje de programación Python, Redes neuronales},
	abstract = {Discover the practical aspects of implementing deep-learning solutions using
the rich Python ecosystem. This book bridges the gap between the academic
state-of-the-art and the industry state-of-the-practice by introducing you to
deep learning frameworks such as Keras, Theano, and Caffe. The practicalities
of these frameworks is often acquired by practitioners by reading source code,
manuals, and posting questions on community forums, which tends to be a slow
and a painful process. _Deep Learning with Python_ allows you to ramp up to
such practical know-how in a short period of time and focus more on the domain,
models, and algorithms.

This book briefly covers the mathematical prerequisites and fundamentals of
deep learning, making this book a good starting point for software developers
who want to get started in deep learning. A brief survey of deep learning
architectures is also included.

_Deep Learning with Python_ also introduces you to key concepts of automatic
differentiation and GPU computation which, while not central to deep learning,
are critical when it comes to conducting large scale experiments.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/978-1-4842-2766-4},
	pagetotal = {226},
	isbn = {9781484227657},
	publisher = Apress,
	language = {english},
	subtitle = {A Hands-on Introduction},
	date = {2017},
	title = {Deep Learning with Python},
	author = {Ketkar, Nihkil}
}

@Book{kirk:data_visualisation,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/3enc2g/alma991013169904204987
},
	publisher = Sage,
	keywords = {Visualización de datos},
	abstract = {With over 200 images and extensive how-to and how-not-to examples, this new
edition has everything students and scholars need to understand and create
effective data visualisations.

Combining ‘how to think’ instruction with a ‘how to produce’ mentality, this
book takes readers step-by-step through analysing, designing, and curating
information into useful, impactful tools of communication.

With this book and its extensive collection of online support, readers can:

* Decide what visualisations work best for their data and their audience using
  the chart gallery
* See data visualisation in action and learn the tools to try it themselves
* Follow online checklists, tutorials, and exercises to build skills and
  confidence
* Get advice from the UK’s leading data visualisation trainer on everything
  from getting started to honing the craft.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://uk.sagepub.com/en-gb/eur/data-visualisation/book266150},
	pagetotal = {328},
	isbn = {9781526468932},
	edition = {2},
	language = {english},
	subtitle = {A Handbook for Data Driven Design},
	date = {2019},
	title = {Data Visualisation},
	author = {Kirk, Andy}
}

@Book{kjærulff_madsen:bayesian_networks_influence_diagrams,
	number = {22},
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-0-387-74101-7},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/978-1-4614-5103-7},
	doi = {10.1007/978-1-4614-5104-4},
	language = {english},
	edition = {2},
	title = {Bayesian Networks and Influence Diagrams: A Guide to Construction and Analysis},
	isbn = {9781461451037},
	series = {Information Science and Statistics},
	abstract = {_Bayesian Networks and Influence Diagrams: A Guide to Construction and Analysis, Second Edition_,
provides a comprehensive guide for practitioners who wish to understand, construct, and analyze
intelligent systems for decision support based on probabilistic networks. This new edition contains
six new sections, in addition to fully-updated examples, tables, figures, and a revised appendix.
Intended primarily for practitioners, this book does not require sophisticated mathematical skills
or deep understanding of the underlying theory and methods nor does it discuss alternative
technologies for reasoning under uncertainty. The theory and methods presented are illustrated
through more than 140 examples, and exercises are included for the reader to check his or her level
of understanding. The techniques and methods presented on model construction and verification,
modeling techniques and tricks, learning models from data, and analyses of models have all been
developed and refined based on numerous courses the authors have held for practitioners worldwide.},
	pagetotal = {382},
	publisher = Springer,
	author = {Kjærulff, Uffe B. and Madsen, Anders L.},
	date = {2013},
	keywords = {Modelos gráficos probabilísticos, Redes bayesianas},
	file = {Libros/Kjærulff_Madsen-2013-Bayesian_Networks_and_Influence_Diagrams.pdf; Libros/Kjærulff_Madsen-2013-Bayesian_Networks_and_Influence_Diagrams-Errata.pdf}
}

@Book{kneusel:numbers_computers,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2705265},
	keywords = {Matemática computacional},
	abstract = {This is a book about numbers and how those numbers are represented in and operated on by computers.
It is crucial that developers understand this area because the numerical operations allowed by
computers, and the limitations of those operations, especially in the area of floating point math,
affect virtually everything people try to do with computers. This book aims to fill this gap by
exploring, in sufficient but not overwhelming detail, just what it is that computers do with
numbers.

Divided into two parts, the first deals with standard representations of integers and floating
point numbers, while the second details several other number representations. Each chapter ends
with exercises to review the key points. Topics covered include interval arithmetic, fixed-point
numbers, floating point numbers, big integers and rational arithmetic.

This book is for anyone who develops software including software engineerings, scientists, computer
science students, engineering students and anyone who programs for fun.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/978-3-319-17259-0},
	doi = {10.1007/978-3-319-17260-6},
	pagetotal = {231},
	isbn = {9783319172590},
	publisher = Springer,
	language = {english},
	date = {2015},
	title = {Numbers and Computers},
	author = {Kneusel, Ronald T.}
}

@XData{knowledge-based_systems,
	journaltitle = {Knowledge-Based Systems},
	issn = {0950-7051},
	publisher = Elsevier
}

@XData{knowledge_engineering_review,
	journaltitle = {The Knowledge Engineering Review},
	issn = {1469-8005},
	publisher = Cambridge
}

@Book{koller_friedman:probabilistic_graphical_models,
	series = Adaptive_Computation,
	publisher = MIT_Press,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2200769},
	file = {Koller_Friedman-2009-Probabilistic_Graphical_Models-Principles_and_Techniques.pdf},
	keywords = {Inteligencia artificial, Redes bayesianas},
	abstract = {Most tasks require a person or an automated system to reason—to reach conclusions based on
available information. The framework of probabilistic graphical models, presented in this book,
provides a general approach for this task. The approach is model-based, allowing interpretable
models to be constructed and then manipulated by reasoning algorithms. These models can also be
learned automatically from data, allowing the approach to be used in cases where manually
constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect
of most real-world applications, the book focuses on probabilistic models, which make the
uncertainty explicit and provide models that are more faithful to reality.

_Probabilistic Graphical Models_ discusses a variety of models, spanning Bayesian networks,
undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical
systems and relational data. For each class of models, the text describes the three fundamental
cornerstones: representation, inference, and learning, presenting both basic concepts and advanced
techniques. Finally, the book considers the use of the proposed framework for causal reasoning and
decision making under uncertainty. The main text in each chapter provides the detailed technical
development of the key ideas. Most chapters also include boxes with additional material: skill
boxes, which describe techniques; case study boxes, which discuss empirical cases related to the
approach described in the text, including applications in computer vision, robotics, natural
language understanding, and computational biology; and concept boxes, which present significant
concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in
various combinations, from core topics to more technically advanced material, to suit their
particular needs.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://mitpress.mit.edu/books/probabilistic-graphical-models},
	pagetotal = {1280},
	isbn = {9780262013192},
	language = {english},
	date = {2009},
	title = {Probabilistic Graphical Models},
	author = {Koller, Daphne and Friedman, Nir}
}

@Article{konak_et_al:multiobjective_optimization_genetic_algorithms,
	xdata = {reliability_engineering_system_safety},
	file = {Artículos_en_revistas/Konak_et_al-2006-Multi-objective_optimization_using_genetic_algorithms:_A_tutorial.pdf},
	keywords = {Algoritmos genéticos, Metaheurísticas},
	abstract = {Multi-objective formulations are realistic models for many complex engineering optimization
problems. In many real-life problems, objectives under consideration conflict with each other, and
optimizing a particular solution with respect to a single objective can result in unacceptable
results with respect to the other objectives. A reasonable solution to a multi-objective problem is
to investigate a set of solutions, each of which satisfies the objectives at an acceptable level
without being dominated by any other solution. In this paper, an overview and tutorial is presented
describing genetic algorithms (GA) developed specifically for problems with multiple objectives.
They differ primarily from traditional GA by using specialized fitness functions and introducing
methods to promote solution diversity.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1016/j.ress.2005.11.018},
	pages = {992-1007},
	number = {9},
	volume = {91},
	language = {english},
	date = {2006},
	title = {Multi-objective optimization using genetic algorithms: A tutorial},
	author = {Konak, Abdullah and Coit, David W. and Smith, Alice E.}
}

@book{korb_nicholson:bayesian_artificial_intelligence,
	title = {Bayesian Artificial Intelligence},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.csse.monash.edu.au/bai/book},
	language = {english},
	edition = {2},
	isbn = {9781439815915},
	series = CRC_Computer_Science,
	abstract = {Updated and expanded, **Bayesian Artificial Intelligence, Second Edition** provides a practical and
accessible introduction to the main concepts, foundation, and applications of Bayesian networks. It
focuses on both the causal discovery of networks and Bayesian inference procedures. Adopting a
causal interpretation of Bayesian networks, the authors discuss the use of Bayesian networks for
causal modeling. They also draw on their own applied research to illustrate various applications of
the technology.

**New to the Second Edition**

* New chapter on Bayesian network classifiers
* New section on object-oriented Bayesian networks
* New section that addresses foundational problems with causal discovery and Markov blanket
  discovery
* New section that covers methods of evaluating causal discovery programs
* Discussions of many common modeling errors
* New applications and case studies
* More coverage on the uses of causal interventions to understand and reason with causal Bayesian
  networks

Illustrated with real case studies, the second edition of this bestseller continues to cover the
groundwork of Bayesian networks. It presents the elements of Bayesian network technology, automated
causal discovery, and learning probabilities from data and shows how to employ these technologies
to develop probabilistic expert systems.},
	pagetotal = {491},
	publisher = CRC,
	author = {Korb, Kevin B. and Nicholson, Ann E.},
	date = {2010},
	keywords = {Inteligencia artificial, Redes bayesianas}
}

@Book{koubaa_et_al:robot_path_planning_cooperation,
	enlaces = {Recurso electrónico (SpringerLink): http://www.us.debiblio.com/login?url=https://doi.org/10.1007/978-3-319-77042-0
},
	file = {Libros/Koubaa_et_al_-2018-Robot_Path_Planning_and_Cooperation.pdf},
	keywords = {Planificación automática, Robótica},
	abstract = {This book presents extensive research on two main problems in robotics: the
path planning problem and the multi-robot task allocation problem. It is the
first book to provide a comprehensive solution for using these techniques in
large-scale environments containing randomly scattered obstacles. The research
conducted resulted in tangible results both in theory and in practice. For path
planning, new algorithms for large-scale problems are devised and implemented
and integrated into the Robot Operating System (ROS). The book also discusses
the parallelism advantage of cloud computing techniques to solve the path
planning problem, and, for multi-robot task allocation, it addresses the task
assignment problem and the multiple traveling salesman problem for mobile
robots applications. In addition, four new algorithms have been devised to
investigate the cooperation issues with extensive simulations and comparative
performance evaluation. The algorithms are implemented and simulated in MATLAB
and Webots.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783319770406},
	doi = {10.1007/978-3-319-77042-0},
	pagetotal = {190},
	isbn = {9783319770406},
	publisher = Springer,
	number = {772},
	series = Springer_Computational_Intelligence,
	language = {english},
	subtitle = {Foundations, Algorithms and Experimentations},
	date = {2018},
	title = {Robot Path Planning and Cooperation},
	author = {Koubaa, Anis and Bennaceur, Hachemi and Chaari, Imen and Trigui, Sahar and Ammar, Adel and Sriti, Mohamed-Foued and Alajlan, Maram and Cheikhrouhou, Omar and Javed, Yasir}
}

@Book{kramer:genetic_algorithm_essentials,
	file = {Libros/Kramer-2017-Genetic_Algorithm_Essentials.pdf},
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2748658},
	keywords = {Algoritmos genéticos, Inteligencia artificial},
	abstract = {This book introduces readers to genetic algorithms (GAs) with an emphasis on
making the concepts, algorithms, and applications discussed as easy to
understand as possible. Further, it avoids a great deal of formalisms and thus
opens the subject to a broader audience in comparison to manuscripts overloaded
by notations and equations. The book is divided into three parts, the first of
which provides an introduction to GAs, starting with basic concepts like
evolutionary operators and continuing with an overview of strategies for tuning
and controlling parameters. In turn, the second part focuses on solution space
variants like multimodal, constrained, and multi-objective solution spaces.
Lastly, the third part briefly introduces theoretical tools for GAs, the
intersections and hybridizations with machine learning, and highlights selected
promising applications.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783319521558},
	doi = {10.1007/978-3-319-52156-5},
	pagetotal = {92},
	isbn = {9783319521558},
	publisher = Springer,
	number = {679},
	series = Springer_Computational_Intelligence,
	language = {english},
	date = {2017},
	title = {Genetic Algorithm Essentials},
	author = {Kramer, Oliver}
}

@Book{krantz:primer_mathematical_writing,
	keywords = {Libro solicitado, Matemáticas, No disponible en la BUS},
	abstract = {This is the second edition of a book originally published in 1997. Today the
internet virtually consumes all of our lives (especially the lives of writers).
As both readers and writers, we are all aware of blogs, chat rooms, and preprint
servers. There are now electronic-only journals and print-on-demand books, Open
Access journals and joint research projects such as MathOverflow—not to mention
a host of other new realities. It truly is a brave new world, one that can be
overwhelming and confusing. The truly new feature of this second edition is an
extensive discussion of technological developments. Similar to the first
edition, Krantz's frank and straightforward approach makes this book
particularly suitable as a textbook for an undergraduate course.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://bookstore.ams.org/mbk-112/},
	pagetotal = {243},
	isbn = {9781470436582},
	publisher = AMS,
	edition = {2},
	language = {english},
	subtitle = {Being a Disquisition on Having Your Ideas Recorded, Typeset, Published, Read, and Appreciated},
	date = {2017},
	title = {A Primer of Mathematical Writing},
	author = {Krantz, Steven G.}
}

@Article{krentel:complexity_optimization_problems,
	file = {Artículos_en_revistas/Krentel-1988-The_complexity_of_optimization_problems.pdf},
	keywords = {Complejidad computacional, Problemas de optimización},
	abstract = {We consider **NP**-complete optimization problems at the level of computing
their optimal value, and define a class of functions called **OptP** to capture
this level of structure. We show that TRAVELING SALESPERSON and KNAPSACK are
complete for **OptP**, and that CLIQUE and COLORING are complete for a subclass
of **OptP**. These results show a deeper level of structure in these problems
than was previously known. We also show that **OptP** is closely related to
**FP**^{SAT}, the class of functions computable in polynomial time with an
oracle for **NP**. This allows us to quantify exactly “how much”
**NP**-completeness is in these problems. In particular, in this measure, we
show that TRAVELING SALESPERSON is strictly harder than CLIQUE and that CLIQUE
is strictly harder than BIN PACKING. A further result is that an
**OptP**-completeness result implies **NP**-, D^{p}-, and
Δ_{2}^{P}-completeness results, thus tying these four classes closely together.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1016/0022-0000(88)90039-6},
	pages = {490-509},
	number = {3},
	volume = {36},
	language = {english},
	date = {1988},
	xdata = {journal_computer_system_sciences},
	title = {The complexity of optimization problems},
	author = {Krentel, Mark W.}
}

@Book{krishnamurthy:partially_observed_markov_decision_processes,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013154696504987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	keywords = {Modelos de Markov},
	abstract = {Covering formulation, algorithms, and structural results, and linking theory to
real-world applications in controlled sensing (including social learning,
adaptive radars and sequential detection), this book focuses on the conceptual
foundations of partially observed Markov decision processes (POMDPs). It
emphasizes structural results in stochastic dynamic programming, enabling
graduate students and researchers in engineering, operations research, and
economics to understand the underlying unifying themes without getting weighed
down by mathematical technicalities. Bringing together research from across the
literature, the book provides an introduction to nonlinear filtering followed
by a systematic development of stochastic dynamic programming, lattice
programming and reinforcement learning for POMDPs. Questions addressed in the
book include: when does a POMDP have a threshold optimal policy? When are
myopic policies optimal? How do local and global decision makers interact in
adaptive decision making in multi-agent social learning where there is herding
and data incest? And how can sophisticated radars and sensors adapt their
sensing in real time?},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.cambridge.org/es/academic/subjects/engineering/communications-and-signal-processing/partially-observed-markov-decision-processes-filtering-controlled-sensing},
	pagetotal = {488},
	isbn = {9781107134607},
	publisher = Cambridge,
	language = {english},
	subtitle = {From Filtering to Controlled Sensing},
	date = {2016},
	title = {Partially Observed Markov Decision Processes},
	author = {Krishnamurthy, Vikram}
}

@book{kroese_et_al:handbook_monte_carlo_methods,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2197566},
	langidopts = {variant=british},
	langid = {english},
	url = {https://people.smp.uq.edu.au/DirkKroese/montecarlohandbook/},
	language = {english},
	title = {Handbook of Monte Carlo Methods},
	isbn = {9780470177938},
	series = Wiley_Probability_Statistics,
	abstract = {A comprehensive overview of Monte Carlo simulation that explores the latest topics, techniques, and
real-world applications

More and more of today’s numerical problems found in engineering and finance are solved through
Monte Carlo methods. The heightened popularity of these methods and their continuing development
makes it important for researchers to have a comprehensive understanding of the Monte Carlo
approach. Handbook of Monte Carlo Methods provides the theory, algorithms, and applications that
helps provide a thorough understanding of the emerging dynamics of this rapidly-growing field.

The authors begin with a discussion of fundamentals such as how to generate random numbers on a
computer. Subsequent chapters discuss key Monte Carlo topics and methods, including:

* Random variable and stochastic process generation
* Markov chain Monte Carlo, featuring key algorithms such as the Metropolis-Hastings method, the
  Gibbs sampler, and hit-and-run
* Discrete-event simulation
* Techniques for the statistical analysis of simulation data including the delta method,
  steady-state estimation, and kernel density estimation
* Variance reduction, including importance sampling, latin hypercube sampling, and conditional
  Monte Carlo
* Estimation of derivatives and sensitivity analysis
* Advanced topics including cross-entropy, rare events, kernel density estimation, quasi Monte
  Carlo, particle systems, and randomized optimization

The presented theoretical concepts are illustrated with worked examples that use MATLAB, a related
Web site houses the MATLAB code, allowing readers to work hands-on with the material and also
features the author's own lecture notes on Monte Carlo methods. Detailed appendices provide
background material on probability theory, stochastic processes, and mathematical statistics as
well as the key optimization concepts and techniques that are relevant to Monte Carlo simulation.

Handbook of Monte Carlo Methods is an excellent reference for applied statisticians and
practitioners working in the fields of engineering and finance who use or would like to learn how
to use Monte Carlo in their research. It is also a suitable supplement for courses on Monte Carlo
methods and computational statistics at the upper-undergraduate and graduate levels.},
	pagetotal = {772},
	publisher = Wiley,
	author = {Kroese, Dirk P. and Taimre, Thomas and Botev, Zdravko I.},
	date = {2011},
	keywords = {Estadística computacional, Métodos de Monte Carlo}
}

@Book{kubat:introduction_machine_learning,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2714855},
	keywords = {Aprendizaje automático},
	abstract = {This book presents basic ideas of machine learning in a way that is easy to understand, by
providing hands-on practical advice, using simple examples, and motivating students with
discussions of interesting applications. The main topics include Bayesian classifiers,
nearest-neighbor classifiers, linear and polynomial classifiers, decision trees, neural networks,
and support vector machines. Later chapters show how to combine these simple tools by way of
“boosting,” how to exploit them in more complicated domains, and how to deal with diverse advanced
practical issues. One chapter is dedicated to the popular genetic algorithms.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/gp/book/9783319200095},
	doi = {10.1007/978-3-319-20010-1},
	pagetotal = {291},
	isbn = {9783319200095},
	publisher = Springer,
	language = {english},
	date = {2015},
	title = {An Introduction to Machine Learning},
	author = {Kubat, Miroslav}
}

@Book{kuhn_johnson:applied_predictive_modeling,
	abstract = {_Applied Predictive Modeling_ covers the overall predictive modeling process, beginning with the
crucial steps of data preprocessing, data splitting and foundations of model tuning. The text then
provides intuitive explanations of numerous common and modern regression and classification
techniques, always with an emphasis on illustrating and solving real data problems. Addressing
practical concerns extends beyond model fitting to topics such as handling class imbalance,
selecting predictors, and pinpointing causes of poor model performance—all of which are problems
that occur frequently in practice.
 
The text illustrates all parts of the modeling process through many hands-on, real-life examples.
And every chapter contains extensive R code for each step of the process. The data sets and
corresponding code are available in the book’s companion AppliedPredictiveModeling R package, which
is freely available on the CRAN archive.
 
This multi-purpose text can be used as an introduction to predictive models and the overall
modeling process, a practitioner’s reference handbook, or as a text for advanced undergraduate or
graduate level predictive modeling courses. To that end, each chapter contains problem sets to help
solidify the covered concepts and uses data available in the book’s R package.
 
Readers and students interested in implementing the methods should have some basic knowledge of R.
And a handful of the more advanced topics require some mathematical knowledge.},
	file = {Libros/Kuhn_Johnson-2013-Applied_Predictive_Modeling.pdf},
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-1-4614-6849-3},
	keywords = {Aprendizaje automático, Lenguaje de programación R},
	langidopts = {variant=british},
	langid = {english},
	url = {http://appliedpredictivemodeling.com/},
	doi = {10.1007/978-1-4614-6849-3},
	pagetotal = {600},
	isbn = {9781461468486},
	publisher = Springer,
	language = {english},
	date = {2013},
	title = {Applied Predictive Modeling},
	author = {Kuhn, Max and Johnson, Kjell}
}

@Book{kuhn_johnson:feature_engineering_selection,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013154695804987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	series = CRC_Data_Science,
	keywords = {Ciencia del dato},
	abstract = {The process of developing predictive models includes many stages. Most
resources focus on the modeling algorithms but neglect other critical aspects
of the modeling process. This book describes techniques for finding the best
representations of predictors for modeling and for nding the best subset of
predictors for improving model performance. A variety of example data sets are
used to illustrate the techniques along with R programs for reproducing the
results.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/9781138079229},
	pagetotal = {298},
	isbn = {9781138079229},
	publisher = CRC,
	language = {english},
	subtitle = {A Practical Approach for Predictive Models},
	date = {2019},
	title = {Feature Engineering and Selection},
	author = {Kuhn, Max and Johnson, Kjell}
}

@book{lemieux:monte_carlo_quasi_monte_carlo_sampling,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2094239
Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-0-387-78165-5},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780387781648},
	doi = {10.1007/978-0-387-78165-5},
	language = {english},
	title = {Monte Carlo and Quasi-Monte Carlo Sampling},
	isbn = {9780387781648},
	series = Springer_Statistics,
	abstract = {Quasi–Monte Carlo methods have become an increasingly popular alternative to Monte Carlo methods
over the last two decades. Their successful implementation on practical problems, especially in
finance, has motivated the development of several new research areas within this field to which
practitioners and researchers from various disciplines currently contribute.

This book presents essential tools for using quasi–Monte Carlo sampling in practice. The first part
of the book focuses on issues related to Monte Carlo methods—uniform and non-uniform random number
generation, variance reduction techniques—but the material is presented to prepare the readers for
the next step, which is to replace the random sampling inherent to Monte Carlo by quasi–random
sampling. The second part of the book deals with this next step. Several aspects of quasi-Monte
Carlo methods are covered, including constructions, randomizations, the use of {ANOVA}
decompositions, and the concept of effective dimension. The third part of the book is devoted to
applications in finance and more advanced statistical tools like Markov chain Monte Carlo and
sequential Monte Carlo, with a discussion of their quasi–Monte Carlo counterpart.

The prerequisites for reading this book are a basic knowledge of statistics and enough mathematical
maturity to follow through the various techniques used throughout the book. This text is aimed at
graduate students in statistics, management science, operations research, engineering, and applied
mathematics. It should also be useful to practitioners who want to learn more about Monte Carlo and
quasi–Monte Carlo methods and researchers interested in an up-to-date guide to these methods.},
	pagetotal = {373},
	publisher = Springer,
	author = {Lemieux, Christiane},
	date = {2009},
	keywords = {Estadística computacional, Métodos de Monte Carlo},
	file = {Libros/Lemieux-2009-Monte_Carlo_and_Quasi-Monte_Carlo_Sampling.pdf}
}

@Article{levin:universal_sequential_search_problems,
	keywords = {Complejidad computacional},
	abstract = {Several well-known large-scale problems of the “sequential search” type are
discussed, and it is proved that those problems can be solved only in the time
that it takes to solve any problems of the indicated type, in general.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://mi.mathnet.ru/eng/ppi/v9/i3/p115},
	pages = {115-116},
	number = {3},
	volume = {9},
	language = {russian},
	date = {1973},
	journaltitle = {Problems of Information Transmission},
	title = {Universal Sequential Search Problems},
	author = {Levin, Leonid Anatolievich}
}

@Article{li_et_al:many_objective_evolutionary_algorithms,
	xdata = {acm_computing_surveys},
	file = {Artículos_en_revistas/Li_et_al-2015-Many-Objective_Evolutionary_Algorithms:_A_Survey.pdf; Artículos_en_revistas/Li_et_al-2015-Many-Objective_Evolutionary_Algorithms:_A_Survey-Appendix.pdf},
	keywords = {Algoritmos evolutivos},
	abstract = {Multiobjective evolutionary algorithms (MOEAs) have been widely used in real-world applications.
However, most MOEAs based on Pareto-dominance handle many-objective problems (MaOPs) poorly due to
a high proportion of incomparable and thus mutually nondominated solutions. Recently, a number of
many-objective evolutionary algorithms (MaOEAs) have been proposed to deal with this scalability
issue. In this article, a survey of MaOEAs is reported. According to the key ideas used, MaOEAs are
categorized into seven classes: relaxed dominance based, diversity-based, aggregation-based,
indicator-based, reference set based, preference-based, and dimensionality reduction approaches.
Several future research directions in this field are also discussed.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1145/2792984},
	pagetotal = {35},
	number = {1},
	volume = {48},
	language = {english},
	date = {2015},
	title = {Many-Objective Evolutionary Algorithms: A Survey},
	author = {Li, Bingdong and Li, Jinlong and Tang, Ke and Yao, Xin}
}

@Article{liang:survey_heuristics_domain_independent_planning,
	xdata = {journal_software},
	file = {Artículos_en_revistas/Liang-2012-A_Survey_of_Heuristics_for_Domain-Independent_Planning.pdf},
	keywords = {Inteligencia artificial, Planificación automática},
	abstract = {Increasing interest has been devoted to Planning as Heuristic Search over the years. Intense
research has focused on deriving accurate heuristics in polynomial computational time for
domain-independent planning. This paper reports on an extensive survey and analysis of research
work related to heuristic derivation techniques for state space search planning, as well as other
planning paradigms. Survey results reveal that heuristic techniques have been extensively applied
in many efficient planners and result in impressive performances. We extend the survey analysis to
suggest promising avenues for future research in heuristic derivation and heuristic search
techniques.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.4304/jsw.7.9.2099-2106},
	pages = {2099-2106},
	number = {9},
	volume = {7},
	language = {english},
	date = {2012},
	title = {A Survey of Heuristics for Domain-Independent Planning},
	author = {Liang, Ruishi}
}

@Book{little_rubin:statistical_analysis_missing_data,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/3enc2g/alma991013169904504987
},
	keywords = {Análisis de valores perdidos},
	abstract = {The topic of missing data has gained considerable attention in recent decades.
This new edition by two acknowledged experts on the subject offers an
up-to-date account of practical methodology for handling missing data problems.
Blending theory and application, authors Roderick Little and Donald Rubin
review historical approaches to the subject and describe simple methods for
multivariate analysis with missing values. They then provide a coherent theory
for analysis of problems based on likelihoods derived from statistical models
for the data and the missing data mechanism, and then they apply the theory to
a wide range of important missing data problems.

*Statistical Analysis with Missing Data, Third Edition* starts by introducing
readers to the subject and approaches toward solving it. It looks at the
patterns and mechanisms that create the missing data, as well as a taxonomy of
missing data. It then goes on to examine missing data in experiments, before
discussing complete-case and available-case analysis, including weighting
methods. The new edition expands its coverage to include recent work on topics
such as nonresponse in sample surveys, causal inference, diagnostic methods,
and sensitivity analysis, among a host of other topics.

* An updated “classic” written by renowned authorities on the subject
* Features over 150 exercises (including many new ones)
* Covers recent work on important methods like multiple imputation, robust
  alternatives to weighting, and Bayesian methods
* Revises previous topics based on past student feedback and class experience
* Contains an updated and expanded bibliography
},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.wiley.com/en-es/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9781118595695},
	series = Wiley_Probability_Statistics,
	pagetotal = {464},
	isbn = {9781118595695},
	publisher = Wiley,
	edition = {3},
	language = {english},
	date = {2019},
	title = {Statistical Analysis with Missing Data},
	author = {Little, Roderick J. A. and Rubin, Donald B.}
}

@Article{loh:classification_regression_trees,
	xdata = {wires_data_mining_knowledge_discovery},
	file = {Artículos_en_revistas/Loh-2011-Classification_and_regression_trees.pdf},
	keywords = {Aprendizaje automático, Inteligencia artificial},
	abstract = {Classification and regression trees are machine-learning methods for constructing prediction models
from data. The models are obtained by recursively partitioning the data space and fitting a simple
prediction model within each partition. As a result, the partitioning can be represented
graphically as a decision tree. Classification trees are designed for dependent variables that take
a finite number of unordered values, with prediction error measured in terms of misclassification
cost. Regression trees are for dependent variables that take continuous or ordered discrete values,
with prediction error typically measured by the squared difference between the observed and
predicted values. This article gives an introduction to the subject by reviewing some widely
available algorithms and comparing their capabilities, strengths, and weakness in two examples.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1002/widm.8},
	pages = {14-23},
	number = {1},
	volume = {1},
	language = {english},
	date = {2011},
	title = {Classification and regression trees},
	author = {Loh, Wei-Yin}
}

@Book{lovelace_et_al:geocomputation_r,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013145401404987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	keywords = {Análisis de datos espaciales, Lenguaje de programación R},
	abstract = {**Geocomputation with R** is for people who want to analyze, visualize and
model geographic data with open source software. It is based on R, a
statistical programming language that has powerful data processing,
visualization, and geospatial capabilities. The book equips you with the
knowledge and skills to tackle a wide range of issues manifested in geographic
data, including those with scientific, societal, and environmental
implications. This book will interest people from many backgrounds, especially
Geographic Information Systems (GIS) users interested in applying their
domain-specific knowledge in a powerful open source language for data science,
and R users interested in extending their skills to handle spatial data.

The book is divided into three parts: (I) Foundations, aimed at getting you
up-to-speed with geographic data in R, (II) extensions, which covers advanced
techniques, and (III) applications to real-world problems. The chapters cover
progressively more advanced topics, with early chapters providing strong
foundations on which the later chapters build. Part I describes the nature of
spatial datasets in R and methods for manipulating them. It also covers
geographic data import/export and transforming coordinate reference systems.
Part II represents methods that build on these foundations. It covers advanced
map making (including web mapping), "bridges" to GIS, sharing reproducible
code, and how to do cross-validation in the presence of spatial
autocorrelation. Part III applies the knowledge gained to tackle real-world
problems, including representing and modeling transport systems, finding
optimal locations for stores or services, and ecological modeling. Exercises at
the end of each chapter give you the skills needed to tackle a range of
geospatial problems. Solutions for each chapter and supplementary materials
providing extended examples are available at
https://geocompr.github.io/geocompkg/articles/.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Geocomputation-with-R/Lovelace-Nowosad-Muenchow/p/book/9781138304512},
	pagetotal = {335},
	isbn = {9781138304512},
	publisher = CRC,
	series = CRC_R_Series,
	language = {english},
	date = {2019},
	title = {Geocomputation with R},
	author = {Lovelace, Robin and Nowosad, Jakub and Muenchow, Jannes}
}

@Book{lovász_plummer:matching_theory,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2091091
Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1013909
Recurso electrónico (ScienceDirect): http://www.us.debiblio.com/login?url=http://www.sciencedirect.com/science/publication?issn=03040208&volume=121
},
	file = {Libros/Lovász_Plummer-1986-Matching_Theory.pdf},
	keywords = {Matemática discreta, Teoría de grafos},
	abstract = {This study of matching theory deals with bipartite matching, network flows, and
presents fundamental results for the non-bipartite case. It goes on to study
elementary bipartite graphs and elementary graphs in general. Further discussed
are 2-matchings, general matching problems as linear programs, the Edmonds
Matching Algorithm (and other algorithmic approaches), f-factors and vertex
packing.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.elsevier.com/books/matching-theory/plummer/978-0-444-87916-5},
	pagetotal = {544},
	isbn = {9780444879165},
	publisher = Elsevier,
	number = {29},
	series = Elsevier_Annals_Discrete_Mathematics,
	language = {english},
	date = {1986},
	title = {Matching Theory},
	author = {Lovász, László and Plummer, Michael David}
}

@XData{machine_learning,
	journaltitle = {Machine Learning},
	issn = {0885-6125},
	publisher = Springer
}

@Thesis{macias-ramos:developing_efficient_simulators_cell_machines,
	type = {phdthesis},
	file = {Tesis_doctorales/Macías-Ramos-2015-Developing_efficient_simulators_for_cell_machines.pdf},
	keywords = {Computación bioinspirada, Computación con membranas},
	langidopts = {variant=british},
	langid = {english},
	url = {http://hdl.handle.net/11441/36828},
	pagetotal = {305},
	language = {english},
	date = {2015},
	institution = {Universidad de Sevilla},
	title = {Developing efficient simulators for cell machines},
	author = {Macías Ramos, Luis Felipe}
}

@Article{madden:classification_performance_TAN_Bayesian_networks,
	file = {Artículos_en_revistas/Madden-2009-On_the_classification_performance_of_TAN_and_general_Bayesian_networks.pdf},
	keywords = {Aprendizaje automático, Redes bayesianas},
	abstract = {Over a decade ago, Friedman et al. introduced the Tree Augmented Naïve Bayes
(TAN) classifier, with experiments indicating that it significantly
outperformed Naïve Bayes (NB) in terms of classification accuracy, whereas
general Bayesian network (GBN) classifiers performed no better than NB. This
paper challenges those claims, using a careful experimental analysis to show
that GBN classifiers significantly outperform NB on datasets analyzed, and are
comparable to TAN performance. It is found that the poor performance reported
by Friedman et al. are not attributable to the GBN per se, but rather to their
use of simple empirical frequencies to estimate GBN parameters, whereas basic
parameter smoothing (used in their TAN analyses but not their GBN analyses)
improves GBN performance significantly. It is concluded that, while GBN
classifiers may have some limitations, they deserve greater attention,
particularly in domains where insight into classification decisions, as well as
good accuracy, is required.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1016/j.knosys.2008.10.006},
	pages = {489-495},
	number = {7},
	volume = {22},
	language = {english},
	date = {2009},
	xdata = {knowledge-based_systems},
	title = {On the classification performance of TAN and general Bayesian networks},
	author = {Madden, Michael G.}
}

@Book{mailund:advanced_object_oriented_programming_r,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013154695704987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es|
},
	keywords = {Lenguaje de programación R, Programación orientada a objetos},
	abstract = {Learn how to write object-oriented programs in R and how to construct classes
and class hierarchies in the three object-oriented systems available in R. This
book gives an introduction to object-oriented programming in the R programming
language and shows you how to use and apply R in an object-oriented manner. You
will then be able to use this powerful programming style in your own
statistical programming projects to write flexible and extendable software.

After reading _Advanced Object-Oriented Programming in R_, you'll come away
with a practical project that you can reuse in your own analytics coding
endeavors. You’ll then be able to visualize your data as objects that have
state and then manipulate those objects with polymorphic or generic methods.
Your projects will benefit from the high degree of flexibility provided by
polymorphism, where the choice of concrete method to execute depends on the
type of data being manipulated.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/978-1-4842-2919-4},
	pagetotal = {110},
	isbn = {9781484229187},
	publisher = Apress,
	language = {english},
	subtitle = {Statistical Programming for Data Science, Analysis and Finance},
	date = {2017},
	title = {Advanced Object-Oriented Programming in R},
	author = {Mailund, Thomas}
}

@Book{manzano_huertas:logica_principiantes,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2519398},
	keywords = {Lógica matemática},
	langid = {spanish},
	pagetotal = {422},
	isbn = {9788420645704},
	publisher = Alianza,
	language = {spanish},
	date = {2011},
	title = {Lógica para principiantes},
	author = {Manzano, María and Huertas, Antonia}
}

@Book{masters:assessing_improving_prediction_classification,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/nhtkp3/alma991013169905504987
},
	keywords = {Aprendizaje automático},
	abstract = {Assess the quality of your prediction and classification models in ways that
accurately reflect their real-world performance, and then improve this
performance using state-of-the-art algorithms such as committee-based decision
making, resampling the dataset, and boosting. This book presents many important
techniques for building powerful, robust models and quantifying their expected
behavior when put to work in your application.

Considerable attention is given to information theory, especially as it relates
to discovering and exploiting relationships between variables employed by your
models. This presentation of an often confusing subject avoids advanced
mathematics, focusing instead on concepts easily understood by those with
modest background in mathematics.

All algorithms include an intuitive explanation of operation, essential
equations, references to more rigorous theory, and commented C++ source code.
Many of these techniques are recent developments, still not in widespread use.
Others are standard algorithms given a fresh look. In every case, the emphasis
is on practical applicability, with all code written in such a way that it can
easily be included in any program.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9781484233351},
	doi = {10.1007/978-1-4842-3336-8},
	pagetotal = {517},
	isbn = {9781484233351},
	publisher = Apress,
	language = {english},
	subtitle = {Theory and Algorithms in C++},
	date = {2018},
	title = {Assessing and Improving Prediction and Classification},
	author = {Masters, Timothy}
}

@Book{matloff:art_r_programming,
	enlaces = {Recurso electrónico (E-Libro): http://0-site.ebrary.com.fama.us.es/lib/unisev/Doc?id=105135501},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.nostarch.com/artofr.htm},
	language = {english},
	subtitle = {A Tour of Statistical Software Design},
	title = {The Art of R Programming},
	isbn = {9781593273842},
	abstract = {R is the world's most popular language for developing statistical software: Archaeologists use it
to track the spread of ancient civilizations, drug companies use it to discover which medications
are safe and effective, and actuaries use it to assess financial risks and keep economies running
smoothly.

The Art of R Programming takes you on a guided tour of software development with R, from basic
types and data structures to advanced topics like closures, recursion, and anonymous functions. No
statistical knowledge is required, and your programming skills can range from hobbyist to pro.

Along the way, you'll learn about functional and object-oriented programming, running mathematical
simulations, and rearranging complex data into simpler, more useful formats.},
	pagetotal = {400},
	publisher = No_Starch,
	author = {Matloff, Norman},
	date = {2011},
	keywords = {Lenguaje de programación R}
}

@Online{mikolov_et_al:efficient_estimation_word_representations_vector_space,
	eprintclass = {cs.CL},
	eprinttype = {arxiv},
	eprint = {1301.3781},
	file = {Publicaciones_electrónicas/Mikolov_et_al-2013-Efficient_estimation_of_word_representations_in_vector_space.pdf},
	keywords = {Procesamiento del lenguaje natural},
	abstract = {We propose two novel model architectures for computing continuous vector
representations of words from very large data sets. The quality of these
representations is measured in a word similarity task, and the results are
compared to the previously best performing techniques based on different types
of neural networks. We observe large improvements in accuracy at much lower
computational cost, i.e. it takes less than a day to learn high quality word
vectors from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring
syntactic and semantic word similarities.},
	langidopts = {variant=british},
	langid = {english},
	language = {english},
	date = {2013},
	title = {Efficient Estimation of Word Representations in Vector Space},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey}
}

@Proceedings{miller_et_al:complexity_computer_computations,
	keywords = {Complejidad computacional},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/978-1-4684-2001-2},
	pagetotal = {225},
	isbn = {9781468420036},
	publisher = Springer,
	editor = {Miller, Raymond E. and Thatcher, James W. and Bohlinger, Jean D.},
	language = {english},
	venue = {IBM Thomas J. Watson Research Center, New York, USA},
	eventdate = {1972-03-20/1972-03-22},
	eventtitle = {Symposium on the Complexity of Computer Computations},
	date = {1972},
	title = {Complexity of Computer Computations}
}

@Book{mohanty_et_al:big_data,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2714880},
	keywords = {Macrodatos},
	abstract = {This book is a collection of chapters written by experts on various aspects of big data. The book
aims to explain what big data is and how it is stored and used. The book starts from the
fundamentals and builds up from there. It is intended to serve as a review of the
state-of-the-practice in the field of big data handling. The traditional framework of relational
databases can no longer provide appropriate solutions for handling big data and making it available
and useful to users scattered around the globe. The study of big data covers a wide range of issues
including management of heterogeneous data, big data frameworks, change management, finding
patterns in data usage and evolution, data as a service, service-generated data, service
management, privacy and security. All of these aspects are touched upon in this book. It also
discusses big data applications in different domains. The book will prove useful to students,
researchers, and practicing database and networking engineers.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/gp/book/9788132224938},
	doi = {10.1007/978-81-322-2494-5},
	pagetotal = {184},
	isbn = {9788132224938},
	publisher = Springer,
	number = {11},
	series = Springer_Studies_Big_Data,
	language = {english},
	date = {2015},
	subtitle = {A Primer},
	title = {Big Data},
	editor = {Mohanty, Hrushikesha and Bhuyan, Prachet and Chenthati, Deepak}
}

@Collection{molenberghs_et_al:handbook_missing_data_methodology,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013145401804987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	keywords = {Análisis de valores perdidos},
	abstract = {Missing data affect nearly every discipline by complicating the statistical
analysis of collected data. But since the 1990s, there have been important
developments in the statistical methodology for handling missing data. Written
by renowned statisticians in this area, **Handbook of Missing Data
Methodology** presents many methodological advances and the latest applications
of missing data methods in empirical research.

Divided into six parts, the handbook begins by establishing notation and
terminology. It reviews the general taxonomy of missing data mechanisms and
their implications for analysis and offers a historical perspective on early
methods for handling missing data. The following three parts cover various
inference paradigms when data are missing, including likelihood and Bayesian
methods; semi-parametric methods, with particular emphasis on inverse
probability weighting; and multiple imputation methods.

The next part of the book focuses on a range of approaches that assess the
sensitivity of inferences to alternative, routinely non-verifiable assumptions
about the missing data process. The final part discusses special topics, such
as missing data in clinical trials and sample surveys as well as approaches to
model diagnostics in the missing data setting. In each part, an introduction
provides useful background material and an overview to set the stage for
subsequent chapters.

Covering both established and emerging methodologies for missing data, this
book sets the scene for future research. It provides the framework for readers
to delve into research and practical applications of missing data methods.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Handbook-of-Missing-Data-Methodology/Molenberghs-Fitzmaurice-Kenward-Tsiatis-Verbeke/p/book/9781439854617},
	pagetotal = {598},
	isbn = {9781439854617},
	publisher = CRC,
	series = CRC_Modern_Statistical_Methods,
	language = {english},
	date = {2014},
	title = {Handbook of Missing Data Methodology},
	editor = {Molenberghs, Geert and Fitzmaurice, Garrett and Kenward, Michael G. and Tsiatis, Anastasios and Verbeke, Geert}
}

@Book{moore_mertens:nature_computation,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2715384},
	publisher = Oxford,
	keywords = {Complejidad computacional},
	abstract = {Computational complexity is one of the most beautiful fields of modern mathematics, and it is
increasingly relevant to other sciences ranging from physics to biology. But this beauty is often
buried underneath layers of unnecessary formalism, and exciting recent results like interactive
proofs, phase transitions, and quantum computing are usually considered too advanced for the
typical student. This book bridges these gaps by explaining the deep ideas of theoretical computer
science in a clear and enjoyable fashion, making them accessible to non-computer scientists and to
computer scientists who finally want to appreciate their field from a new point of view. The
authors start with a lucid and playful explanation of the P vs. NP problem, explaining why it is so
fundamental, and so hard to resolve. They then lead the reader through the complexity of mazes and
games; optimization in theory and practice; randomized algorithms, interactive proofs, and
pseudorandomness; Markov chains and phase transitions; and the outer reaches of quantum computing.
At every turn, they use a minimum of formalism, providing explanations that are both deep and
accessible. The book is intended for graduate and undergraduate students, scientists from other
areas who have long wanted to understand this subject, and experts who want to fall in love with
this field all over again.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://global.oup.com/academic/product/the-nature-of-computation-9780199233212},
	pagetotal = {1004},
	isbn = {9780199233212},
	language = {english},
	date = {2011},
	title = {The Nature of Computation},
	author = {Moore, Cristopher and Mertens, Stephan}
}

@Book{muller_et_al:handbook_floating_point_arithmetic,
	enlaces = {Recurso electrónico (SpringerLink): http://www.us.debiblio.com/login?url=http://dx.doi.org/10.1007/978-3-319-76526-6
},
	file = {Libros/Muller_et_al-2018-Handbook_of_Floating-Point_Arithmetic.pdf},
	keywords = {Ciencias de la computación},
	abstract = {This handbook is a definitive guide to the effective use of modern
floating-point arithmetic, which has considerably evolved, from the frequently
inconsistent floating-point number systems of early computing to the recent
IEEE 754-2008 standard. Most of computational mathematics depends on
floating-point numbers, and understanding their various implementations will
allow readers to develop programs specifically tailored for the standard’s
technical features. Algorithms for floating-point arithmetic are presented
throughout the book and illustrated where possible by example programs which
show how these techniques appear in actual coding and design.

The volume itself breaks its core topic into four parts: the basic concepts and
history of floating-point arithmetic; methods of analyzing floating-point
algorithms and optimizing them; implementations of IEEE 754-2008 in hardware
and software; and useful extensions to the standard floating-point system, such
as interval arithmetic, double- and triple-word arithmetic, operations on
complex numbers, and formal verification of floating-point algorithms. This new
edition updates chapters to reflect recent changes to programming languages and
compilers and the new prevalence of GPUs in recent years. The revisions also
add material on fused multiply-add instruction, and methods of extending the
floating-point precision.

As supercomputing becomes more common, more numerical engineers will need to
use number representation to account for trade-offs between various parameters,
such as speed, accuracy, and energy consumption. The _Handbook of
Floating-Point Arithmetic_ is designed for students and researchers in
numerical analysis, programmers of numerical algorithms, compiler designers,
and designers of arithmetic operators.
},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783319765259},
	doi = {10.1007/978-3-319-76526-6},
	pagetotal = {627},
	isbn = {978-3-319-76525-9},
	publisher = Birkhäuser,
	edition = {2},
	language = {english},
	date = {2018},
	title = {Handbook of Floating-Point Arithmetic},
	author = {Muller, Jean-Michel and Brunie, Nicolas and de Dinechin, Florent and Jeannerod, Claude-Pierre and Joldes, Mioara and Lefèvre, Vincent and Melquiond, Guillaume and Revol, Nathalie and Torres, Serge}
}

@thesis{murphy:dynamic_bayesian_networks,
	type = {phdthesis},
	langidopts = {variant=british},
	langid = {english},
	language = {english},
	location = {California, Estados Unidos},
	title = {Dynamic Bayesian Networks: Representation, Inference and Learning},
	abstract = {Modelling sequential data is important in many areas of science and engineering. Hidden Markov
models (HMMs) and Kalman filter models (KFMs) are popular for this because they are simple and
flexible. For example, HMMs have been used for speech recognition and bio-sequence analysis, and
KFMs have been used for problems ranging from tracking planes and missiles to predicting the
economy. However, HMMs and KFMs are limited in their “expressive power”. Dynamic Bayesian Networks
(DBNs) generalize HMMs by allowing the state space to be represented in factored form, instead of
as a single discrete random variable. DBNs generalize KFMs by allowing arbitrary probability
distributions, not just (unimodal) linear-Gaussian. In this thesis, I will discuss how to represent
many different kinds of models as DBNs, how to perform exact and approximate inference in DBNs, and
how to learn DBN models from sequential data.

In particular, the main novel technical contributions of this thesis are as follows: a way of
representing Hierarchical HMMs as DBNs, which enables inference to be done in O(T) time instead of
O(T³), where T is the length of the sequence; an exact smoothing algorithm that takes O(logT) space
instead of O(T); a simple way of using the junction tree algorithm for online inference in DBNs;
new complexity bounds on exact online inference in DBNs; a new deterministic approximate inference
algorithm called factored frontier; an analysis of the relationship between the BK algorithm and
loopy belief propagation; a way of applying Rao-Blackwellised particle filtering to DBNs in
general, and the SLAM (simultaneous localization and mapping) problem in particular; a way of
extending the structural EM algorithm to DBNs; and a variety of different applications of DBNs.
However, perhaps the main value of the thesis is its catholic presentation of the field of
sequential data modelling.},
	pagetotal = {212},
	institution = {University of California, Berkeley},
	author = {Murphy, Kevin Patrick},
	date = {2002},
	keywords = {Inteligencia artificial, Redes bayesianas},
	file = {Tesis_doctorales/Murphy-2002-Dynamic_Bayesian_Networks:_Representation,_Inference_and_Learning.pdf}
}

@Book{murphy:introduction_ai_robotics,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/nhtkp3/alma991013090703504987
},
	keywords = {Inteligencia artificial, Robótica},
	abstract = {This textbook offers a comprehensive survey of artificial intelligence (AI)
algorithms and programming organization for robot systems. Readers who master
the topics covered will be able to design and evaluate an artificially
intelligent robot for applications involving sensing, acting, planning, and
learning. A background in AI is not required; the book introduces key AI topics
from all AI subdisciplines throughout the book and explains how they contribute
to autonomous capabilities.

This second edition is a major expansion and reorganization of the first
edition, reflecting the dramatic advances made in AI over the past fifteen
years. An introductory overview provides a framework for thinking about AI for
robotics, distinguishing between the fundamentally different design paradigms
of automation and autonomy. The book then discusses the reactive functionality
of sensing and acting in AI robotics; introduces the deliberative functions
most often associated with intelligence and the capability of autonomous
initiative; surveys multi-robot systems and (in a new chapter) human-robot
interaction; and offers a “metaview” of how to design and evaluate autonomous
systems and the ethical considerations in doing so. New material covers
locomotion, simultaneous localization and mapping, human-robot interaction,
machine learning, and ethics. Each chapter includes exercises, and many
chapters provide case studies. Endnotes point to additional reading, highlight
advanced topics, and offer robot trivia.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://mitpress.mit.edu/9780262038485},
	pagetotal = {648},
	isbn = {9780262038485},
	publisher = MIT_Press,
	edition = {2},
	language = {english},
	date = {2019},
	title = {Introduction to AI Robotics},
	author = {Murphy, Robin Roberson}
}

@Book{murphy:robotics_through_science_fiction,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/nhtkp3/alma991013169904104987
},
	keywords = {Inteligencia artificial, Robótica},
	abstract = {This book presents six classic science fiction stories and commentary that
illustrate and explain key algorithms or principles of artificial intelligence.
Even though all the stories were originally published before 1973, they help
readers grapple with two questions that stir debate even today: how are
intelligent robots programmed? and what are the limits of autonomous robots?
The stories—by Isaac Asimov, Vernor Vinge, Brian Aldiss, and Philip K.
Dick—cover telepresence, behavior-based robotics, deliberation, testing,
human-robot interaction, the “uncanny valley,” natural language understanding,
machine learning, and ethics. Each story is preceded by an introductory note,
“As You Read the Story,” and followed by a discussion of its implications,
“After You Have Read the Story.” Together with the commentary, the stories
offer a nontechnical introduction to robotics. The stories can also be
considered as a set of—admittedly fanciful—case studies to be read in
conjunction with more serious study.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://mitpress.mit.edu/9780262536264},
	pagetotal = {200},
	isbn = {9780262536264},
	publisher = MIT_Press,
	editor = {Murphy, Robin Roberson},
	language = {english},
	subtitle = {Artificial Intelligence Explained Through Six Classic Robot Short Stories},
	date = {2018},
	title = {Robotics Through Science Fiction}
}

@Book{murray-smith:testing_validation_computer_simulation_models,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2714879},
	series = Springer_Simulation_Foundations,
	keywords = {Ciencias de la computación, Simulación por ordenador},
	abstract = {This must-read text/reference provides a practical guide to processes involved in the development
and application of dynamic simulation models, covering a wide range of issues relating to testing,
verification and validation. Illustrative example problems in continuous system simulation are
presented throughout the book, supported by extended case studies from a number of
interdisciplinary applications. Topics and features: provides an emphasis on practical issues of
model quality and validation, along with questions concerning the management of simulation models,
the use of model libraries, and generic models; contains numerous step-by-step examples; presents
detailed case studies, often with accompanying datasets; includes discussion of hybrid models,
which involve a combination of continuous system and discrete-event descriptions; examines
experimental modeling approaches that involve system identification and parameter estimation;
offers supplementary material at an associated website.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/gp/book/9783319150987},
	doi = {10.1007/978-3-319-15099-4},
	pagetotal = {252},
	isbn = {9783319150987},
	publisher = Springer,
	language = {english},
	date = {2015},
	subtitle = {Principles, Methods and Applications},
	title = {Testing and Validation of Computer Simulation Models},
	author = {Murray-Smith, David J.}
}

@Book{murrel:r_graphics,
	series = CRC_Computer_Science,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1735085},
	keywords = {Lenguaje de programación R, Visualización de datos},
	langidopts = {variant=british},
	langid = {english},
	pagetotal = {301},
	isbn = {9781584884866},
	publisher = CRC,
	language = {english},
	date = {2006},
	title = {R Graphics},
	author = {Murrel, Paul}
}

@Book{murrel:r_graphics_2_ed,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2714850},
	series = CRC_R_Series,
	abstract = {Extensively updated to reflect the evolution of statistics and computing, the second edition of the
bestselling __R Graphics__ comes complete with new packages and new examples. Paul Murrell, widely
known as the leading expert on R graphics, has developed an in-depth resource that helps both
neophyte and seasoned users master the intricacies of R graphics.

__New in the Second Edition__

* Updated information on the core graphics engine, the traditional graphics system, the grid
  graphics system, and the lattice package
* A new chapter on the ggplot2 package
* New chapters on applications and extensions of R Graphics, including geographic maps, dynamic and
  interactive graphics, and node-and-edge graphs

Organized into five parts, __R Graphics__ covers both "traditional" and newer, R-specific graphics
systems. The book reviews the graphics facilities of the R language and describes R’s powerful grid
graphics system. It then covers the graphics engine, which represents a common set of fundamental
graphics facilities, and provides a series of brief overviews of the major areas of application for
R graphics and the major extensions of R graphics.},
	keywords = {Lenguaje de programación R, Visualización de datos},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/R-Graphics-Second-Edition/Murrell/p/book/9781439831762},
	pagetotal = {546},
	isbn = {9781439831762},
	publisher = CRC,
	edition = {2},
	language = {english},
	date = {2011},
	title = {R Graphics},
	author = {Murrel, Paul}
}

@Book{murtaza:getting_started_data_science,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013090791604987&context=L&vid=34CBUA_US:VU1&search_scope=all_libraries_profile&tab=LibrariesSearch&lang=es
},
	publisher = IBM_Press,
	keywords = {Ciencia del dato},
	abstract = {**Master Data Analytics Hands-On by Solving Fascinating Problems You’ll
Actually Enjoy!**

*Harvard Business Review* recently called data science “The Sexiest Job of the
21st Century.” It’s not just sexy: For millions of managers, analysts, and
students who need to solve real business problems, it’s indispensable.
Unfortunately, there’s been nothing easy about learning data science–until now.

**Getting Started with Data Science** takes its inspiration from worldwide
best-sellers like Freakonomics and Malcolm Gladwell’s Outliers: It teaches
through a powerful narrative packed with unforgettable stories.

Murtaza Haider offers informative, jargon-free coverage of basic theory and
technique, backed with plenty of vivid examples and hands-on practice
opportunities. Everything’s software and platform agnostic, so you can learn
data science whether you work with R, Stata, SPSS, or SAS. Best of all, Haider
teaches a crucial skillset most data science books ignore: how to tell powerful
stories using graphics and tables. Every chapter is built around real research
challenges, so you’ll always know why you’re doing what you’re doing.

You’ll master data science by answering fascinating questions, such as:

* Are religious individuals more or less likely to have extramarital affairs?
* Do attractive professors get better teaching evaluations?
* Does the higher price of cigarettes deter smoking?
* What determines housing prices more: lot size or the number of bedrooms?
* How do teenagers and older people differ in the way they use social media?
* Who is more likely to use online dating services?
* Why do some purchase iPhones and others Blackberry devices?
* Does the presence of children influence a family’s spending on alcohol?

For each problem, you’ll walk through defining your question and the answers
you’ll need; exploring how others have approached similar challenges; selecting
your data and methods; generating your statistics; organizing your report; and
telling your story. Throughout, the focus is squarely on what matters most:
transforming data into insights that are clear, accurate, and can be acted
upon.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.informit.com/store/getting-started-with-data-science-making-sense-of-data-9780133991024},
	pagetotal = {608},
	isbn = {9780133991024},
	language = {english},
	subtitle = {Making Sense of Data with Analytics},
	date = {2015},
	title = {Getting Started with Data Science},
	author = {Haider Murtaza}
}

@Book{nagarajan_et_al:bayesian_networks_r,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-1-4614-6446-4},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/978-1-4614-6445-7},
	doi = {10.1007/978-1-4614-6446-4},
	language = {english},
	subtitle = {With Applications in Systems Biology},
	title = {Bayesian Networks in R},
	isbn = {9781461464457},
	series = {Use R!},
	abstract = {_Bayesian Networks in R with Applications in Systems Biology_ is unique as it introduces the reader
to the essential concepts in Bayesian network modeling and inference in conjunction with examples
in the open-source statistical environment R. The level of sophistication is also gradually
increased across the chapters with exercises and solutions for enhanced understanding for hands-on
experimentation of the theory and concepts. The application focuses on systems biology with
emphasis on modeling pathways and signaling mechanisms from high-throughput molecular data.
Bayesian networks have proven to be especially useful abstractions in this regard. Their usefulness
is especially exemplified by their ability to discover new associations in addition to validating
known ones across the molecules of interest. It is also expected that the prevalence of publicly
available high-throughput biological data sets may encourage the audience to explore investigating
novel paradigms using the approaches presented in the book.},
	pagetotal = {157},
	number = {48},
	publisher = Springer,
	author = {Nagarajan, Radhakrishnan and Scutari, Marco and Lèbre, Sophie},
	date = {2013},
	keywords = {Lenguaje de programación R, Modelos gráficos probabilísticos, Redes bayesianas},
	file = {Libros/Nagarajan_et_al-2013-Bayesian_Networks_in_R.pdf}
}

@XData{natural_computing,
	journaltitle = {Natural Computing},
	issn = {1567-7818},
	publisher = Springer
}

@Book{navarro:compact_data_structures,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991012421039704987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	keywords = {Estructuras de datos},
	abstract = {Compact data structures help represent data in reduced space while allowing it
to be queried, navigated, and operated in compressed form. They are essential
tools for efficiently handling massive amounts of data by exploiting the memory
hierarchy. They also reduce the resources needed in distributed deployments and
make better use of the limited memory in low-end devices. The field has
developed rapidly, reaching a level of maturity that allows practitioners and
researchers in application areas to benefit from the use of compact data
structures. This first comprehensive book on the topic focuses on the
structures that are most relevant for practical use. Readers will learn how the
structures work, how to choose the right ones for their application scenario,
and how to implement them. Researchers and students in the area will find in
the book a definitive guide to the state of the art in compact data structures.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.cambridge.org/es/academic/subjects/computer-science/algorithmics-complexity-computer-algebra-and-computational-g/compact-data-structures-practical-approach},
	pagetotal = {570},
	isbn = {9781107152380},
	publisher = Cambridge,
	language = {english},
	subtitle = {A Practical Approach},
	date = {2016},
	title = {Compact Data Structures},
	author = {Navarro, Gonzalo}
}

@Book{neapolitan:learning_bayesian_networks,
	series = Prentice_Hall_Artificial_Intelligence,
	publisher = Prentice_Hall,
	file = {Libros/Neapolitan-2004-Learning_Bayesian_Networks.pdf},
	keywords = {Inteligencia artificial, Redes bayesianas},
	abstract = {For courses in Bayesian Networks or Advanced Networking focusing on Bayesian networks found in
departments of Computer Science, Computer Engineering and Electrical Engineering. Also appropriate
as a supplementary text in courses on Expert Systems, Machine Learning, and Artificial Intelligence
where the topic of Bayesian Networks is covered.

This book provides an accessible and unified discussion of Bayesian networks. It includes
discussions of topics related to the areas of artificial intelligence, expert systems and decision
analysis, the fields in which Bayesian networks are frequently applied. The author discusses both
methods for doing inference in Bayesian networks and influence diagrams. The book also covers the
Bayesian method for learning the values of discrete and continuous parameters. Both the Bayesian
and constraint-based methods for learning structure are discussed in detail.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.pearsonhighered.com/program/Neapolitan-Learning-Bayesian-Networks/PGM134910.html},
	pagetotal = {693},
	isbn = {9780130125347},
	language = {english},
	date = {2004},
	title = {Learning Bayesian Networks},
	author = {Neapolitan, Richard E.}
}

@Article{neil_et_al:building_large-scale_bayesian_networks,
	xdata = {knowledge_engineering_review},
	enlaces = {Recurso electrónico: http://0-journals.cambridge.org.fama.us.es/article_S0269888900003039},
	file = {Artículos_en_revistas/Neil_et_al-2000-Building_large-scale_Bayesian_networks.pdf},
	keywords = {Inteligencia artificial, Redes bayesianas},
	abstract = {Bayesian networks (BNs) model problems that involve uncertainty. A BN is a directed graph, whose
nodes are the uncertain variables and whose edges are the causal or influential links between the
variables. Associated with each node is a set of conditional probability functions that model the
uncertain relationship between the node and its parents. The benefits of using BNs to model
uncertain domains are well known, especially since the recent breakthroughs in algorithms and tools
to implement them. However, there have been serious problems for practitioners trying to use BNs to
solve realistic problems. This is because, although the tools make it possible to _execute_
large-scale BNs efficiently, there have been no guidelines on _building_ BNs. Specifically,
practitioners face two significant barriers. The first barrier is that of specifying the graph
structure such that it is a sensible model of the types of reasoning being applied. The second
barrier is that of eliciting the conditional probability values. In this paper we concentrate on
this first problem. Our solution is based on the notion of generally applicable “building blocks”,
called _idioms_, which serve solution patterns. These can then in turn be combined into larger BNs,
using simple combination rules and by exploiting recent ideas on modular and object oriented BNs
(OOBNs). This approach, which has been implemented in a BN tool, can be applied in many problem
domains. We use examples to illustrate how it has been applied to build large-scale BNs for
predicting software safety. In the paper we review related research from the knowledge and software
engineering literature. This provides some context to the work and supports our argument that BN
knowledge engineers require the same types of processes, methods and strategies enjoyed by systems
and software engineers if they are to succeed in producing timely, quality and cost-effective BN
decision support solutions.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://journals.cambridge.org/article_S0269888900003039},
	pages = {257-284},
	number = {03},
	volume = {15},
	language = {english},
	date = {2000-09},
	title = {Building large-scale Bayesian networks},
	author = {Neil, Martin and Fenton, Norman and Nielson, Lars}
}

@Book{nelli:python_data_analytics,
	enlaces = {Recurso electrónico: http://encore.fama.us.es/iii/encore/record/C__Rb2693011},
	keywords = {Lenguaje de programación Python},
	abstract = {_Python Data Analytics_ will help you tackle the world of data acquisition and analysis using the
power of the Python language. At the heart of this book lies the coverage of pandas, an open
source, BSD-licensed library providing high-performance, easy-to-use data structures and data
analysis tools for the Python programming language.

Author Fabio Nelli expertly shows the strength of the Python programming language when applied to
processing, managing and retrieving information. Inside, you will see how intuitive and flexible it
is to discover and communicate meaningful patterns of data using Python scripts, reporting systems,
and data export. This book examines how to go about obtaining, processing, storing, managing and
analyzing data using the Python programming language.

You will use Python and other open source tools to wrangle data and tease out interesting and
important trends in that data that will allow you to predict future patterns. Whether you are
dealing with sales data, investment data (stocks, bonds, etc.), medical data, web page usage, or
any other type of data set, Python can be used to interpret, analyze, and glean information from a
pile of numbers and statistics.

This book is an invaluable reference with its examples of storing and accessing data in a database;
it walks you through the process of report generation; it provides three real world case studies or
examples that you can take with you for your everyday analysis needs.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.apress.com/9781484209592},
	pagetotal = {364},
	isbn = {9781484209592},
	publisher = Apress,
	language = {english},
	date = {2015},
	subtitle = {Data Analysis and Science using {PANDAs}, {matplotlib} and the Python Programming Language},
	title = {Python Data Analytics},
	author = {Nelli, Fabio}
}

@Proceedings{nips_2013,
	keywords = {Aprendizaje automático, Redes neuronales},
	langidopts = {variant=british},
	langid = {english},
	language = {english},
	venue = {Lake Tahoe, Nevada, USA},
	eventdate = {2013-12-05/2013-12-10},
	eventtitleaddon = {NIPS 2013},
	eventtitle = {27th Annual Conference on Neural Information Processing System},
	title = {Advances in Neural Information Processing Systems}
}

@Book{noback:principles_package_design,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013145401504987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	keywords = {Programación},
	abstract = {Apply design principles to your classes, preparing them for reuse. You will use
package design principles to create packages that are just right in terms of
cohesion and coupling, and are user- and maintainer-friendly at the same time.

The first part of this book walks you through the five SOLID principles that
will help you improve the design of your classes. The second part introduces
you to the best practices of package design, and covers both package cohesion
principles and package coupling principles. Cohesion principles show you which
classes should be put together in a package, when to split packages, and if a
combination of classes may be considered a "package" in the first place.
Package coupling principles help you choose the right dependencies and prevent
wrong directions in the dependency graph of your packages.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9781484241189},
	doi = {10.1007/978-1-4842-4119-6},
	pagetotal = {275},
	isbn = {9781484241189},
	publisher = Apress,
	language = {english},
	subtitle = {Creating Reusable Software Components},
	date = {2018},
	title = {Principles of Package Design},
	author = {Noback, Matthias}
}

@Book{ohagan_et_al:uncertain_judgements,
	subtitle = {Eliciting Experts' Probabilities},
	keywords = {Inteligencia artificial, Redes bayesianas},
	abstract = {Elicitation is the process of extracting expert knowledge about some unknown quantity or
quantities, and formulating that information as a probability distribution. Elicitation is
important in situations, such as modelling the safety of nuclear installations or assessing the
risk of terrorist attacks, where expert knowledge is essentially the only source of good
information. It also plays a major role in other contexts by augmenting scarce observational data,
through the use of Bayesian statistical methods. However, elicitation is not a simple task, and
practitioners need to be aware of a wide range of research findings in order to elicit expert
judgements accurately and reliably. _Uncertain Judgements_ introduces the area, before guiding the
reader through the study of appropriate elicitation methods, illustrated by a variety of
multi-disciplinary examples.

This is achieved by:

* Presenting a methodological framework for the elicitation of expert knowledge incorporating
  findings from both statistical and psychological research.
* Detailing techniques for the elicitation of a wide range of standard distributions, appropriate
  to the most common types of quantities.
* Providing a comprehensive review of the available literature and pointing to the best practice
  methods and future research needs.
* Using examples from many disciplines, including statistics, psychology, engineering and health
  sciences.
* Including an extensive glossary of statistical and psychological terms.

An ideal source and guide for statisticians and psychologists with interests in expert judgement or
practical applications of Bayesian analysis, _Uncertain Judgements_ will also benefit
decision-makers, risk analysts, engineers and researchers in the medical and social sciences.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470029994.html},
	pagetotal = {338},
	isbn = {9780470029992},
	publisher = Wiley,
	series = Statistics_Practice,
	language = {english},
	date = {2006},
	title = {Uncertain Judgements},
	author = {O'Hagan, Anthony and Buck, Caitlin E. and Daneshkhah, Alireza and Eiser, J. Richard and Garthwaite, Paul H. and Jenkinson, David J. and Oakley, Jeremy E. and Rakow, Tim}
}

@XData{operations_research_perspectives,
	journaltitle = {Operations Research Perspectives},
	issn = {2214-7160},
	publisher = Elsevier
}

@Book{oregan:concise_guide_software_engineering,
	series = Springer_Undergraduate_Topics,
	keywords = {Ingeniería del software, Libro solicitado, No disponible en la BUS},
	abstract = {This essential textbook presents a concise introduction to the fundamental
principles of software engineering, together with practical guidance on how to
apply the theory in a real-world, industrial environment. The wide-ranging
coverage encompasses all areas of software design, management, and quality.

Topics and features: presents a broad overview of software engineering,
including software lifecycles and phases in software development, and project
management for software engineering; examines the areas of requirements
engineering, software configuration management, software inspections, software
testing, software quality assurance, and process quality; covers topics on
software metrics and problem solving, software reliability and dependability,
and software design and development, including Agile approaches; explains
formal methods, a set of mathematical techniques to specify and derive a
program from its specification, introducing the Z specification language;
discusses software process improvement, describing the CMMI model, and
introduces UML, a visual modelling language for software systems; reviews a
range of tools to support various activities in software engineering, and
offers advice on the selection and management of a software supplier; describes
such innovations in the field of software as distributed systems,
service-oriented architecture, software as a service, cloud computing, and
embedded systems; includes key learning topics, summaries and review questions
in each chapter, together with a useful glossary.

This practical and easy-to-follow textbook/reference is ideal for computer
science students seeking to learn how to build high quality and reliable
software on time and on budget. The text also serves as a self-study primer for
software engineers, quality professionals, and software managers.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/978-3-319-57750-0},
	pagetotal = {331},
	isbn = {9783319577494},
	publisher = Springer,
	language = {english},
	subtitle = {From Fundamentals to Application Methods},
	date = {2017},
	title = {Concise Guide to Software Engineering},
	author = {O'Regan, Gerard}
}

@Book{oregan:concise_guide_software_testing,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/nhtkp3/alma991013154695604987
},
	keywords = {Programación},
	abstract = {This practically-focused textbook provides a concise and accessible
introduction to the field of software testing, explaining the fundamental
principles and offering guidance on applying the theory in an industrial
environment.

Topics and features: presents a brief history of software quality and its
influential pioneers, as well as a discussion of the various software
lifecycles used in software development; describes the fundamentals of testing
in traditional software engineering, and the role that static testing plays in
building quality into a product; explains the process of software test
planning, test analysis and design, and test management; discusses test
outsourcing, and test metrics and problem solving; reviews the tools available
to support software testing activities, and the benefits of a software process
improvement initiative; examines testing in the Agile world, and the
verification of safety critical systems; considers the legal and ethical
aspects of software testing, and the importance of software configuration
management; provides key learning topics and review questions in every chapter,
and supplies a helpful glossary at the end of the book.

This easy-to-follow guide is an essential resource for undergraduate students
of computer science seeking to learn about software testing, and how to build
high quality and reliable software on time and on budget. The work will also be
of interest to industrialists including software engineers, software testers,
quality professionals and software managers, as well as the motivated general
reader.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783030284930},
	doi = {10.1007/978-3-030-28494-7},
	pagetotal = {293},
	isbn = {9783030284930},
	publisher = Springer,
	series = Springer_Undergraduate_Topics,
	language = {english},
	title = {Concise Guide to Software Testing},
	author = {O'Regan, Gerard}
}

@Book{oregan:world_computing,
	keywords = {Ciencias de la computación},
	abstract = {This engaging work provides a concise introduction to the exciting world of
computing, encompassing the theory, technology, history, and societal impact of
computer software and computing devices. Spanning topics from global conflict
to home gaming, international business, and human communication, this text
reviews the key concepts unpinning the technology which has shaped the modern
world.

Topics and features: introduces the foundations of computing, the fundamentals
of algorithms, and the essential concepts from mathematics and logic used in
computer science; presents a concise history of computing, discussing the
historical figures who made important contributions, and the machines which
formed major milestones; examines the fields of human−computer interaction, and
software engineering; provides accessible introductions to the core aspects of
programming languages, operating systems, and databases; describes the Internet
revolution, the invention of the smartphone, and the rise of social media, as
well as the Internet of Things and cryptocurrencies; explores legal and ethical
aspects of computing, including issues of hacking and cybercrime, and the
nature of online privacy, free speech and censorship; discusses such
innovations as distributed systems, service-oriented architecture, software as
a service, cloud computing, and embedded systems; includes key learning topics
and review questions in every chapter, and a helpful glossary.

Offering an enjoyable overview of the fascinating and broad-ranging field of
computing, this easy-to-understand primer introduces the general reader to the
ideas on which the digital world was built, and the historical developments
that helped to form the modern age.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783319758435},
	doi = {10.1007/978-3-319-75844-2},
	pagetotal = {320},
	isbn = {9783319758435},
	publisher = Springer,
	language = {english},
	subtitle = {A Primer Companion for the Digital Age},
	date = {2018},
	title = {World of Computing},
	author = {O'Regan, Gerard}
}

@Book{orr_muller:neural_networks_tricks_trade,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/3-540-49430-8},
	file = {Libros/Orr_Müller-1998-Neural_Networks:_Tricks_of_the_Trade.pdf},
	keywords = {Inteligencia artificial, Redes neuronales},
	abstract = {It is our belief that researchers and practitioners acquire, through experience and word-of-mouth,
techniques and heuristics that help them successfully apply neural networks to di cult real world
problems. Often these "tricks" are theoretically well motivated. Sometimes they are the result of
trial and error. However, their most common link is that they are usually hidden in people’s heads
or in the back pages of space-constrained conference papers. As a result newcomers to the eld waste
much time wondering why their networks train so slowly and perform so poorly. This book is an
outgrowth of a 1996 NIPS workshop called Tricks of the Trade whose goal was to begin the process of
gathering and documenting these tricks. The interest that the workshop generated motivated us to
expand our collection and compile it into this book. Although we have no doubt that there are many
tricks we have missed, we hope that what we have included will prove to be useful, particularly to
those who are relatively new to the eld. Each chapter contains one or more tricks presented by a
given author (or authors). We have attempted to group related chapters into sections, though we
recognize that the di erent sections are far from disjoint. Some of the chapters (e.g., 1, 13, 17)
contain entire systems of tricks that are far more general than the category they have been placed
in.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783540494300},
	doi = {10.1007/3-540-49430-8},
	pagetotal = {432},
	isbn = {9783540653110},
	publisher = Springer,
	number = {1524},
	language = {english},
	editor = {Orr, Genevieve B. and Müller, Klaus-Robert},
	date = {1998},
	title = {Neural Networks: Tricks of the Trade}
}

@Book{owen:monte_carlo_theory_methods_examples,
	file = {Libros/Owen-Monte_Carlo_theory,_methods_and_examples.pdf},
	keywords = {Métodos de Monte Carlo, Simulación estocástica},
	langidopts = {variant=british},
	langid = {english},
	url = {https://statweb.stanford.edu/~owen/mc/},
	language = {english},
	title = {Monte Carlo theory, methods and examples},
	author = {Owen, Art B.}
}

@Book{parr:definitive_antlr_reference,
	publisher = Pragmatic_Bookshelf,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2495711
},
	file = {Libros/Parr-2012-The_Definitive_ANTLR_4_Reference.pdf},
	keywords = {Análisis de lenguajes},
	abstract = {Build your own languages with ANTLR v4, using ANTLR’s new advanced parsing
technology. In this book, you’ll learn how ANTLR automatically builds a data
structure representing the input (parse tree) and generates code that can walk
the tree (visitor). You can use that combination to implement data readers,
language interpreters, and translators.

You’ll start by learning how to identify grammar patterns in language reference
manuals and then slowly start building increasingly complex grammars. Next,
you’ll build applications based upon those grammars by walking the
automatically generated parse trees. Then you’ll tackle some nasty language
problems by parsing files containing more than one language (such as XML, Java,
and Javadoc). You’ll also see how to take absolute control over parsing by
embedding Java actions into the grammar.

You’ll learn directly from well-known parsing expert Terence Parr, the ANTLR
creator and project lead. You’ll master ANTLR grammar construction and learn
how to build language tools using the built-in parse tree visitor mechanism.
The book teaches using real-world examples and shows you how to use ANTLR to
build such things as a data file reader, a JSON to XML translator, an R parser,
and a Java class->interface extractor. This book is your ticket to becoming a
parsing guru!
},
	langidopts = {variant=british},
	langid = {english},
	url = {https://pragprog.com/book/tpantlr2/the-definitive-antlr-4-reference},
	pagetotal = {305},
	isbn = {9781934356999},
	language = {english},
	date = {2012},
	title = {The Definitive ANTLR 4 Reference},
	author = {Parr, Terence}
}

@XData{pattern_recognition,
	journaltitle = {Pattern Recognition},
	issn = {0031-3203},
	publisher = Elsevier
}

@XData{pattern_recognition_letters,
	journaltitle = {Pattern Recognition Letters},
	issn = {0167-8655},
	publisher = Elsevier
}

@Book{patterson_gibson:deep_learning,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/nhtkp3/alma991012471029704987
},
	file = {Libros/Patterson_Gibson-2017-Deep_Learning.pdf},
	keywords = {Aprendizaje automático, Aprendizaje profundo},
	abstract = {Although interest in machine learning has reached a high point, lofty
expectations often scuttle projects before they get very far. How can machine
learning—especially deep neural networks—make a real difference in your
organization? This hands-on guide not only provides the most practical
information available on the subject, but also helps you get started building
efficient deep learning networks.

Authors Adam Gibson and Josh Patterson provide theory on deep learning before
introducing their open-source Deeplearning4j (DL4J) library for developing
production-class workflows. Through real-world examples, you’ll learn methods
and strategies for training deep network architectures and running deep
learning workflows on Spark and Hadoop with DL4J.

* Dive into machine learning concepts in general, as well as deep learning in particular
* Understand how deep networks evolved from neural network fundamentals
* Explore the major deep network architectures, including Convolutional and Recurrent
* Learn how to map specific deep networks to the right problem
* Walk through the fundamentals of tuning general neural networks and specific deep network architectures
* Use vectorization techniques for different data types with DataVec, DL4J’s workflow tool
* Learn how to use DL4J natively on Spark and Hadoop
},
	langidopts = {variant=british},
	langid = {english},
	url = {http://shop.oreilly.com/product/0636920035343.do},
	pagetotal = {532},
	isbn = {9781491914250},
	publisher = OReilly,
	language = {english},
	subtitle = {A Practitioner’s Approach},
	date = {2017},
	title = {Deep Learning},
	author = {Patterson, Josh and Gibson, Adam}
}

@Book{pearl:heuristics,
	file = {Libros/Pearl-1985-Heuristics.pdf},
	keywords = {Búsqueda heurística, Inteligencia artificial},
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1056962
},
	langidopts = {variant=british},
	langid = {english},
	pagetotal = {382},
	publisher = Addison-Wesley,
	isbn = {9780201055948},
	series = {Addison-Wesley Series in Artificial Intelligence},
	language = {english},
	subtitle = {Intelligent Search Strategies for Computer Problem Solving},
	date = {1985},
	title = {Heuristics},
	author = {Pearl, Judea}
}

@Article{perez-jimenez_et_al:FRSN_P_systems_revisited,
	xdata = {theoretical_computer_science},
	file = {Artículos_en_revistas/Pérez-Jiménez_et_al-2017-Fuzzy_reasoning_spiking_neural_P_systems_revisited:_A_formalization.pdf},
	keywords = {Computación con membranas, Lógica difusa},
	abstract = {Research interest within membrane computing is becoming increasingly
interdisciplinary. In particular, one of the latest applications is fault
diagnosis. The underlying mechanism was conceived by bridging spiking neural P
systems with fuzzy rule-based reasoning systems. Despite having a number of
publications associated with it, this research line still lacks a proper
formalization of the foundations.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1016/j.tcs.2017.04.014},
	language = {english},
	date = {2017},
	title = {Fuzzy reasoning spiking neural P systems revisited: A formalization},
	author = {Pérez-Jiménez, Mario J. and Graciani, Carmen and Orellana-Martín, David and Riscos-Núñez, Agustín and Romero-Jiménez, Álvaro and Valencia-Cabrera, Luis}
}

@Book{perpiñan-lamigueiro:displaying_time_series_spatial_space-time_data,
	keywords = {Lenguaje de programación R, Libro solicitado, No disponible en la BUS, Visualización de datos},
	abstract = {Focusing on the exploration of data with visual methods, *Displaying Time
Series, Spatial, and Space-Time Data with R, Second Edition*, presents methods
and R code for producing high-quality static graphics, interactive
visualizations, and animations of time series, spatial, and space-time data.
Practical examples using real-world datasets help you understand how to apply
the methods and code.

The book illustrates how to display a dataset starting with an easy and direct
approach, and progressively adds improvements that involve more complexity. Each
of the three parts of the book is devoted to different types of data. In each
part, the chapters are grouped according to the various visualization methods or
data characteristics.

The first edition of this book was mainly focused on static graphics. Four years
later, recent developments in the "htmlwidgets" family of packages are covered
in this second edition with many new interactive graphics. In addition, the
"ggplot2" approach is now used in most of the spatial graphics, thanks to the
new "sf" package. Finally, code has been cleaned and improved, and data has been
updated.

Features

* Offers detailed information on producing high-quality graphics, interactive
  visualizations, and animations
* Uses real data from meteorological, climate, economic, social science, energy,
  engineering, environmental, and epidemiological research in many practical
  examples
* Shows how to improve graphics based on visualization theory
* Provides the graphics, data, and R code on the author’s website, enabling you
  to practice with the methods and modify the code to suit your own needs.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.routledge.com/Displaying-Time-Series-Spatial-and-Space-Time-Data-with-R/Lamigueiro/p/book/9781138089983},
	pagetotal = {270},
	isbn = {9781138089983},
	publisher = CRC,
	edition = {2},
	language = {english},
	date = {2018},
	title = {Displaying Time Series, Spatial, and Space-Time Data with R},
	author = {Perpiñán Lamigueiro, Óscar}
}

@Book{peters:foundations_computer_vision,
	file = {Libros/Peters-2017-Foundations_of_Computer_Vision.pdf},
	keywords = {Visión por ordenador},
	abstract = {This book introduces the fundamentals of computer vision (CV), with a focus on
extracting useful information from digital images and videos. Including a
wealth of methods used in detecting and classifying image objects and their
shapes, it is the first book to apply a trio of tools (computational geometry,
topology and algorithms) in solving CV problems, shape tracking in image object
recognition and detecting the repetition of shapes in single images and video
frames. Computational geometry provides a visualization of topological
structures such as neighborhoods of points embedded in images, while image
topology supplies us with structures useful in the analysis and classiﬁcation
of image regions. Algorithms provide a practical, step-by-step means of viewing
image structures.

The implementations of CV methods in Matlab and Mathematica, classiﬁcation of
chapter problems with the symbols (easily solved) and (challenging) and its
extensive glossary of key words, examples and connections with the fabric of CV
make the book an invaluable resource for advanced undergraduate and ﬁrst year
graduate students in Engineering, Computer Science or Applied Mathematics.

It offers insights into the design of CV experiments, inclusion of image
processing methods in CV projects, as well as the reconstruction and
interpretation of recorded natural scenes.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783319524818},
	doi = {10.1007/978-3-319-52483-2},
	pagetotal = {431},
	isbn = {9783319524818},
	publisher = Springer,
	number = {124},
	series = Intelligent_Systems_Library,
	language = {english},
	subtitle = {Computational Geometry, Visual Image Structures and Object Shape Detection},
	date = {2017},
	title = {Foundations of Computer Vision},
	author = {Peters, James F.}
}

@Book{poole_mackworth:artificial_intelligence,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2196688},
	keywords = {Inteligencia artificial},
	langidopts = {variant=british},
	langid = {english},
	language = {english},
	author = {Poole, David L. and Mackworth, Alan K.},
	title = {Artificial Intelligence},
	subtitle = {Foundations of Computational Agents},
	pagetotal = {662},
	publisher = Cambridge,
	date = {2010},
	isbn = {9780521519007},
	url = {http://artint.info/}
}

@Book{pourret_et_al:bayesian_networks,
	subtitle = {A Practical Guide to Applications},
	series = Statistics_Practice,
	file = {Libros/Pourret_et_al-2008-Bayesian_Networks-A_Practical_Guide_to_Applications.pdf},
	keywords = {Inteligencia artificial, Redes bayesianas},
	abstract = {Bayesian Networks, the result of the convergence of artificial intelligence with statistics, are
growing in popularity. Their versatility and modelling power is now employed across a variety of
fields for the purposes of analysis, simulation, prediction and diagnosis.

This book provides a general introduction to Bayesian networks, defining and illustrating the basic
concepts with pedagogical examples and twenty real-life case studies drawn from a range of fields
including medicine, computing, natural sciences and engineering.

Designed to help analysts, engineers, scientists and professionals taking part in complex decision
processes to successfully implement Bayesian networks, this book equips readers with proven methods
to generate, calibrate, evaluate and validate Bayesian networks.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470060301.html},
	doi = {10.1002/9780470994559},
	pagetotal = {446},
	isbn = {9780470060308},
	publisher = Wiley,
	language = {english},
	editor = {Pourret, Olivier and Naïm, Patrick and Marcot, Bruce},
	date = {2008},
	title = {Bayesian Networks}
}

@Article{prasad_et_al:survey_SAT-based_formal_verification,
	xdata = {software_tools_technology_transfer},
	file = {Artículos_en_revistas/Prasad_et_el-2005-A_survey_of_recent_advances_in_SAT-based_formal_verification.pdf},
	keywords = {Problema SAT, Verificación de modelos},
	abstract = {Dramatic improvements in SAT solver technology over the last decade and the
growing need for more efficient and scalable verification solutions have fueled
research in verification methods based on SAT solvers. This paper presents a
survey of the latest developments in SAT-based formal verification, including
incomplete methods such as bounded model checking and complete methods for
model checking. We focus on how the surveyed techniques formulate the
verification problem as a SAT problem and how they exploit crucial aspects of a
SAT solver, such as application-specific heuristics and conflict-driven
learning. Finally, we summarize the noteworthy achievements in this area so far
and note the major challenges in making this technology more pervasive in
industrial design verification flows.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/s10009-004-0183-4},
	pages = {156-173},
	volume = {7},
	language = {english},
	date = {2005},
	title = {A survey of recent advances in SAT-based formal verification},
	author = {Prasad, Mukul R. and Biere, Armin and Gupta, Aarti}
}

@Book{privault:understanding_markov_chains,
	series = Springer_Undergraduate_Mathematics,
	enlaces = {Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-981-4451-51-2},
	file = {Libros/Privault-2013-Understanding_Markov_Chains.pdf},
	keywords = {Estadística, Procesos de Markov},
	abstract = {This book provides an undergraduate introduction to discrete and continuous-time Markov chains and
their applications. A large focus is placed on the first step analysis technique and its
applications to average hitting times and ruin probabilities. Classical topics such as recurrence
and transience, stationary and limiting distributions, as well as branching processes, are also
covered. Two major examples (gambling processes and random walks) are treated in detail from the
beginning, before the general theory itself is presented in the subsequent chapters. An
introduction to discrete-time martingales and their relation to ruin probabilities and mean exit
times is also provided, and the book includes a chapter on spatial Poisson processes with some
recent results on moment identities and deviation inequalities for Poisson stochastic integrals.
The concepts presented are illustrated by examples and by 72 exercises and their complete
solutions.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9789814451505},
	doi = {10.1007/978-981-4451-51-2},
	pagetotal = {354},
	isbn = {9789814451505},
	publisher = Springer,
	language = {english},
	subtitle = {Examples and Applications},
	date = {2013},
	title = {Understanding Markov Chains},
	author = {Privault, Nicolas}
}

@Online{r_project_statistical_computing,
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.r-project.org/},
	title = {The R Project for Statistical Computing}
}

@Book{ramalho:fluent_python,
	enlaces = {Recurso electrónico: http://0-proquest.safaribooksonline.com.fama.us.es//?uiCode=sevil&xmlId=9781491946237},
	keywords = {Lenguaje de programación Python},
	abstract = {Python’s simplicity lets you become productive quickly, but this often means you aren’t using
everything it has to offer. With this hands-on guide, you’ll learn how to write effective,
idiomatic Python code by leveraging its best—and possibly most neglected—features. Author Luciano
Ramalho takes you through Python’s core language features and libraries, and shows you how to make
your code shorter, faster, and more readable at the same time.

Many experienced programmers try to bend Python to fit patterns they learned from other languages,
and never discover Python features outside of their experience. With this book, those Python
programmers will thoroughly learn how to become proficient in Python 3.

This book covers:
* __Python data model__: understand how special methods are the key to the consistent behavior of
  objects
* __Data structures__: take full advantage of built-in types, and understand the text vs bytes
  duality in the Unicode age
* __Functions as objects__: view Python functions as first-class objects, and understand how this
  affects popular design patterns
* __Object-oriented idioms__: build classes by learning about references, mutability, interfaces,
  operator overloading, and multiple inheritance
* __Control flow__: leverage context managers, generators, coroutines, and concurrency with the
  concurrent.futures and asyncio packages
* __Metaprogramming__: understand how properties, attribute descriptors, class decorators, and
  metaclasses work

},
	langidopts = {variant=british},
	langid = {english},
	url = {http://shop.oreilly.com/product/0636920032519.do},
	pagetotal = {792},
	isbn = {9781491946008},
	publisher = OReilly,
	language = {english},
	date = {2105},
	subtitle = {Clear, Concise, and Effective Programming},
	title = {Fluent Python},
	author = {Ramalho, Luciano}
}

@Book{ramasubramanian_singh:machine_learning_using_r,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2748659},
	keywords = {Aprendizaje automático, Inteligencia artificial, Lenguaje de programación R},
	abstract = {Examine the latest technological advancements in building a scalable machine
learning model with Big Data using R. This book shows you how to work with a
machine learning algorithm and use it to build a ML model from raw data.

All practical demonstrations will be explored in R, a powerful programming
language and software environment for statistical computing and graphics. The
various packages and methods available in R will be used to explain the topics.
For every machine learning algorithm covered in this book, a 3-D approach of
theory, case-study and practice will be given. And where appropriate, the
mathematics will be explained through visualization in R.

This new paradigm of teaching machine learning will bring about a radical
change in perception for many of those who think this subject is difficult to
learn. Though theory sometimes looks difficult, especially when there is heavy
mathematics involved, the seamless flow from the theoretical aspects to
example-driven learning provided in this book makes it easy for someone to
connect the dots.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781484223338},
	doi = {10.1007/978-1-4842-2334-5},
	pagetotal = {566},
	isbn = {9781484223338},
	publisher = Springer,
	language = {english},
	date = {2017},
	title = {Machine Learning Using R},
	author = {Ramasubramanian, Karthik and Singh, Abhishek}
}

@XData{reliability_engineering_system_safety,
	journaltitle = {Reliability Engineering and System Safety},
	issn = {0951-8320},
	publisher = Elsevier
}

@Book{robbins:creating_more_effective_graphs,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1735477
},
	keywords = {Visualización de datos},
	langidopts = {variant=british},
	langid = {english},
	pagetotal = {402},
	isbn = {9780471274025},
	publisher = Wiley,
	language = {english},
	date = {2005},
	title = {Creating More Effective Graphs},
	author = {Robbins, Naomi Bograd}
}

@book{robert_casella:introducing_monte_carlo_methods_r,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2118918
Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-1-4419-1576-4},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781441915757},
	doi = {10.1007/978-1-4419-1576-4},
	language = {english},
	title = {Introducing Monte Carlo Methods with R},
	isbn = {9781441915757},
	series = Springer_Use_R,
	abstract = {Computational techniques based on simulation have now become an essential part of the
statistician's toolbox. It is thus crucial to provide statisticians with a practical understanding
of those methods, and there is no better way to develop intuition and skills for simulation than to
use simulation to solve statistical problems. *Introducing Monte Carlo Methods with R* covers the
main tools used in statistical simulation from a programmer's point of view, explaining the R
implementation of each simulation technique and providing the output for better understanding and
comparison. While this book constitutes a comprehensive treatment of simulation methods, the
theoretical justification of those methods has been considerably reduced, compared with Robert and
Casella (2004). Similarly, the more exploratory and less stable solutions are not covered here.

This book does not require a preliminary exposure to the R programming language or to Monte Carlo
methods, nor an advanced mathematical background. While many examples are set within a Bayesian
framework, advanced expertise in Bayesian statistics is not required. The book covers basic random
generation algorithms, Monte Carlo techniques for integration and optimization, convergence
diagnoses, Markov chain Monte Carlo methods, including Metropolis-Hastings and Gibbs algorithms,
and adaptive algorithms. All chapters include exercises and all R programs are available as an R
package called mcsm. The book appeals to anyone with a practical interest in simulation methods but
no previous exposure. It is meant to be useful for students and practitioners in areas such as
statistics, signal processing, communications engineering, control theory, econometrics, finance
and more. The programming parts are introduced progressively to be accessible to any reader.},
	pagetotal = {284},
	publisher = Springer,
	author = {Robert, Christian P. and Casella, George},
	date = {2010},
	keywords = {Estadística computacional, Métodos de Monte Carlo},
	file = {Libros/Robert_Casella-2010-Introducing_Monte_Carlo_Methods_with_R.pdf}
}

@book{robert_casella:monte_carlo_statistical_methods,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1735497},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780387212395},
	doi = {10.1007/978-1-4757-4145-2},
	language = {english},
	edition = {2},
	title = {Monte Carlo Statistical Methods},
	isbn = {9780387212396},
	series = Springer_Text_Statistics,
	abstract = {Monte Carlo statistical methods, particularly those based on Markov chains, are now an essential
component of the standard set of techniques used by statisticians. This new edition has been
revised towards a coherent and flowing coverage of these simulation techniques, with incorporation
of the most recent developments in the field. In particular, the introductory coverage of random
variable generation has been totally revised, with many concepts being unified through a
fundamental theorem of simulation

There are five completely new chapters that cover Monte Carlo control, reversible jump, slice
sampling, sequential Monte Carlo, and perfect sampling. There is a more in-depth coverage of Gibbs
sampling, which is now contained in three consecutive chapters. The development of Gibbs sampling
starts with slice sampling and its connection with the fundamental theorem of simulation, and
builds up to two-stage Gibbs sampling and its theoretical properties. A third chapter covers the
multi-stage Gibbs sampler and its variety of applications. Lastly, chapters from the previous
edition have been revised towards easier access, with the examples getting more detailed coverage.

This textbook is intended for a second year graduate course, but will also be useful to someone who
either wants to apply simulation techniques for the resolution of practical problems or wishes to
grasp the fundamental principles behind those methods. The authors do not assume familiarity with
Monte Carlo techniques (such as random variable generation), with computer programming, or with any
Markov chain theory (the necessary concepts are developed in Chapter 6). A solutions manual, which
covers approximately 40\% of the problems, is available for instructors who require the book for a
course.},
	pagetotal = {649},
	publisher = Springer,
	author = {Robert, Christian P. and Casella, George},
	date = {2004},
	keywords = {Estadística computacional, Métodos de Monte Carlo}
}

@Article{rodríguez-sánchez_et_al:ciencia_reproducible,
	xdata = {ecosistemas},
	file = {Artículos_en_revistas/Rodríguez-Sánchez_et_al-2016-Ciencia_reproducible:_qué,_por_qué,_cómo.pdf},
	keywords = {Investigación reproducible},
	abstract = {La inmensa mayoría de los estudios científicos no son reproducibles: resulta
muy difícil, si no imposible, trazar todo el proceso de análisis y obtención de
resultados a partir de un conjunto de datos – incluso tratándose de los mismos
investigadores. La trazabilidad y reproducibilidad de los resultados son sin
embargo condiciones inherentes a la ciencia de calidad, y un requisito cada vez
más frecuente por parte de revistas y organismos financiadores de la
investigación. Los estudios científicos reproducibles incluyen código
informático capaz de recrear todos los resultados a partir de los datos
originales. De esta manera el proceso de análisis queda perfectamente
registrado, se reduce drásticamente el riesgo de errores, y se facilita la
reutilización de código para otros análisis. Pero la ciencia reproducible no
sólo acelera el progreso científico sino que también reporta múltiples
beneficios para el investigador como el ahorro de tiempo y esfuerzo, o el
incremento de la calidad e impacto de sus publicaciones. En este artículo
explicamos en qué consiste la reproducibilidad, por qué es necesaria en
ciencia, y cómo podemos hacer ciencia reproducible. Presentamos una serie de
recomendaciones y herramientas para el manejo y análisis de datos, control de
versiones de archivos, organización de ficheros y manejo de programas
informáticos que nos permiten desarrollar flujos de trabajo reproducibles en el
contexto actual de la ecología.},
	langid = {spanish},
	doi = {10.7818/ECOS.2016.25-2.11},
	pages = {83-92},
	number = {2},
	volume = {25},
	language = {spanish},
	date = {2016},
	title = {Ciencia reproducible: qué, por qué, cómo},
	author = {Rodríguez-Sánchez, Francisco and Pérez-Luque, Antonio Jesús and Bartomeus, Ignasi and Varela, Sara}
}

@Book{rose:data_science,
	file = {Libros/Rose-2016-Data_Science-Create_Teams_That_Ask_the_Right_Questions_and_Deliver_Real_Value.pdf},
	keywords = {Ciencia del dato, No disponible en la BUS},
	abstract = {Learn how to build a data science team within your organization rather than
hiring from the outside. Teach your team to ask the right questions to gain
actionable insights into your business.

Most organizations still focus on objectives and deliverables. Instead, a data
science team is exploratory. They use the scientific method to ask interesting
questions and run small experiments. Your team needs to see if the data
illuminate their questions. Then, they have to use critical thinking techniques
to justify their insights and reasoning. They should pivot their efforts to
keep their insights aligned with business value. Finally, your team needs to
deliver these insights as a compelling story.

_Insight!: How to Build Data Science Teams that Deliver Real Business Value_
shows that the most important thing you can do now is help your team think
about data. Management coach Doug Rose walks you through the process of
creating and managing effective data science teams. You will learn how to find
the right people inside your organization and equip them with the right
mindset. The book has three overarching concepts:

* You should mine your own company for talent. You can’t change your
  organization by hiring a few data science superheroes.
* You should form small, agile-like data teams that focus on delivering
  valuable insights early and often.
* You can make real changes to your organization by telling compelling data
  stories. These stories are the best way to communicate your insights about
  your customers, challenges, and industry.

What Your Will Learn:

* Create data science teams from existing talent in your organization to
  cost-efficiently extract maximum business value from your organization’s data
* Understand key data science terms and concepts
* Follow practical guidance to create and integrate an effective data science
  team with key roles and the responsibilities for each team member
* Utilize the data science life cycle (DSLC) to model essential processes and
  practices for delivering value
* Use sprints and storytelling to help your team stay on track and adapt to new
  knowledge
},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.apress.com/9781484222522},
	doi = {10.1007/978-1-4842-2253-9},
	pagetotal = {251},
	isbn = {9781484222522},
	publisher = Apress,
	language = {english},
	subtitle = {Create Teams That Ask the Right Questions and Deliver Real Value},
	date = {2016},
	title = {Data Science},
	author = {Rose, Doug}
}

@Book{rother:pro_python_best_practices,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2793835
Recurso electrónico (Safari Books Online): http://0-proquest.safaribooksonline.com.fama.us.es/?uiCode=sevil&xmlId=9781484222416
},
	keywords = {Lenguaje de programación Python},
	abstract = {Learn software engineering and coding best practices to write Python code right
and error free. In this book you’ll see how to properly debug, organize, test,
and maintain your code, all of which leads to better, more efficient coding.

Software engineering is difficult. Programs of any substantial length are
inherently prone to errors of all kinds. The development cycle is full of traps
unknown to the apprentice developer. Yet, in Python textbooks little attention
is paid to this aspect of getting your code to run. At most, there is a chapter
on debugging or unit testing in your average basic Python book. However, the
proportion of time spent on getting your code to run is much higher in the real
world. _Pro Python Best Practices_ aims to solve this problem.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781484222409},
	doi = {10.1007/978-1-4842-2241-6},
	pagetotal = {264},
	isbn = {9781484222409},
	publisher = Apress,
	language = {english},
	date = {2017},
	subtitle = {Debugging, Testing and Maintenance},
	title = {Pro Python Best Practices},
	author = {Rother, Kristian}
}

@Book{rothlauf:design_modern_heuristics,
	subtitle = {Principles and Application},
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-3-540-72962-4},
	file = {Libros/Rothlauf-2011-Design_of_Modern_Heuristics.pdf},
	keywords = {Inteligencia artificial, Metaheurísticas},
	abstract = {Most textbooks on modern heuristics provide the reader with detailed descriptions of the
functionality of single examples like genetic algorithms, genetic programming, tabu search,
simulated annealing, and others, but fail to teach the underlying concepts behind these different
approaches.

The author takes a different approach in this textbook by focusing on the users' needs and
answering three fundamental questions: First, he tells us which problems modern heuristics are
expected to perform well on, and which should be left to traditional optimization methods. Second,
he teaches us to systematically design the "right" modern heuristic for a particular problem by
providing a coherent view on design elements and working principles. Third, he shows how we can
make use of problem-specific knowledge for the design of efficient and effective modern heuristics
that solve not only small toy problems but also perform well on large real-world problems.

This book is written in an easy-to-read style and it is aimed at students and practitioners in
computer science, operations research and information systems who want to understand modern
heuristics and are interested in a guide to their systematic design and use.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783540729617},
	doi = {10.1007/978-3-540-72962-4},
	pagetotal = {267},
	isbn = {9783540729617},
	publisher = Springer,
	series = Springer_Natural_Computing,
	language = {english},
	date = {2011},
	title = {Design of Modern Heuristics},
	author = {Rothlauf, Franz}
}

@Book{rothlauf:representations_genetic_evolutionary_algorithms,
	file = {Libros/Rothlauf-2006-Representations_for_Genetic_and_Evolutionary_Algorithms.pdf},
	enlaces = {Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/3-540-32444-5},
	keywords = {Algoritmos evolutivos, Algoritmos genéticos},
	abstract = {In the field of genetic and evolutionary algorithms (GEAs), a large amount of theory and empirical
study has focused on operators and test problems, while problem representation has often been taken
as given. This book breaks away from this tradition and provides a comprehensive overview on the
influence of problem representations on GEA performance. The book summarizes existing knowledge
regarding problem representations and describes how basic properties of representations, such as
redundancy, scaling, or locality, influence the performance of GEAs and other heuristic
optimization methods. Using the developed theory, representations can be analyzed and designed in a
theory-guided matter. The theoretical concepts are used for solving integer optimization problems
and network design problems more efficiently. The book is written in an easy-to-read style and is
intended for researchers, practitioners, and students who want to learn about representations. This
second edition extends the analysis of the basic properties of representations and introduces a new
chapter on the analysis of direct representations.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/978-3-540-25059-3},
	doi = {10.1007/3-540-32444-5},
	pagetotal = {325},
	isbn = {9783540250593},
	publisher = Springer,
	edition = {2},
	language = {english},
	date = {2006},
	title = {Representations for Genetic and Evolutionary Algorithms},
	author = {Rothlauf, Franz}
}

@Proceedings{rozenberg_et_al:membrane_computing,
	language = {english},
	volume = {9504},
	series = LNCS,
	publisher = Springer,
	isbn = {9783319284743},
	pagetotal = {387},
	doi = {10.1007/978-3-319-28475-0},
	url = {http://www.springer.com/gp/book/9783319284743},
	langidopts = {variant=british},
	langid = {english},
	keywords = {Computación bioinspirada, Computación con membranas},
	file = {Actas_de_congresos/Rozenberg_et_al-2015-Membrane_Computing.pdf},
	eventtitleaddon = {CMC 2015},
	venue = {Valencia, Spain},
	enlaces = {Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-3-319-28475-0},
	eventdate = {2015-08-17/2015-08-21},
	eventtitle = {16th International Conference on Membrane Computing},
	date = {2015},
	title = {Membrane Computing},
	editor = {Rozenberg, Grzegorz and Salomaa, Arto and Sempere, José. M and Zandron, Claudio }
}

@Book{rumpe:modeling_uml,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/discovery/fulldisplay?docid=alma991013154695504987&context=L&vid=34CBUA_US:VU1&search_scope=all_data_not_idus&tab=all_data_not_idus&lang=es
},
	keywords = {Ingeniería del software, Lenguaje de modelización UML},
	abstract = {This book presents a variant of UML that is especially suitable for agile
development of high-quality software. It adjusts the language UML profile,
called UML/P, for optimal assistance for the design, implementation, and agile
evolution to facilitate its use especially in agile, yet model based
development methods for data intensive or control driven systems.

After a general introduction to UML and the choices made in the development of
UML/P in Chapter 1, Chapter 2 includes a definition of the language elements of
class diagrams and their forms of use as views and representations. Next,
Chapter 3 introduces the design and semantic facets of the Object Constraint
Language (OCL), which is conceptually improved and syntactically adjusted to
Java for better comfort. Subsequently, Chapter 4 introduces object diagrams as
an independent, exemplary notation in UML/P, and Chapter 5 offers a detailed
introduction to UML/P Statecharts. Lastly, Chapter 6 presents a simplified form
of sequence diagrams for exemplary descriptions of object interactions. For
completeness, appendixes A–C describe the full syntax of UML/P, and appendix D
explains a sample application from the E-commerce domain, which is used in all
chapters.

This book is ideal for introductory courses for students and practitioners
alike.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/978-3-319-33933-7},
	pagetotal = {281},
	isbn = {9783319339320},
	publisher = Springer,
	language = {english},
	subtitle = {Language, Concepts, Methods},
	date = {2016},
	title = {Modeling with UML},
	author = {Rumpe, Bernhard}
}

@Book{russell_norvig:artificial_intelligence,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2147524
},
	abstract = {_Artificial Intelligence: A Modern Approach, 3e_ offers the most comprehensive, up-to-date
introduction to the theory and practice of artificial intelligence. Number one in its field, this
textbook is ideal for one or two-semester, undergraduate or graduate-level courses in Artificial
Intelligence.},
	keywords = {Inteligencia artificial},
	langidopts = {variant=british},
	langid = {english},
	url = {http://aima.cs.berkeley.edu/},
	language = {english},
	author = {Russell, Stuart J. and Norvig, Peter},
	title = {Artificial Intelligence: A Modern Approach},
	series = {Prentice Hall Series in Artificial Intelligence},
	pagetotal = {1132},
	publisher = {Pearson},
	edition = {3},
	date = {2009},
	isbn = {9780136042594}
}

@Book{schneider_kirkpatrick:stochastic_optimization,
	series = Springer_Scientific_Computation,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb1917994
Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-3-540-34560-2
},
	file = {Libros/Schneider_Kirkpatrick-2006-Stochastic_Optimization.pdf},
	keywords = {Optimización estocástica},
	abstract = {The search for optimal solutions pervades our daily lives. From the scientific
point of view, optimization procedures play an eminent role whenever exact
solutions to a given problem are not at hand or a compromise has to be sought,
e.g. to obtain a sufficiently accurate solution within a given amount of time.
This book addresses stochastic optimization procedures in a broad manner,
giving an overview of the most relevant optimization philosophies in the first
part. The second part deals with benchmark problems in depth, by applying in
sequence a selection of optimization procedures to them. While having primarily
scientists and students from the physical and engineering sciences in mind,
this book addresses the larger community of all those wishing to learn about
stochastic optimization techniques and how to use them.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783540345596},
	doi = {10.1007/978-3-540-34560-2},
	pagetotal = {568},
	isbn = {9783540345596},
	publisher = Springer,
	language = {english},
	date = {2006},
	title = {Stochastic Optimization},
	author = {Schneider, Johannes Josef and Kirkpatrick, Scott}
}

@Book{schoning:logic_computer_scientists,
	enlaces = {Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-0-8176-4763-6},
	file = {Libros/Schöning-2008-Logic_for_computer_scientists.pdf},
	keywords = {Lógica matemática},
	abstract = {This book introduces the notions and methods of formal logic from a computer science standpoint,
covering propositional logic, predicate logic, and foundations of logic programming. It presents
applications and themes of computer science research such as resolution, automated deduction, and
logic programming in a rigorous but readable way.

The style and scope of the work, rounded out by the inclusion of exercises, make this an excellent
textbook for an advanced undergraduate course in logic for computer scientists.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780817647629},
	doi = {10.1007/978-0-8176-4763-6},
	pagetotal = {168},
	isbn = {9780817647629},
	publisher = Birkhäuser,
	series = Modern_Birkhäuser_Classics,
	language = {english},
	date = {2008},
	title = {Logic for Computer Scientists},
	author = {Schöning, Uwe}
}

@Book{schöning_torán:satisfiability_problem,
	publisher = Lehmanns_Media,
	keywords = {Problema SAT},
	abstract = {The satisfiability problem of propositional logic, SAT for short, is the first
algorithmic problem that was shown to be NP-complete, and is the cornerstone of
virtually all NP-completeness proofs. The SAT problem consists of deciding
whether a given Boolean formula has a “solution”, in the sense of an assignment
to the variables making the entire formula to evaluate to true.

Over the last few years very powerful algorithms have been devised being able
to solve SAT problems with hundreds of thousands of variables. For difficult
(or randomly generated) formulas these algorithms can be compared to the
proverbial search for the needle in a haystack. This book explains how such
algorithms work, for example, by exploiting the structure of the SAT problem
with an appropriate logical calculus, like resolution. But also algorithms
based on “physical” principles are considered.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.lehmanns.de/shop/mathematik-informatik/27340853-9783865415271-the-satisfiability-problem},
	pagetotal = {184},
	isbn = {9783865415271},
	language = {english},
	subtitle = {Algorithms and Analyses},
	date = {2013},
	title = {The Satisfiability Problem},
	author = {Schöning, Uwe and Torán, Jacobo}
}

@Book{scutari_denis:bayesian_networks,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2667538},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.crcpress.com/9781482225587},
	language = {english},
	subtitle = {With Examples in R},
	title = {Bayesian Networks},
	isbn = {9781482225587},
	series = CRC_Statistical_Science,
	abstract = {_Bayesian Networks: With Examples in R_ introduces Bayesian networks using a hands-on approach.
Simple yet meaningful examples in R illustrate each step of the modeling process. The examples
start from the simplest notions and gradually increase in complexity. The authors also distinguish
the probabilistic models from their estimation with data sets.

The first three chapters explain the whole process of Bayesian network modeling, from structure
learning to parameter learning to inference. These chapters cover discrete Bayesian, Gaussian
Bayesian, and hybrid networks, including arbitrary random variables.

The book then gives a concise but rigorous treatment of the fundamentals of Bayesian networks and
offers an introduction to causal Bayesian networks. It also presents an overview of R and other
software packages appropriate for Bayesian networks. The final chapter evaluates two real-world
examples: a landmark causal protein signaling network paper and graphical modeling approaches for
predicting the composition of different body parts.

Suitable for graduate students and non-statisticians, this text provides an introductory overview
of Bayesian networks. It gives readers a clear, practical understanding of the general approach and
steps involved.},
	pagetotal = {241},
	publisher = CRC,
	author = {Scutari, Marco and Denis, Jean-Baptiste},
	date = {2014},
	keywords = {Lenguaje de programación R, Modelos gráficos probabilísticos, Redes bayesianas}
}

@Book{siarry:metaheuristics,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2748661},
	keywords = {Inteligencia artificial, Metaheurísticas},
	abstract = {Metaheuristics exhibit desirable properties like simplicity, easy
parallelizability, and ready applicability to different types of optimization
problems. After a comprehensive introduction to the field, the contributed
chapters in this book include explanations of the main metaheuristics
techniques, including simulated annealing, tabu search, evolutionary
algorithms, artificial ants, and particle swarms, followed by chapters that
demonstrate their applications to problems such as multiobjective optimization,
logistics, vehicle routing, and air traffic management.

The authors are leading researchers in this domain, with considerable teaching
and applications experience, and the book will be of value to industrial
practitioners, graduate students, and research academics.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783319454016},
	doi = {10.1007/978-3-319-45403-0},
	pagetotal = {489},
	isbn = {9783319454016},
	publisher = Springer,
	language = {english},
	editor = {Siarry, Patrick},
	date = {2016},
	title = {Metaheuristics}
}

@Collection{sigaud_buffet:markov_decision_processes_artificial_intelligence,
	subtitle = {MDPs, Beyond MDPs and Applications},
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2659949},
	url = {http://eu.wiley.com/WileyCDA/WileyTitle/productCd-1848211678.html},
	langidopts = {variant=british},
	langid = {english},
	publisher = Wiley,
	language = {english},
	title = {Markov Decision Processes in Artificial Intelligence},
	isbn = {9781848211674},
	abstract = {Markov Decision Processes (MDPs) are a mathematical framework for modeling sequential decision
problems under uncertainty as well as Reinforcement Learning problems. Written by experts in the
field, this book provides a global view of current research using MDPs in Artificial Intelligence.
It starts with an introductory presentation of the fundamental aspects of MDPs (planning in MDPs,
Reinforcement Learning, Partially Observable MDPs, Markov games and the use of non-classical
criteria). Then it presents more advanced research trends in the domain and gives some concrete
examples using illustrative applications.},
	pagetotal = {480},
	editor = {Sigaud, Olivier and Buffet, Olivier},
	date = {2010},
	keywords = {Modelos de Markov}
}

@Book{sipser:introduction_theory_computation,
	publisher = Cengage,
	file = {Libros/Sipser-2013-Introduction_to_the_Theory_of_Computation.pdf},
	keywords = {Ciencias de la computación},
	abstract = {Now you can clearly present even the most complex computational theory topics
to your students with Sipser's distinct, market-leading INTRODUCTION TO THE
THEORY OF COMPUTATION, 3E. The number one choice for today's computational
theory course, this highly anticipated revision retains the unmatched clarity
and thorough coverage that make it a leading text for upper-level undergraduate
and introductory graduate students. This edition continues author Michael
Sipser's well-known, approachable style with timely revisions, additional
exercises, and more memorable examples in key areas. A new first-of-its-kind
theoretical treatment of deterministic context-free languages is ideal for a
better understanding of parsing and LR(k) grammars. This edition's refined
presentation ensures a trusted accuracy and clarity that make the challenging
study of computational theory accessible and intuitive to students while
maintaining the subject's rigor and formalism. Readers gain a solid
understanding of the fundamental mathematical properties of computer hardware,
software, and applications with a blend of practical and philosophical coverage
and mathematical treatments, including advanced theorems and proofs.
INTRODUCTION TO THE THEORY OF COMPUTATION, 3E's comprehensive coverage makes
this an ideal ongoing reference tool for those studying theoretical computing.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.cengage.co.uk/books/9781133187790/},
	pagetotal = {504},
	isbn = {9781133187790},
	edition = {3},
	language = {english},
	date = {2013},
	title = {Introduction to the Theory of Computation},
	author = {Sipser, Michael}
}

@Book{sivanandam_deepa:introduction_genetic_algorithms,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2007252
Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-3-540-73190-0},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/978-3-540-73189-4},
	doi = {10.1007/978-3-540-73190-0},
	language = {english},
	title = {Introduction to Genetic Algorithms},
	isbn = {9783540731894},
	abstract = {Genetic Algorithms are adaptive heuristic search algorithm premised on the evolutionary ideas of
natural selection and genetic. The basic concept of Genetic Algorithms is designed to simulate
processes in natural system necessary for evolution, specifically those that follow the principles
first laid down by Charles Darwin of survival of the fittest. This book is designed to provide an
in-depth knowledge on the basic operational features and characteristics of Genetic Algorithms. The
various operators and techniques given in the book are pertinent to carry out Genetic Algorithm
Research Projects. The book also explores the different types are Genetic Algorithms available with
their importance. Implementation of Genetic Algorithm concept has been performed using the
universal language C/C++ and the discussion also extends to Genetic Algorithm MATLAB Toolbox. Few
Genetic Algorithm problems are programmed using MATLAB and the simulated results are given for the
ready reference of the reader. The applications of Genetic Algorithms in Machine learning,
Mechanical Engineering, Electrical Engineering, Civil Engineering, Data Mining, Image Processing,
and VLSI are dealt to make the readers understand where the concept can be applied.},
	pagetotal = {442},
	publisher = Springer,
	author = {Sivanandam, S. N. and Deepa, S. N.},
	date = {2008},
	keywords = {Algoritmos genéticos, Metaheurísticas},
	file = {Libros/Sivanandam_Deepa-2008-Introduction_to_Genetic_Algorithms.pdf}
}

@XData{software_tools_technology_transfer,
	journaltitle = {International Journal on Software Tools and Technology Transfer},
	issn = {1433-2779}
}

@Book{stephenson:python_workbook,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-3-319-14240-1},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/978-3-319-14239-5},
	doi = {10.1007/978-3-319-14240-1},
	language = {english},
	subtitle = {A Brief Introduction with Exercises and Solutions},
	title = {The Python Workbook},
	isbn = {9783319142395},
	abstract = {While other textbooks devote their pages to explaining introductory programming concepts, The
Python Workbook focuses exclusively on exercises, following the philosophy that computer
programming is a skill best learned through experience and practice.

Designed to support and encourage hands-on learning about programming, this student-friendly work
contains 174 exercises, spanning a variety of academic disciplines and everyday situations.
Solutions to selected exercises are also provided, supported by brief annotations that explain the
technique used to solve the problem, or highlight specific points of Python syntax. No background
knowledge is required to solve the exercises, beyond the material covered in a typical introductory
Python programming course.},
	pagetotal = {165},
	publisher = Springer,
	author = {Stephenson, Ben},
	date = {2014},
	keywords = {Lenguaje de programación Python},
	file = {Libros/Stephenson-2014-The_Python_Workbook.pdf}
}

@Book{stevens:how_write_good_programs,
	keywords = {Libro solicitado, No disponible en la BUS, Programación},
	abstract = {Learning to program isn't just learning the details of a programming language:
to become a good programmer you have to become expert at debugging, testing,
writing clear code and generally unsticking yourself when you get stuck, while
to do well in a programming course you have to learn to score highly in
coursework and exams. Featuring tips, stories and explanations of key terms,
this book teaches these skills explicitly. Examples in Python, Java and Haskell
are included, helping you to gain transferable programming skills whichever
language you are learning. Intended for students in Higher or Further Education
studying early programming courses, it will help you succeed in, and get the
most out of, your course, and support you in developing the software engineering
habits that lead to good programs.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.cambridge.org/es/academic/subjects/computer-science/computing-general-interest/how-write-good-programs-guide-students},
	pagetotal = {214},
	isbn = {9781108789875},
	publisher = Cambridge,
	language = {english},
	subtitle = {A Guide for Students},
	date = {2020},
	title = {How to Write Good Programs},
	author = {Stevens, Perdita}
}

@Proceedings{stoc-71,
	keywords = {Teoría de la Computación},
	date = {1971},
	eventtitleaddon = {STOC '71},
	eventtitle = {Third annual ACM symposium on Theory of computing},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1145/800157},
	isbn = {9781450374644},
	publisher = ACM,
	language = {english},
	venue = {Shaker Heights, Ohio, USA},
	eventdate = {1971-05-03/1971-05-05},
	title = {Proceedings of the third annual ACM symposium on Theory of computing}
}

@book{sucar:probabilistic_graphical_models,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2672827},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781447166986},
	doi = {10.1007/978-1-4471-6699-3},
	language = {english},
	subtitle = {Principles and Applications},
	title = {Probabilistic Graphical Models},
	isbn = {9781447166986},
	series = Springer_Computer_Vision,
	abstract = {This accessible text/reference provides a general introduction to probabilistic graphical models
(PGMs) from an engineering perspective. The book covers the fundamentals for each of the main
classes of PGMs, including representation, inference and learning principles, and reviews
real-world applications for each type of model. These applications are drawn from a broad range of
disciplines, highlighting the many uses of Bayesian classifiers, hidden Markov models, Bayesian
networks, dynamic and temporal Bayesian networks, Markov random fields, influence diagrams, and
Markov decision processes. Features: presents a unified framework encompassing all of the main
classes of PGMs; describes the practical application of the different techniques; examines the
latest developments in the field, covering multidimensional Bayesian classifiers, relational
graphical models and causal models; provides exercises, suggestions for further reading, and ideas
for research or programming projects at the end of each chapter.},
	pagetotal = {253},
	publisher = Springer,
	author = {Sucar, Luis Enrique},
	date = {2015},
	keywords = {Inteligencia artificial, Redes bayesianas}
}

@Book{suess_trumbo:introduction_probability_simulation_gibbs_sampling_r,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-0-387-68765-0
},
	file = {Libros/Suess_Trumbo-2010-Introduction_to_Probability_Simulation_and_Gibbs_Sampling_with_R.pdf},
	keywords = {Lenguaje de programación R, Simulación estocástica},
	abstract = {The first seven chapters use R for probability simulation and computation,
including random number generation, numerical and Monte Carlo integration, and
finding limiting distributions of Markov Chains with both discrete and
continuous states. Applications include coverage probabilities of binomial
confidence intervals, estimation of disease prevalence from screening tests,
parallel redundancy for improved reliability of systems, and various kinds of
genetic modeling. These initial chapters can be used for a non-Bayesian course
in the simulation of applied probability models and Markov Chains. Chapters 8
through 10 give a brief introduction to Bayesian estimation and illustrate the
use of Gibbs samplers to find posterior distributions and interval estimates,
including some examples in which traditional methods do not give satisfactory
results. WinBUGS software is introduced with a detailed explanation of its
interface and examples of its use for Gibbs sampling for Bayesian estimation.
No previous experience using R is required. An appendix introduces R, and
complete R code is included for almost all computational examples and problems
(along with comments and explanations). Noteworthy features of the book are its
intuitive approach, presenting ideas with examples from biostatistics,
reliability, and other fields; its large number of figures; and its
extraordinarily large number of problems (about a third of the pages), ranging
from simple drill to presentation of additional topics. Hints and answers are
provided for many of the problems. These features make the book ideal for
students of statistics at the senior undergraduate and at the beginning
graduate levels.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780387402734},
	doi = {10.1007/978-0-387-68765-0},
	pagetotal = {307},
	isbn = {9780387402734},
	publisher = Springer,
	series = Springer_Use_R,
	language = {english},
	date = {2010},
	title = {Introduction to Probability Simulation and Gibbs Sampling with R},
	author = {Suess, Eric A. and Trumbo, Bruce E.}
}

@Book{suthaharan:machine_learning_big_data,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2715367},
	series = Springer_Information_Systems,
	keywords = {Aprendizaje automático, Macrodatos},
	abstract = {This book presents machine learning models and algorithms to address big data classification
problems. Existing machine learning techniques like the decision tree (a hierarchical approach),
random forest (an ensemble hierarchical approach), and deep learning (a layered approach) are
highly suitable for the system that can handle such problems. This book helps readers, especially
students and newcomers to the field of big data and machine learning, to gain a quick understanding
of the techniques and technologies; therefore, the theory, examples, and programs (Matlab and R)
presented in this book have been simplified, hardcoded, repeated, or spaced for improvements. They
provide vehicles to test and understand the complicated concepts of various topics in the field. It
is expected that the readers adopt these programs to experiment with the examples, and then modify
or write their own programs toward advancing their knowledge for solving more complex and
challenging problems.

The presentation format of this book focuses on simplicity, readability, and dependability so that
both undergraduate and graduate students as well as new researchers, developers, and practitioners
in this field can easily trust and grasp the concepts, and learn them effectively. It has been
written to reduce the mathematical complexity and help the vast majority of readers to understand
the topics and get interested in the field. This book consists of four parts, with the total of 14
chapters. The first part mainly focuses on the topics that are needed to help analyze and
understand data and big data. The second part covers the topics that can explain the systems
required for processing big data. The third part presents the topics required to understand and
select machine learning techniques to classify big data. Finally, the fourth part concentrates on
the topics that explain the scaling-up machine learning, an important solution for modern big data
problems.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/978-1-4899-7640-6},
	doi = {10.1007/978-1-4899-7641-3},
	pagetotal = {359},
	isbn = {9781489976406},
	publisher = Springer,
	number = {36},
	language = {english},
	date = {2016},
	subtitle = {Thinking with Examples for Effective Learning},
	title = {Machine Learning Models and Algorithms for Big Data Classification},
	author = {Suthaharan, Shan}
}

@Book{sutton_barto:reinforcement_learning,
	file = {Libros/Sutton_Barto-2018-Reinforcement_Learning.pdf},
	keywords = {Aprendizaje por refuerzo},
	abstract = {Reinforcement learning, one of the most active research areas in artificial
intelligence, is a computational approach to learning whereby an agent tries to
maximize the total amount of reward it receives while interacting with a
complex, uncertain environment. In Reinforcement Learning, Richard Sutton and
Andrew Barto provide a clear and simple account of the field's key ideas and
algorithms. This second edition has been significantly expanded and updated,
presenting new topics and updating coverage of other topics.

Like the first edition, this second edition focuses on core online learning
algorithms, with the more mathematical material set off in shaded boxes. Part I
covers as much of reinforcement learning as possible without going beyond the
tabular case for which exact solutions can be found. Many algorithms presented
in this part are new to the second edition, including UCB, Expected Sarsa, and
Double Learning. Part II extends these ideas to function approximation, with
new sections on such topics as artificial neural networks and the Fourier
basis, and offers expanded treatment of off-policy learning and policy-gradient
methods. Part III has new chapters on reinforcement learning's relationships to
psychology and neuroscience, as well as an updated case-studies chapter
including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's
wagering strategy. The final chapter discusses the future societal impacts of
reinforcement learning.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://mitpress.mit.edu/books/reinforcement-learning-second-edition},
	pagetotal = {552},
	isbn = {9780262039246},
	publisher = MIT_Press,
	series = Adaptive_Computation,
	edition = {2},
	language = {english},
	subtitle = {An Introduction},
	date = {2018},
	title = {Reinforcement Learning},
	author = {Sutton, Richard S. and Barto, Andrew G.}
}

@Book{tadeusiewicz_et_al:exploring_neural_networks_c,
	enlaces = {Recurso electrónico (Safari Books Online): http://0-proquest.safaribooksonline.com.fama.us.es//?uiCode=sevil&xmlId=9781482233391},
	keywords = {Inteligencia artificial, Redes neuronales},
	abstract = {The utility of artificial neural network models lies in the fact that they can be used to infer
functions from observations—making them especially useful in applications where the complexity of
data or tasks makes the design of such functions by hand impractical.

**Exploring Neural Networks with C#** presents the important properties of neural networks—while
keeping the complex mathematics to a minimum. Explaining how to build and use neural networks, it
presents complicated information about neural networks structure, functioning, and learning in a
manner that is easy to understand.

Taking a "learn by doing" approach, the book is filled with illustrations to guide you through the
mystery of neural networks. Examples of experiments are provided in the text to encourage
individual research. Online access to C# programs is also provided to help you discover the
properties of neural networks.

Following the procedures and using the programs included with the book will allow you to learn how
to work with neural networks and evaluate your progress. You can download the programs as both
executable applications and C# source code from
http://home.agh.edu.pl/~tad//index.php?page=programy&lang=en},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Exploring-Neural-Networks-with-C/Tadeusiewicz-Chaki-Chaki/p/book/9781482233391},
	pagetotal = {298},
	isbn = {9781482233391},
	publisher = CRC,
	language = {english},
	date = {2014},
	title = {Exploring Neural Networks with C#},
	author = {Tadeusiewicz, Ryszard and Chaki, Rituparna and Chaki, Nabendu}
}

@Article{taha_hadi:anomaly_detection_methods_categorical_data,
	file = {Artículos_en_revistas/Taha_Hadi-2019-Anomaly_Detection_Methods_for_Categorical_Data:_A_Review.pdf},
	keywords = {Análisis de datos, Análisis de valores atípicos},
	abstract = {Anomaly detection has numerous applications in diverse fields. For example, it
has been widely used for dis- covering network intrusions and malicious events.
It has also been used in numerous other applications such as identifying
medical malpractice or credit fraud. Detection of anomalies in quantitative
data has received a considerable attention in the literature and has a
venerable history. By contrast, and despite the widespread availability use of
categorical data in practice, anomaly detection in categorical data has
received relatively little attention as compared to quantitative data. This is
because detection of anomalies in categorical data is a challenging problem.
Some anomaly detection techniques depend on identifying a representative pat-
tern then measuring distances between objects and this pattern. Objects that
are far from this pattern are declared as anomalies. However, identifying
patterns and measuring distances are not easy in categorical data compared with
quantitative data. Fortunately, several papers focussing on the detection of
anomalies in categorical data have been published in the recent literature. In
this article, we provide a comprehensive review of the research on the anomaly
detection problem in categorical data. Previous review articles focus on either
the statistics literature or the machine learning and computer science
literature. This review article combines both literatures. We review 36 methods
for the detection of anomalies in categorical data in both literatures and
classify them into 12 different categories based on the conceptual definition
of anomalies they use. For each approach, we survey anomaly detection methods,
and then show the similarities and differences among them. We emphasize two
important issues, the number of parameters each method requires and its time
complexity. The first issue is critical, because the performance of these
methods are sensitive to the choice of these parameters. The time complexity is
also very important in real applications especially in big data applications.
We report the time complexity if it is reported by the authors of the methods.
If it is not, then we derive it ourselves and report it in this article. In
addition, we discuss the common problems and the future directions of the
anomaly detection in categorical data.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1145/3312739},
	pages = {38:1-38:35},
	number = {2},
	volume = {52},
	language = {english},
	date = {2019},
	xdata = {acm_computing_surveys},
	title = {Anomaly Detection Methods for Categorical Data: A Review},
	author = {Taha, Ayman and Hadi, Ali S.}
}

@Book{talbi:metaheuristics,
	enlaces = {Biblioteca de la Universidad de Sevilla: https://fama.us.es/permalink/34CBUA_US/3enc2g/alma991013169904304987
},
	keywords = {Metaheurísticas},
	abstract = {This book provides a complete background on metaheuristics and shows readers
how to design and implement efficient algorithms to solve complex optimization
problems across a diverse range of applications, from networking and
bioinformatics to engineering design, routing, and scheduling. It presents the
main design questions for all families of metaheuristics and clearly
illustrates how to implement the algorithms under a software framework to reuse
both the design and code.

Throughout the book, the key search components of metaheuristics are considered as a toolbox for:

* Designing efficient metaheuristics (e.g. local search, tabu search, simulated
  annealing, evolutionary algorithms, particle swarm optimization, scatter
  search, ant colonies, bee colonies, artificial immune systems) for
  optimization problems
* Designing efficient metaheuristics for multi-objective optimization problems
* Designing hybrid, parallel, and distributed metaheuristics
* Implementing metaheuristics on sequential and parallel machines

Using many case studies and treating design and implementation independently,
this book gives readers the skills necessary to solve large-scale optimization
problems quickly and efficiently. It is a valuable reference for practicing
engineers and researchers from diverse areas dealing with optimization or
machine learning; and graduate students in computer science, operations
research, control, engineering, business and management, and applied
mathematics.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.wiley.com/en-es/Metaheuristics:+From+Design+to+Implementation+-p-9780470278581},
	pagetotal = {624},
	isbn = {9780470278581},
	publisher = Wiley,
	language = {english},
	subtitle = {From Design to Implementation},
	date = {2009},
	title = {Metaheuristics},
	author = {Talbi, El-Ghazali}
}

@Book{tang_et_al:neural_networks_computational_models,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-3-540-69226-3},
	file = {Libros/Tang_et_al-2007-Neural_Networks:_Computational_Models_and_Applications.pdf},
	keywords = {Inteligencia artificial, Redes neuronales},
	abstract = {Neural Networks: Computational Models and Applications covers a wealth of important theoretical and
practical issues in neural networks, including the learning algorithms of feed-forward neural
networks, various dynamical properties of recurrent neural networks, winner-take-all networks and
their applications in broad manifolds of computational intelligence: pattern recognition, uniform
approximation, constrained optimization, NP-hard problems, and image segmentation. By presenting
various computational models, this book is developed to provide readers with a quick but insightful
understanding of the broad and rapidly growing areas in the neural networks domain.

Besides laying down fundamentals on artificial neural networks, this book also studies biologically
inspired neural networks. Some typical computational models are discussed, and subsequently applied
to objection recognition, scene analysis and associative memory. The studies of bio-inspired models
have important implications in computer vision and robotic navigation, as well as new efficient
algorithms for image analysis. Another significant feature of the book is that it begins with
fundamental dynamical problems in presenting the mathematical techniques extensively used in
analyzing neurodynamics, thus allowing non-mathematicians to develop and apply these analytical
techniques easily.

Written for a wide readership, engineers, computer scientists and mathematicians interested in
machine learning, data mining and neural networks modeling will find this book of value. This book
will also act as a helpful reference for graduate students studying neural networks and complex
dynamical systems.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783540692256},
	doi = {10.1007/978-3-540-69226-3},
	pagetotal = {300},
	isbn = {9783540692256},
	publisher = Springer,
	number = {53},
	series = Springer_Computational_Intelligence,
	language = {english},
	date = {2007},
	title = {Neural Networks: Computational Models and Applications},
	author = {Tang, Huajin and Tan, Kay Chen and Yi, Zhang}
}

@XData{theoretical_computer_science,
	journaltitle = {Theoretical Computer Science},
	issn = {0304-3975},
	publisher = Elsevier
}

@Book{theus_urbanek:interactive_graphics_data_analysis,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2640878
},
	keywords = {Gráficos interactivos, Visualización de datos},
	abstract = {__Interactive Graphics for Data Analysis: Principles and Examples__ discusses
exploratory data analysis (EDA) and how interactive graphical methods can help
gain insights as well as generate new questions and hypotheses from datasets.

_Fundamentals of Interactive Statistical Graphics_
The first part of the book summarizes principles and methodology, demonstrating
how the different graphical representations of variables of a dataset are
effectively used in an interactive setting. The authors introduce the most
important plots and their interactive controls. They also examine various types
of data, relations between variables, and plot ensembles.

_Case Studies Illustrate the Principles_
The second section focuses on nine case studies. Each case study describes the
background, lists the main goals of the analysis and the variables in the
dataset, shows what further numerical procedures can add to the graphical
analysis, and summarizes important findings. Wherever applicable, the authors
also provide the numerical analysis for datasets found in Cox and Snell’s
landmark book.

_Understand How to Analyze Data through Graphical Means_
This full-color text shows that interactive graphical methods complement the
traditional statistical toolbox to achieve more complete, easier to understand,
and easier to interpret analyses.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Interactive-Graphics-for-Data-Analysis-Principles-and-Examples/Theus-Urbanek/p/book/9781584885948},
	pagetotal = {290},
	isbn = {9781584885948},
	publisher = CRC,
	series = CRC_Computer_Science,
	language = {english},
	subtitle = {Principles and Examples},
	date = {2008},
	title = {Interactive Graphics for Data Analysis},
	author = {Theus, Martin and Urbanek, Simon}
}

@Book{thomopoulos:essentials_monte_carlo_simulation,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/978-1-4614-6022-0
},
	file = {Libros/Thomopoulos-2013-Essentials_of_Monte_Carlo_Simulation.pdf},
	keywords = {Métodos de Monte Carlo, Simulación estocástica},
	abstract = {**Essentials of Monte Carlo Simulation** focuses on the fundamentals of Monte
Carlo methods using basic computer simulation techniques. The theories
presented in this text deal with systems that are too complex to solve
analytically. As a result, readers are given a system of interest and
constructs using computer code, as well as algorithmic models to emulate how
the system works internally. After the models are run several times, in a
random sample way, the data for each output variable(s) of interest is analyzed
by ordinary statistical methods. This book features 11 comprehensive chapters,
and discusses such key topics as random number generators, multivariate random
variates, and continuous random variates. Over 100 numerical examples are
presented as part of the appendix to illustrate useful real world applications.
The text also contains an easy to read presentation with minimal use of
difficult mathematical concepts. Very little has been published in the area of
computer Monte Carlo simulation methods, and this book will appeal to students
and researchers in the fields of Mathematics and Statistics.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781461460213},
	doi = {10.1007/978-1-4614-6022-0},
	pagetotal = {174},
	isbn = {9781461460213},
	publisher = Springer,
	language = {english},
	subtitle = {Statistical Methods for Building Simulation Models},
	date = {2013},
	title = {Essentials of Monte Carlo Simulation},
	author = {Thomopoulos, Nick T.}
}

@Book{thomopoulos:probability_distributions,
	file = {Libros/Thomopoulos-2018-Probability_Distributions.pdf},
	keywords = {Teoría de la probabilidad},
	abstract = {This volume presents a concise and practical overview of statistical methods
and tables not readily available in other publications. It begins with a review
of the commonly used continuous and discrete probability distributions. Several
useful distributions that are not so common and less understood are described
with examples and applications in full detail: discrete normal, left-partial,
right-partial, left-truncated normal, right-truncated normal, lognormal,
bivariate normal, and bivariate lognormal. Table values are provided with
examples that enable researchers to easily apply the distributions to real
applications and sample data. The left- and right-truncated normal
distributions offer a wide variety of shapes in contrast to the symmetrically
shaped normal distribution, and a newly developed spread ratio enables analysts
to determine which of the three distributions best fits a particular set of
sample data. The book will be highly useful to anyone who does statistical and
probability analysis. This includes scientists, economists, management
scientists, market researchers, engineers, mathematicians, and students in many
disciplines.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/978-3-319-76042-1},
	pagetotal = {163},
	isbn = {9783319760414},
	publisher = Springer,
	language = {english},
	subtitle = {With Truncated, Log and Bivariate Extensions},
	date = {2018},
	title = {Probability Distributions},
	author = {Thomopoulos, Nick T.}
}

@Article{turney_pantel:from_frequency_to_meaning,
	file = {Artículos_en_revistas/Turney_Pantel-2010-From_Frequency_to_Meaning:_Vector_Space_Models_of_Semantics.pdf},
	keywords = {Procesamiento del lenguaje natural},
	abstract = {Computers understand very little of the meaning of human language. This
profoundly limits our ability to give instructions to computers, the ability of
computers to explain their actions to us, and the ability of computers to
analyse and process text. Vector space models (VSMs) of semantics are beginning
to address these limits. This paper surveys the use of VSMs for semantic
processing of text. We organize the literature on VSMs according to the
structure of the matrix in a VSM. There are currently three broad classes of
VSMs, based on term-document, word-context, and pair-pattern matrices, yielding
three classes of applications. We survey a broad range of applications in these
three categories and we take a detailed look at a specific open source project
in each category. Our goal in this survey is to show the breadth of
applications of VSMs for semantics, to provide a new perspective on VSMs for
those who are already familiar with the area, and to provide pointers into the
literature for those who are less familiar with the field.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1613/jair.2934},
	pages = {141-188},
	volume = {37},
	language = {english},
	xdata = {journal_artificial_intelligence_research},
	date = {2010},
	title = {From Frequency to Meaning: Vector Space Models of Semantics},
	author = {Turney, Peter David and Pantel, Patrick}
}

@book{unwin:graphical_data_analysis_r,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2672806},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.gradaanwr.net/},
	language = {english},
	title = {Graphical Data Analysis with R},
	isbn = {9781498715232},
	series = CRC_R_Series,
	abstract = {*See How Graphics Reveal Information*

**Graphical Data Analysis with R** shows you what information you can gain from graphical displays.
The book focuses on why you draw graphics to display data and which graphics to draw (and uses R to
do so). All the datasets are available in R or one of its packages and the R code is available at
rosuda.org/GDA.

Graphical data analysis is useful for data cleaning, exploring data structure, detecting outliers
and unusual groups, identifying trends and clusters, spotting local patterns, evaluating modelling
output, and presenting results. This book guides you in choosing graphics and understanding what
information you can glean from them. It can be used as a primary text in a graphical data analysis
course or as a supplement in a statistics course. Colour graphics are used throughout.},
	pagetotal = {310},
	publisher = CRC,
	author = {Unwin, Antony},
	date = {2015},
	keywords = {Lenguaje de programación R, Visualización de datos}
}

@book{unwin_et_al:graphics_large_datasets,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/0-387-37977-0},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780387329062},
	doi = {10.1007/0-387-37977-0},
	language = {english},
	subtitle = {Visualizing a Million},
	title = {Graphics of Large Datasets},
	isbn = {9780387329062},
	series = Springer_Statistics_Computing,
	abstract = {Graphics are great for exploring data, but how can they be used for looking at the large datasets
that are commonplace to-day? This book shows how to look at ways of visualizing large datasets,
whether large in numbers of cases or large in numbers of variables or large in both. Data
visualization is useful for data cleaning, exploring data, identifying trends and clusters,
spotting local patterns, evaluating modeling output, and presenting results. It is essential for
exploratory data analysis and data mining. Data analysts, statisticians, computer scientists-indeed
anyone who has to explore a large dataset of their own-should benefit from reading this book.

New approaches to graphics are needed to visualize the information in large datasets and most of
the innovations described in this book are developments of standard graphics. There are
considerable advantages in extending displays which are well-known and well-tried, both in
understanding how best to make use of them in your work and in presenting results to others. It
should also make the book readily accessible for readers who already have a little experience of
drawing statistical graphics. All ideas are illustrated with displays from analyses of real
datasets and the authors emphasize the importance of interpreting displays effectively. Graphics
should be drawn to convey information and the book includes many insightful examples.},
	pagetotal = {271},
	publisher = Springer,
	author = {Unwin, Antony and Theus, Martin and Hofmann, Heike},
	date = {2006},
	keywords = {Macrodatos, Visualización de datos},
	file = {Libros/Unwin_et_al-2006-Graphics_of_Large_Datasets.pdf}
}

@Article{vadlamudi_et_al:anytime_pack_search,
	file = {Artículos_en_revistas/Vadlamudi_et_al-2016-Anytime_pack_search.pdf},
	keywords = {Búsqueda heurística, Inteligencia artificial},
	abstract = {Heuristic search is one of the fundamental problem solving techniques in artificial intelligence,
which is used in general to efficiently solve computationally hard problems in various domains,
especially in planning and optimization. In this paper, we present an anytime heuristic search
algorithm called anytime pack search (APS) which produces good quality solutions quickly and
improves upon them over time, by focusing the exploration on a limited set of most promising nodes
in each iteration. We discuss the theoretical properties of APS and show that it is complete. We
also present the complexity analysis of the proposed algorithm on a tree state-space model and show
that it is asymptotically of the same order as that of A*, which is a widely applied best-first
search method. Furthermore, we present a parallel formulation of the proposed algorithm, called
parallel anytime pack search (PAPS), which is applicable for searching tree state-spaces. We
theoretically prove the completeness of PAPS. Experimental results on the sliding-tile puzzle
problem, traveling salesperson problem, and single machine scheduling problem depict that the
proposed sequential algorithm produces much better anytime performance when compared to some of the
existing methods. Also, the proposed parallel formulation achieves super-linear speedups over the
sequential method.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/s11047-015-9490-9},
	pages = {395-414},
	number = {3},
	volume = {15},
	language = {english},
	date = {2016},
	xdata = {natural_computing},
	title = {Anytime pack search},
	author = {Vadlamudi, Satya Gautam and Aine, Sandip and Chakrabarti, Partha Pratim}
}

@Article{waddington_et_al:cloud_repositories_research_data,
	xdata = {journal_cloud_computing},
	file = {Artículos_en_revistas/Waddington_et_al-2013-Cloud_repositories_for_research_data_–_addressing_the_needs_of_researchers.pdf},
	number = {13},
	keywords = {Investigación reproducible},
	abstract = {This paper describes the problems and explores potential solutions for providing long term storage
and access to research outputs, focusing mainly on research data. The ready availability of cloud
storage and compute services provides a potentially attractive option for curation and preservation
of research information. In contrast to deploying infrastructure within an organisation, which
normally requires long lead times and upfront capital investment, cloud infrastructure is available
on demand and is highly scalable. However, use of commercial cloud services in particular raises
issues of governance, cost-effectiveness, trust and quality of service. We describe a set of
in-depth case studies conducted with researchers across the sciences and humanities performing
data-intensive research, which demonstrate the issues that need to be considered when preserving
data in the cloud. We then describe the design of a repository framework that addresses these
requirements. The framework uses hybrid cloud, combining internal institutional storage, cloud
storage and cloud-based preservation services into a single integrated repository infrastructure.
Allocation of content to storage providers is performed using on a rules-based approach. The
results of an evaluation of the proof-of-concept system are described.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1186/2192-113X-2-13},
	volume = {2},
	language = {english},
	date = {2013},
	title = {Cloud repositories for research data – addressing the needs of researchers},
	author = {Waddington, Simon and Zhang, Jun and Knight, Gareth and Jensen, Jens and Downing, Roger and Ketley, Cheney}
}

@Article{wang_et_al:fuzzy_membrane_computing,
	issuesubtitle = {Special Issue on Fuzzy Sets and Applications (Celebration of the 50th Anniversary of Fuzzy Sets)},
	xdata = {computers_communications_control},
	file = {Artículos_en_revistas/Wang_et_al-2015-Fuzzy_Membrane_Computing:_Theory_and_Applications.pdf},
	keywords = {Computación con membranas, Lógica difusa},
	abstract = {Fuzzy membrane computing is a newly developed and promising research direction
in the area of membrane computing that aims at exploring the complex
interaction between membrane computing and fuzzy theory. This paper provides a
comprehensive survey of theoretical developments and various applications of
fuzzy membrane computing, and sketches future research lines. The theoretical
developments are reviewed from the aspects of uncertainty processing in P
systems, fuzzification of P systems and fuzzy knowledge representation and
reasoning. The applications of fuzzy membrane computing are mainly focused on
fuzzy knowledge representation and fault diagnosis. An overview of different
types of fuzzy P systems, differences between spiking neural P systems and
fuzzy reasoning spiking neural P systems and newly obtained results on these P
systems are presented.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.15837/ijccc.2015.6.2080},
	pages = {904-935},
	number = {6},
	volume = {10},
	language = {english},
	date = {2015},
	title = {Fuzzy Membrane Computing: Theory and Applications},
	author = {Wang, Tao and Zhang, Gexiang and Pérez-Jiménez, Mario J.}
}

@Book{ward_et_al:interactive_data_visualization,
	keywords = {Libro solicitado, No disponible en la BUS, Visualización de datos},
	abstract = {**Interactive Data Visualization: Foundations, Techniques, and Applications,
Second Edition** provides all the theory, details, and tools necessary to build
visualizations and systems involving the visualization of data. In color
throughout, it explains basic terminology and concepts, algorithmic and software
engineering issues, and commonly used techniques and high-level algorithms. Full
source code is provided for completing implementations.

**New to the Second Edition**

* New related readings, exercises, and programming projects
* Better quality figures and numerous new figures
* New chapter on techniques for time-oriented data

This popular book continues to explore the fundamental components of the
visualization process, from the data to the human viewer. For developers, the
book offers guidance on designing effective visualizations using methods derived
from human perception, graphical design, art, and usability analysis. For
practitioners, it shows how various public and commercial visualization systems
are used to solve specific problems in diverse domains. For researchers, the
text describes emerging technology and hot topics in development at academic and
industrial centers today.

Each chapter presents several types of exercises, including review questions and
problems that motivate readers to build on the material covered and design
alternate approaches to solving a problem. In addition, programming projects
encourage readers to perform a range of tasks, from the simple implementation of
algorithms to the extension of algorithms and programming techniques.

*Web Resource*

A supplementary website includes downloadable software tools and example data
sets, enabling hands-on experience with the techniques covered in the text. The
site also offers links to useful data repositories and data file formats, an
up-to-date listing of software packages and vendors, and instructional tools,
such as reading lists, lecture slides, and demonstration programs.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.routledge.com/Interactive-Data-Visualization-Foundations-Techniques-and-Applications/Ward-Grinstein-Keim/p/book/9781482257373},
	pagetotal = {578},
	isbn = {9781482257373},
	publisher = CRC,
	edition = {2},
	language = {english},
	subtitle = {Foundations, Techniques, and Applications},
	date = {2015},
	title = {Interactive Data Visualization},
	author = {Ward, Matthew O. and Grinstein, Georges and Keim, Daniel}
}

@Book{wexler_et_al:big_book_dashboards,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2835344
},
	keywords = {Visualización de datos},
	abstract = {**The definitive reference book with real-world solutions you won't find
anywhere else**

*The Big Book of Dashboards* presents a comprehensive reference for those
tasked with building or overseeing the development of business dashboards.

Comprising dozens of examples that address different industries and departments
(healthcare, transportation, finance, human resources, marketing, customer
service, sports, etc.) and different platforms (print, desktop, tablet,
smartphone, and conference room display) *The Big Book of Dashboards* is the
only book that matches great dashboards with real-world business scenarios.

By organizing the book based on these scenarios and offering practical and
effective visualization examples, *The Big Book of Dashboards* will be the
trusted resource that you open when you need to build an effective business
dashboard.

In addition to the scenarios there's an entire section of the book that is
devoted to addressing many practical and psychological factors you will
encounter in your work. It's great to have theory and evidenced-based research
at your disposal, but what will you do when somebody asks you to make your
dashboard 'cooler' by adding packed bubbles and donut charts?

The expert authors have a combined 30-plus years of hands-on experience helping
people in hundreds of organizations build effective visualizations. They have
fought many 'best practices' battles and having endured bring an uncommon
empathy to help you, the reader of this book, survive and thrive in the data
visualization world.

A well-designed dashboard can point out risks, opportunities, and more; but
common challenges and misconceptions can make your dashboard useless at best,
and misleading at worst. The Big Book of Dashboards gives you the tools,
guidance, and models you need to produce great dashboards that inform,
enlighten, and engage.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://bigbookofdashboards.com/
https://www.wiley.com/en-es/The+Big+Book+of+Dashboards:+Visualizing+Your+Data+Using+Real+World+Business+Scenarios-p-9781119282716
},
	pagetotal = {448},
	isbn = {9781119282716},
	publisher = Wiley,
	language = {english},
	subtitle = {Visualizing Your Data Using Real-World Business Scenarios},
	date = {2017},
	title = {The Big Book of Dashboards},
	author = {Wexler, Steve and Shaffer, Jeffrey and Cotgreave, Andy}
}

@Book{wickham:ggplot2,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2060400
Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-0-387-98141-3},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/gp/book/9780387981413},
	doi = {10.1007/978-0-387-98141-3},
	language = {english},
	subtitle = {Elegant Graphics for Data Analysis},
	title = {ggplot2},
	isbn = {9780387981406},
	series = Springer_Use_R,
	abstract = {This book describes ggplot2, a new data visualization package for R that uses the insights from
Leland Wilkison's _Grammar of Graphics_ to create a powerful and flexible system for creating data
graphics. With ggplot2, it's easy to:

* produce handsome, publication-quality plots, with automatic legends created from the plot
  specification
* superpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different
  data sources, with automatically adjusted common scales
* add customisable smoothers that use the powerful modelling capabilities of R, such as loess,
  linear models, generalised additive models and robust regression
* save any ggplot2 plot (or part thereof) for later modification or reuse
* create custom themes that capture in-house or journal style requirements, and that can easily be
  applied to multiple plots
* approach your graph from a visual perspective, thinking about how each component of the data is
  represented on the final plot

This book will be useful to everyone who has struggled with displaying their data in an informative
and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your
data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and
you'll learn everything you need in the book. After reading this book you'll be able to produce
graphics customized precisely for your problems, and you'll find it easy to get graphics out of
your head and on to the screen or page.},
	pagetotal = {213},
	publisher = Springer,
	author = {Wickham, Hadley},
	date = {2009},
	keywords = {Lenguaje de programación R, Visualización de datos},
	file = {Libros/Wickham-2009-ggplot2-Elegant_Graphics_for_Data_Analysis.pdf}
}

@Book{wickham:ggplot2_2_ed,
	file = {Libros/Wickham-2016-ggplot2-Elegant_Graphics_for_Data_Analysis.pdf},
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2714853},
	edition = {2},
	subtitle = {Elegant Graphics for Data Analysis},
	keywords = {Lenguaje de programación R, Visualización de datos},
	abstract = {This new edition to the classic book by ggplot2 creator Hadley Wickham highlights compatibility
with knitr and RStudio. ggplot2 is a data visualization package for R that helps users create data
graphics, including those that are multi-layered, with ease. With ggplot2, it's easy to:

* produce handsome, publication-quality plots with automatic legends created from the plot
  specification
* superimpose multiple layers (points, lines, maps, tiles, box plots) from different data sources
  with automatically adjusted common scales
* add customizable smoothers that use powerful modeling capabilities of R, such as loess, linear
  models, generalized additive models, and robust regression
* save any ggplot2 plot (or part thereof) for later modification or reuse
* create custom themes that capture in-house or journal style requirements and that can easily be
  applied to multiple plots
* approach a graph from a visual perspective, thinking about how each component of the data is
  represented on the final plot

This book will be useful to everyone who has struggled with displaying data in an informative and
attractive way. Some basic knowledge of R is necessary (e.g., importing data into R). ggplot2 is a
mini-language specifically tailored for producing graphics, and you'll learn everything you need in
the book. After reading this book you'll be able to produce graphics customized precisely for your
problems, and you'll find it easy to get graphics out of your head and on to the screen or page.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9783319242750},
	doi = {10.1007/978-3-319-24277-4},
	pagetotal = {260},
	isbn = {9783319242750},
	publisher = Springer,
	series = Springer_Use_R,
	language = {english},
	date = {2016},
	title = {ggplot2},
	author = {Wickham, Hadley}
}

@Book{wickham:r_packages,
	enlaces = {Recurso electrónico (Safari Books Online): http://0-proquest.safaribooksonline.com.fama.us.es//?uiCode=sevil&xmlId=9781491910580},
	keywords = {Lenguaje de programación R},
	abstract = {Turn your R code into packages that others can easily download and use. This practical book shows
you how to bundle reusable R functions, sample data, and documentation together by applying author
Hadley Wickham’s package development philosophy. In the process, you’ll work with devtools,
roxygen, and testthat, a set of R packages that automate common development tasks. Devtools
encapsulates best practices that Hadley has learned from years of working with this programming
language.

Ideal for developers, data scientists, and programmers with various backgrounds, this book starts
you with the basics and shows you how to improve your package writing over time. You’ll learn to
focus on what you want your package to do, rather than think about package structure.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://shop.oreilly.com/product/0636920034421.do},
	pagetotal = {202},
	isbn = {9781491910597},
	publisher = OReilly,
	language = {english},
	date = {2015},
	subtitle = {Organize, Test, Document, and Share Your Code},
	title = {R Packages},
	author = {Wickham, Hadley}
}

@Book{wickham_grolemund:r_data_science,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2715355
Recurso electrónico (Safari Books Online): http://0-proquest.safaribooksonline.com.fama.us.es/?uiCode=sevil&xmlId=9781491910382},
	keywords = {Ciencia del dato, Lenguaje de programación R},
	abstract = {What exactly is data science? With this book, you’ll gain a clear understanding of this discipline
for discovering natural laws in the structure of data. Along the way, you’ll learn how to use the
versatile R programming language for data analysis.

Whenever you measure the same thing twice, you get two results—as long as you measure precisely
enough. This phenomenon creates uncertainty and opportunity. Author Garrett Grolemund, Master
Instructor at RStudio, shows you how data science can help you work with the uncertainty and
capture the opportunities. You’ll learn about:

* __Data Wrangling__—how to manipulate datasets to reveal new information
* __Data Visualization__—how to create graphs and other visualizations
* __Exploratory Data Analysis__—how to find evidence of relationships in your measurements
* __Modelling__—how to derive insights and predictions from your data
* __Inference__—how to avoid being fooled by data analyses that cannot provide foolproof results

Through the course of the book, you’ll also learn about the statistical worldview, a way of seeing
the world that permits understanding in the face of uncertainty, and simplicity in the face of
complexity.
},
	langidopts = {variant=british},
	langid = {english},
	url = {http://shop.oreilly.com/product/0636920034407.do},
	pagetotal = {250},
	isbn = {9781491910399},
	publisher = OReilly,
	language = {english},
	date = {2016},
	subtitle = {Visualize, Model, Transform, Tidy, and Import Data},
	title = {R for Data Science},
	author = {Wickham, Hadley and Grolemund, Garret}
}

@Book{wiering_otterlo:reinforcement_learning,
	series = Springer_Adaptation_Learning_Optimization,
	file = {Libros/Wiering_van-Otterlo-2012-Reinforcement_Learning.pdf},
	keywords = {Aprendizaje por refuerzo},
	abstract = {Reinforcement learning encompasses both a science of adaptive behavior of
rational beings in uncertain environments and a computational methodology for
finding optimal behaviors for challenging problems in control, optimization and
adaptive behavior of intelligent agents. As a field, reinforcement learning has
progressed tremendously in the past decade.

The main goal of this book is to present an up-to-date series of survey
articles on the main contemporary sub-fields of reinforcement learning. This
includes surveys on partially observable environments, hierarchical task
decompositions, relational knowledge representation and predictive state
representations. Furthermore, topics such as transfer, evolutionary methods and
continuous spaces in reinforcement learning are surveyed. In addition, several
chapters review reinforcement learning methods in robotics, in games, and in
computational neuroscience. In total seventeen different subfields are
presented by mostly young experts in those areas, and together they truly
represent a state-of-the-art of current reinforcement learning research.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9783642276446},
	doi = {10.1007/978-3-642-27645-3},
	pagetotal = {638},
	isbn = {    9783642276446},
	publisher = Springer,
	number = {12},
	language = {english},
	subtitle = {State-of-the-Art},
	editor = {Wiering, Marco and van Otterlo, Martjin},
	date = {2012},
	title = {Reinforcement Learning}
}

@Book{wilkes:advanced_python_development,
	url = {https://www.springer.com/9781484257920},
	keywords = {Lenguaje de programación Python, Libro solicitado, No disponible en la BUS},
	abstract = {This book builds on basic Python tutorials to explain various Python language
features that aren’t routinely covered: from reusable console scripts that play
double duty as micro-services by leveraging entry points, to using asyncio
efficiently to collate data from a large number of sources. Along the way, it
covers type-hint based linting, low-overhead testing and other automated quality
checking to demonstrate a robust real-world development process.

Some powerful aspects of Python are often documented with contrived examples
that explain the feature as a standalone example only. By following the design
and build of a real-world application example from prototype to production
quality you'll see not only how the various pieces of functionality work but how
they integrate as part of the larger system design process. In addition, you'll
benefit from the kind of useful asides and library recommendations that are a
staple of conference Q&A sessions at Python conferences as well as discussions
of modern Python best practice and techniques to better produce clear code that
is easily maintainable.

_Advanced Python Development_ is intended for developers who can already write
simple programs in Python and want to understand when it’s appropriate to use
new and advanced language features and to do so in a confident manner. It is
especially of use to developers looking to progress to a more senior level and
to very experienced developers who have thus far used older versions of Python.},
	langidopts = {variant=british},
	langid = {english},
	doi = {10.1007/978-1-4842-5793-7},
	pagetotal = {605},
	isbn = {9781484257920},
	publisher = Apress,
	language = {english},
	subtitle = {Using Powerful Language Features in Real-World Applications},
	date = {2020},
	title = {Advanced Python Development},
	author = {Wilkes, Matthew}
}

@Book{wilkinson:grammar_graphics,
	enlaces = {Recurso electrónico (SpringerLink): http://0-dx.doi.org.fama.us.es/10.1007/0-387-28695-0
},
	file = {Libros/Wilkinson-2005-The_Grammar_of_Graphics.pdf},
	keywords = {Visualización de datos},
	abstract = {This book was written for statisticians, computer scientists, geographers,
researchers, and others interested in visualizing data. It presents a unique
foundation for producing almost every quantitative graphic found in scientific
journals, newspapers, statistical packages, and data visualization systems.
While the tangible results of this work have been several visualization
software libraries, this book focuses on the deep structures involved in
producing quantitative graphics from data. What are the rules that underlie the
production of pie charts, bar charts, scatterplots, function plots, maps,
mosaics, and radar charts? Those less interested in the theoretical and
mathematical foundations can still get a sense of the richness and structure of
the system by examining the numerous and often unique color graphics it can
produce. The second edition is almost twice the size of the original, with six
new chapters and substantial revision. Much of the added material makes this
book suitable for survey courses in visualization and statistical graphics.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780387245447},
	doi = {10.1007/0-387-28695-0},
	pagetotal = {691},
	isbn = {9780387245447},
	publisher = Springer,
	series = Springer_Statistics_Computing,
	edition = {2},
	language = {english},
	date = {2005},
	title = {The Grammar of Graphics},
	author = {Wilkinson, Leland}
}

@Book{wills:visualizing_time,
	enlaces = {Recurso electrónico (SpringerLink): http://www.us.debiblio.com/login?url=http://dx.doi.org/10.1007/978-0-387-77907-2
},
	file = {Libros/Wills-2012-Visualizing_Time.pdf},
	keywords = {Visualización de datos},
	abstract = {Art, or Science? Which of these is the right way to think of the field of
visualization? This is not an easy question to answer, even for those who have
many years experience in making graphical depictions of data with a view to
help people understand it and take action. In this book, Graham Wills bridges
the gap between the art and the science of visually representing data. He does
not simply give rules and advice, but bases these on general principles and
provide a clear path between them

This book is concerned with the graphical representation of time data and is
written to cover a range of different users. A visualization expert designing
tools for displaying time will find it valuable, but so also should a financier
assembling a report in a spreadsheet, or a medical researcher trying to display
gene sequences using a commercial statistical package.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9780387779065},
	doi = {10.1007/978-0-387-77907-2},
	pagetotal = {256},
	isbn = {9780387779065},
	publisher = Springer,
	series = Springer_Statistics_Computing,
	language = {english},
	subtitle = {Designing Graphical Representations for Statistical Data},
	date = {2012},
	title = {Visualizing Time},
	author = {Wills, Graham}
}

@XData{wires_data_mining_knowledge_discovery,
	journaltitle = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	issn = {1942-4795},
	shortjournal = {WIREs DMKD},
	publisher = Wiley
}

@book{xie:dynamic_documents_r_knitr,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2672816},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/Dynamic-Documents-with-R-and-knitr-Second-Edition/Xie/p/book/9781498716963},
	language = {english},
	edition = {2},
	title = {Dynamic Documents with R and knitr},
	isbn = {9781498716963},
	series = CRC_R_Series,
	abstract = {*Quickly and Easily Write Dynamic Documents*

Suitable for both beginners and advanced users, **Dynamic Documents with R and knitr, Second
Edition** makes writing statistical reports easier by integrating computing directly with
reporting. Reports range from homework, projects, exams, books, blogs, and web pages to virtually
any documents related to statistical graphics, computing, and data analysis. The book covers basic
applications for beginners while guiding power users in understanding the extensibility of the
knitr package.

*New to the Second Edition*

* A new chapter that introduces R Markdown v2
* Changes that reflect improvements in the knitr package
* New sections on generating tables, defining custom printing methods for objects in code chunks,
  the C/Fortran engines, the Stan engine, running engines in a persistent session, and starting a
  local server to serve dynamic documents

*Boost Your Productivity in Statistical Report Writing and Make Your Scientific Computing with R
 Reproducible*

Like its highly praised predecessor, this edition shows you how to improve your efficiency in
writing reports. The book takes you from program output to publication-quality reports, helping you
fine-tune every aspect of your report.},
	pagetotal = {294},
	publisher = CRC,
	author = {Xie, Yihui},
	date = {2015},
	keywords = {Investigación reproducible, Lenguaje de programación R}
}

@Book{xie_et_al:r_markdown,
	enlaces = {Versión en línea: https://bookdown.org/yihui/rmarkdown/
},
	keywords = {Lenguaje de programación R},
	abstract = {**R Markdown: The Definitive Guide** is the first official book authored by the
core R Markdown developers that provides a comprehensive and accurate reference
to the R Markdown ecosystem. With R Markdown, you can easily create
reproducible data analysis reports, presentations, dashboards, interactive
applications, books, dissertations, websites, and journal articles, while
enjoying the simplicity of Markdown and the great power of R and other
languages.

In this book, you will learn

* Basics: Syntax of Markdown and R code chunks, how to generate figures and
  tables, and how to use other computing languages
* Built-in output formats of R Markdown: PDF/HTML/Word/RTF/Markdown documents
  and ioslides/Slidy/Beamer/PowerPoint presentations
* Extensions and applications: Dashboards, Tufte handouts, xaringan/reveal.js
  presentations, websites, books, journal articles, and interactive tutorials
* Advanced topics: Parameterized reports, HTML widgets, document templates,
  custom output formats, and Shiny documents.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.crcpress.com/p/book/9781138359338},
	pagetotal = {304},
	isbn = {9781138359338},
	publisher = CRC,
	series = CRC_R_Series,
	language = {english},
	subtitle = {The Definitive Guide},
	date = {2018},
	title = {R Markdown},
	author = {Xie, Yihui and Allaire, Joseph J. and Grolemund, Garret}
}

@Book{yu_gen:introduction_evolutionary_algorithms,
	series = Springer_Decision_Engineering,
	enlaces = {Biblioteca de la Universidad de Sevilla: http://encore.fama.us.es/iii/encore/record/C__Rb2160176
Recurso electrónico: http://0-dx.doi.org.fama.us.es/10.1007/978-1-84996-129-5},
	file = {Libros/Yu_Gen-2010-Introduction_to_Evolutionary_Algorithms.pdf},
	keywords = {Algoritmos evolutivos, Computación bioinspirada, Inteligencia artificial},
	abstract = {Evolutionary algorithms are becoming increasingly attractive across various disciplines, such as
operations research, computer science, industrial engineering, electrical engineering, social
science and economics. Introduction to Evolutionary Algorithms presents an insightful,
comprehensive, and up-to-date treatment of evolutionary algorithms. It covers such hot topics as: •
genetic algorithms, • differential evolution, • swarm intelligence, and • artificial immune
systems. The reader is introduced to a range of applications, as Introduction to Evolutionary
Algorithms demonstrates how to model real world problems, how to encode and decode individuals, and
how to design effective search operators according to the chromosome structures with examples of
constraint optimization, multiobjective optimization, combinatorial optimization, and
supervised/unsupervised learning. This emphasis on practical applications will benefit all
students, whether they choose to continue their academic career or to enter a particular industry.
Introduction to Evolutionary Algorithms is intended as a textbook or self-study material for both
advanced undergraduates and graduate students. Additional features such as recommended further
reading and ideas for research projects combine to form an accessible and interesting pedagogical
approach to this widely used discipline.},
	langidopts = {variant=british},
	langid = {english},
	url = {http://www.springer.com/9781849961288},
	doi = {10.1007/978-1-84996-129-5},
	pagetotal = {422},
	isbn = {9781849961288},
	publisher = Springer,
	language = {english},
	date = {2010},
	title = {Introduction to Evolutionary Algorithms},
	author = {Yu, Xinjie and Gen, Mitsuo}
}

@Book{zimmerman:teaching_AI,
	publisher = ISTE,
	keywords = {Docencia, Inteligencia artificial, Libro solicitado, No disponible en la BUS},
	abstract = {For many, artificial intelligence, or AI, may seem like a new and possibly
overwhelming concept. The reality is that AI is already being applied in
industry and, for many of us, in our daily lives as well. A better understanding
of AI can help you make informed decisions now that will impact the future of
your learners.


This book features:

* Perspectives from educators and industry experts on how they are using AI.
* Approaches to teaching about AI, including design thinking, project-based
  learning and STEM connections.
* Tools for exploring AI and sharing it with your students.
* Activities to introduce AI concepts, reflection questions and lesson ideas.

In *Teaching AI*, you’ll learn what AI is, how it works and how to use it to
better prepare students in a world with increased human-computer interaction.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://my.iste.org/s/store#/store/browse/detail/a1w1U000004LpbQQAS},
	pagetotal = {216},
	isbn = {9781564847058},
	language = {english},
	subtitle = {Exploring New Frontiers for Learning},
	date = {2018},
	title = {Teaching AI},
	author = {Zimmerman, Michelle}
}

@Book{zobel:writing_computer_science,
	file = {Libros/Zobel-2014-Writing_for_Computer_Science.pdf},
	keywords = {Ciencias de la computación},
	abstract = {All researchers need to write or speak about their work, and to have research
that is worth presenting. Based on the author's decades of experience as a
researcher and advisor, this third edition provides detailed guidance on
writing and presentations and a comprehensive introduction to research methods,
the how-to of being a successful scientist.

Topics include:

  * Development of ideas into research questions;
  * How to find, read, evaluate and referee other research;
  * Design and evaluation of experiments and appropriate use of statistics;
  * Ethics, the principles of science and examples of science gone wrong.

Much of the book is a step-by-step guide to effective communication, with
advice on:

  * Writing style and editing;
  * Figures, graphs and tables;
  * Mathematics and algorithms;
  * Literature reviews and referees’ reports;
  * Structuring of arguments and results into papers and theses;
  * Writing of other professional documents;
  * Presentation of talks and posters.

Written in an accessible style and including handy checklists and exercises,
*Writing for Computer Science* is not only an introduction to the doing and
describing of research, but is a valuable reference for working scientists in
the computing and mathematical sciences.},
	langidopts = {variant=british},
	langid = {english},
	url = {https://www.springer.com/9781447166382},
	doi = {10.1007/978-1-4471-6639-9},
	pagetotal = {284},
	isbn = {978-1-4471-6638-2},
	publisher = Springer,
	edition = {3},
	language = {english},
	date = {2014},
	title = {Writing for Computer Science},
	author = {Zobel, Justin}
}

